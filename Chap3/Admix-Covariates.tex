\input{../header.tex}
\section{Admixture Models with Covariates}
In this section we investigate how to effectively incorporate covariates into admixture models. Such methods can be used to improve the fit of admixture models, or use admixture models to incorporate text into prediction problems. In particular, we consider models where topic distributions are conditioned on features. This is is a multi-task prediction problem, and so we consider multi-task modelling techniques. In particular, we are interested in micro-texts, where documents have so fewer than 20 words, and so standard topic model configurations (e.g. $K=50$) would be grossly over-parameterised. We demonstrate empirical results on a dataset of tweets from Twittter

\subsection{Incorporating Covariates into Admixture Models}
Our setting is a set of documents $\vv{w}_d$ each of which is accompanied by some features $\vv{x}_d$ . The methods of incorporating covariates into LDA fall into two categories, as described in\cite{Mimno2008}: ``upstream" models which consider the case $p(\wdoc | \xd) = \int p(\wdoc|\thd, \Phi)p(\thd|\xd) d\thd$ 
and ``downstream" models which consider the model $p(\wdoc, \xd) = \int p(\wdoc|\thd, \Phi)p(\xd|\thd, \Omega)p(\thd)d\thd$. Downstream models are sometimes known as multi-modal\cite{Virtanen2012a} or multi-field\cite{Salomatin2009} models.

\subsubsection{Downstream Models}
In the case where the feature $\xd$ are, like the words $\wdoc$, a set of conditionally independent, exchangeable observations, a downstream model is trivially implemented\cite{Erosheva2004}\cite{Blei2003} by sampling separate per-observation topic assignments for each ``modality" $z_{dn}^{(m)}|\thd \sim \muln{\thd}{1}$ and then sampling the observation according to a modality-specific distribution $\vv{x}_d^{(m)} \sim \muln{\vv{\phi}^{(m)}_{z^{(m)}_{dn}}}{1}$, were words are just one particular modality.

If one feels that the bulk of information is in a particular modality, one can use a variant\cite{Blei2003} where topics are sampled from the topic distribution for one modality only, and then these are resampled to generate topics for all other modalities, such that $z_{dn}^{(m)}|\vv{z}_d^{(0)} = z_{di}^{(0)}$ for $i \sim \mathcal{U}(0, n_{d\cdot\cdot}^{(0)})$. It is a simple matter to use a Dirichet Process instead of the Dirichet prior to infer the topic count\cite{Yakhnenko2009}. This model has been used with images (using bags of image ``words") and their captions\cite{Blei2003}, and paired documents in different languages \cite{Zhao2001}. The latter allows one to derive up topic-specific translations of words.

Given the links between LDA and Multinomial PCA\cite{Buntine2002}, one can see how this construction could be used to generalise CCA to the case of many heterogeneous distributions neither which are necessarily Gaussian, which has been discussed in \cite{Virtanen2012a}. 

A disadvantage of the approaches outlined thus far is that they all require the same number of topics for each modality, and even the same topics. In practice the number of set of topics used to generate, for example, words, may differ to those necessary to generate images. To address this, one can sample a large topic-strength vector $\etd \sim \nor{\vv{\mu}}{\Sigma}$ of dimension $K$ from a Gaussian instead, and partition it into disjoint subvectors $\vv{\eta}_d^{(m)}$ of length $K^{(m)}$ such that $K = \sum_m K^{(m)}$. The topic subvectors can then be used to generate observation-specific topics $z^{(m)} \sim \muln{\sigma(\vv{\eta}_d^{(m)})}{1}$. These modality-specific topics are conditionally independent given the prior, and so by learning the parameters of the prior, and particular the covariance, one can determine how they interact\cite{Salomatin2009}. As an example, two or three topics used to generate words may only correspond to one topic used to generate features.

A further complication is the fact that there are some topics which need to be totally private - i.e. which are used solely to model, say, modality-specific noise or structure. In these cases we want to encode a preference for sparse covariance matrices. Furthermore, we want to be able to infer modality-specific topic counts. Both of these issues can be addressed, by replacing the softmax transformation with a used the sparsifying stick-breaking construction of \cite{Paisley2012} when converting unnormalised vectors into probability vectors\cite{Virtanen2012a}. An obvious advantage of this approach is the ability to specify an upper bound on topics and have the model infer the appropriate number of topics. A second advantage however is the ability to infer ``private" topics per modality, topics which have no direct correlation with topics in other modalities.

Thus far we have considered downstream models featuring exchangeable counts, such as words in different languages and visual-words. However one needs also address the case where one of the modalities does not meet these assumptions.

For example if one of the modalities is a single, scalar variable, modelled using a generalised linear model (GLM), one can construct the feature passed to that GLM as $\bar{z}_d = \frac{1}{N_d} \sum_n \vv{z}_{dn}$, i.e the mean of the 1-of-K indicator vectors giving the per-token topic assignments in a principle modality (typically words) \cite{Blei2008}. By incorporating this in the model, instead of running separate topic-inference and GLM inference steps, one can use the second modality to affect the distribution over topics for the overall document set $(\wdoc, x)$

A special case of this is time. Given a bounded interval, one rescale this to lie between zero and one and so sample that particular modality using a Beta distribution.\cite{Wang2006}. This induces a dependency on time in the topic posterior, thereby handling the case of corpora which span a large time-period, where the distribution over topics for one early document should differ to the that for a much later document

While not strictly a ``downstream" model, or indeed an ``upstream" model, it is worth nothing that this approach was extended in the dynamic topic model of \cite{Blei2006a}. In this the prior over topics is represented by a logistic-normal distribution, as in CTM, but so too are the per-topic vocabulary distributions. By implementing Kalman filters, one can cause the learnt prior over topics to change other time, making some topics more likely at time $t + \Delta_t$ than at time $t$, while simultaneously, using the same procedure, also permute the topic vocabularies. An example given is that the topic corresponding to neuroscience articles may assign high-probability to words such as ``movement" and low probability to ``neurone" in articles published in 1905, and yet assign high probability to ``neurone" and low probability to ``movement" in 1995.

Another example, also mentioned earlier, is the case where the two modalities are words, and latitude-longitude coordinates\cite{Eisenstein2010}. Here the task is to infer latent regions, and use these to infer region-specific vocabularies, which can then in turn be used to determine regions for the case where words but not coordinates are available.

As with the dynamic topic-model, permuted Gaussians for vocabularies can also be used, in this case a three level hierarchy of prior, global vocabularies and regional vocabularies. In published work, on held-out data with missing coordinates, this the model exhibited 24\% accuracy when identifying from which of 49 US states (Hawaii was excluded) a tweet was sent using the tweet words alone.

\subsubsection{Upstream Models}
The second approach to covariates are ``upstream" models modelling $p(\wdoc | \xd)$ which is our focus in this section. More so than with ``downstream" models, there have been many cases of ad-hoc models in the literature, with few generic approaches.

For example, the author-topic model\cite{RosenZvi2004} models the distribution $p(\wdoc | \xd)$ where $\xd$ is an author indicator. Each author has their own topic-distribution, and each word is generated by one of the document's authors, according to their corresponding topic-distribution, such that $z_{dn} | \Theta \sim \muln{\thd_{a_{dn}}}{1}$, where $a_{dn}$ is sampled uniformly from the document's authors.

An extension of this is the author-recipient topic model\cite{MacCallum2007}. There are a many possible approaches. A simple extension of the author-model is just to given each author-recipient pair its own topic distribution $\vv{\theta}_{ar}$, with the author-pair combination for each token creating by separately sampling uniformly from the authors and the recipients. This leads to an explosion of parameters however, so it's best to consider author-recipient discussions are manifested by an author's latent role $e$, with each role having it's own topic distribution $\vv{\theta}_e$.

This idea of capturing network relationships and communities by shared topics among authors and recipients has been further explored in \cite{Sachan2012} and \cite{Kang2013}.

While effective, it is preferable to have a single model that can be applied to many classes of data instead of a many ad-hoc examples. Given labelled data for example, one can associated each label with a topic, and use the label vector $\vv{t}_d \in \{0,1\}^L$ as a ``mask" over inferred topic such that $\thd \sim \dir{\vv{t} .* \vv{\alpha}}$\cite{Ramage2009} where $.*$ indicates the Hadamard product.

In practice the constraint that $L = K$ is too restrictive, generally many topics are needed to adequately model words such that typically $L < K$. To address this one can infer a label-topic matrix $A \in \{0,1\}^{L \times K}$ such that topics are sampled to $\thd \sim \dir{\vv{t}_d A \vv{\alpha}}$, where the distribution over A is represented by the product of $K \times L$ independent Bernoulli distributions (\cite{Ramage2009} again).

An interesting extension of this is to consider the labels as partially observed random variables instead of observations. A model inference procedure could then predict the absent labels. This in turn requires learning a distribution over labels. Choices range from a simple Dirichlet, to a mixture model, to even using LDA over the labels, before using the labels to affect the LDA model over words\cite{Rubin2011}. Observing that LDA can simply be viewed as multinomial PCA, this is equivalent to a multitask prediction problem where low-rank projections are learnt for both the features (the document words) and the tasks (the document labels)

This model is still limited to the case of binary label vectors only. A more general model would seek to take any vector $\xd \in \mathbb{R}$. A simple way to address this is to solve K independent regression problems such that $\thd \sim \dir{\vv{\alpha}_{d}}$ where $\alpha_{dk} = \exp(\vv{w}_k\T\vv{x}_d)$\cite{Mimno2008}. This model is known as Dirichlet Multinomial Regression (DMR)

Extensions of this model have been investigated using Gaussian processes instead of plain linear regression, using a softmax function\cite{Hennig2012} and a Laplace approximation\cite{WilliamsBarber1998} to handle the non-conjugacy in the model. Unsurprisingly, such a model does not scale to the size of datasets typical in topic models, for example the 8,000 document Reuters-21578 dataset is considered a toy, yet produces a kernel matrix with $64 \times 10^6$ elements.

An element of DMR is that the $K$ regression problems may be correlated, that is to say that the appearance of one topics may make others more or less likely. This in turn suggests the usual of a multi-task learning approach may improve inference.

\input{../footer.tex}
