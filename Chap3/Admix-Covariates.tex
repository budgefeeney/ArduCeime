\input{../header.tex}
\chapter{Admixture Models with Covariates}
In this section we investigate how to effectively incorporate covariates into admixture models. Such methods can be used to improve the fit of admixture models, or use admixture models to incorporate text into prediction problems. In particular, we consider models where topic distributions are conditioned on features which naturally falls into the family of multi-task prediction problems. We present a model using multi-task prediction and admixture modelling of discrete outputs. An application of this is the modelling of ``micro-texts", where documents have so fewer than 20 words, and so standard topic model configurations (e.g. $K=50$) would be grossly over-parameterised. We demonstrate empirical results on a dataset of tweets from Twittter

\section{Incorporating Covariates into Admixture Models}
Our setting is a set of documents $\vv{w}_d$ each of which is accompanied by some features $\vv{x}_d$. The methods of incorporating covariates into LDA fall into two categories, as described in\cite{Mimno2008}: ``upstream" models which consider the case $p(\wdoc | \xd) = \int p(\wdoc|\thd, \Phi)p(\thd|\xd) d\thd$ 
and ``downstream" models which consider the model $p(\wdoc, \xd) = \int p(\wdoc|\thd, \Phi)p(\xd|\thd, \Omega)p(\thd)d\thd$. Downstream models are sometimes known as multi-modal\cite{Virtanen2012a} or multi-field\cite{Salomatin2009} models.

\subsection{Downstream Models}
In the case where the feature $\xd$ are, like the words $\wdoc$, a set of conditionally independent, exchangeable observations, a downstream model is trivially implemented\cite{Erosheva2004}\cite{Blei2003} by sampling separate per-observation topic assignments for each ``modality" $z_{dn}^{(m)}|\thd \sim \muln{\thd}{1}$ and then sampling the observation according to a modality-specific distribution $\vv{x}_d^{(m)} \sim \muln{\vv{\phi}^{(m)}_{z^{(m)}_{dn}}}{1}$, were words are just one particular modality.

If one feels that the bulk of information is in a particular modality, one can use a variant\cite{Blei2003} where topics are sampled from the topic distribution for one modality only, and then these are resampled to generate topics for all other modalities, such that $z_{dn}^{(m)}|\vv{z}_d^{(0)} = z_{di}^{(0)}$ for $i \sim \mathcal{U}(0, n_{d\cdot\cdot}^{(0)})$. It is a simple matter to use a Dirichet Process instead of the Dirichet prior to infer the topic count\cite{Yakhnenko2009}. This model has been used with images (using bags of image ``words") and their captions\cite{Blei2003}, and paired documents in different languages \cite{Zhao2001}. The latter allows one to derive up topic-specific translations of words.

Given the links between LDA and Multinomial PCA\cite{Buntine2002}, one can see how this construction could be used to generalise CCA to the case of many heterogeneous distributions neither which are necessarily Gaussian, which has been discussed in \cite{Virtanen2012a}. 

A disadvantage of the approaches outlined thus far is that they all require the same number of topics for each modality, and even the same topics. In practice the number of set of topics used to generate, for example, words, may differ to those necessary to generate images. To address this, one can sample a large topic-strength vector $\etd \sim \nor{\vv{\mu}}{\Sigma}$ of dimension $K$ from a Gaussian instead, and partition it into disjoint subvectors $\vv{\eta}_d^{(m)}$ of length $K^{(m)}$ such that $K = \sum_m K^{(m)}$. The topic subvectors can then be used to generate observation-specific topics $z^{(m)} \sim \muln{\sigma(\vv{\eta}_d^{(m)})}{1}$. These modality-specific topics are conditionally independent given the prior, and so by learning the parameters of the prior, and particular the covariance, one can determine how they interact\cite{Salomatin2009}. As an example, two or three topics used to generate words may only correspond to one topic used to generate features.

A further complication is the fact that there are some topics which need to be totally private - i.e. which are used solely to model, say, modality-specific noise or structure. In these cases we want to encode a preference for sparse covariance matrices. Additionally, we want to be able to infer modality-specific topic counts automatically. Both of these issues can be addressed, by replacing the softmax transformation with a used the sparsifying stick-breaking construction of \cite{Paisley2012} when converting unnormalised vectors into probability vectors\cite{Virtanen2012a}.

Thus far we have considered downstream models featuring exchangeable counts, such as words in different languages and visual-words. However one needs also address the case where one of the modalities does not meet these assumptions.

For example if one of the modalities is a single, scalar variable, modelled using a generalised linear model (GLM), one can construct the feature passed to that GLM as $\bar{z}_d = \frac{1}{N_d} \sum_n \vv{z}_{dn}$, i.e the mean of the 1-of-K indicator vectors giving the per-token topic assignments in a principle modality (typically words) \cite{Blei2008}. By incorporating this in the model, instead of running separate topic-inference and GLM inference steps, one can use the second modality to affect the distribution over topics for the overall document set $(\wdoc, x)$

A special case of this is time. Given a bounded interval, one can rescale this to lie between zero and one and so sample that particular modality using a Beta distribution.\cite{Wang2006}. This induces a dependency on time in the topic posterior, thereby handling the case of corpora which span a large time-period, where the distribution over topics for one early document should differ to the that for a much later document. A competing, unbounded solution to this problem is the dynamic topic model\cite{Blei2006a}, which mutated topic priors and topic word-vocabularies over time, by using logistic-normal distributions for both, and a Kalman Filter to handle the dynamics. This allowed certain topics to fade over time, while those which remained, ``neuroscience" say, changed such that the more likely word in 1905 was ``movement", but by 1995 this had low probability in the same topic, with its position taken by ``neurone"

In the case where the two modalities are words and latitude-longitude coordinates one can infer latent regions, and use these to infer region-specific vocabularies\cite{Eisenstein2010}., which can then in turn be used to determine regions for the case where words but not coordinates are available. In this case the use of the logistic-normal distribution for text also allows word-distributions to be permuted, in this case a hierarchy of global prior, to topic to regional topic. This the model exhibited 24\% accuracy when identifying from which of 49 US states (Hawaii was excluded) a tweet was sent using the tweet words alone.

\subsection{Upstream Models}
The second approach to covariates are ``upstream" models modelling $p(\wdoc | \xd)$ which is our focus in this section. More so than with ``downstream" models, there have been many cases of ad-hoc models in the literature, with few generic approaches.

For example, the author-topic model\cite{RosenZvi2004} models the distribution $p(\wdoc | \xd)$ where $\xd$ is an author indicator. Each author has a topic-distribution $\vv{\theta}_a$, and each word is generated by one of the document's authors, such that $z_{dn} | \Theta \sim \muln{\vv{\theta}_{a_{dn}}}{1}$, where $a_{dn}$ is sampled uniformly from the document's authors.

Many extensions of this have been investigated in the author-recipient topic model\cite{MacCallum2007}. The simple approach is to give every author-recipient pair its own topic distribution $\vv{\theta}_{ar}$, and sample the pair uniformly as in the author-topic model This leads to an explosion of parameters however, so instead latent roles $e$ for each individual are introduced, with fewer roles than individuals, and with each role-pair having its own topic distribution $\vv{\theta}_e$. This idea of capturing network relationships and communities by shared topics among authors and recipients has been further explored in \cite{Sachan2012} and \cite{Kang2013}.

While these methods are effective, it is preferable to have a single model that can be applied to many classes of data instead of a many ad-hoc examples. Given discrete labelled data, one very simply approach is to associate each label with a topic, and use the label vector $\vv{t}_d \in \{0,1\}^L$ as a ``mask" over inferred topic such that $\thd \sim \dir{\vv{t} .* \vv{\alpha}}$\cite{Ramage2009} where $.*$ indicates the Hadamard product.

In practice the constraint that $L = K$ is too restrictive, generally many topics are needed to adequately model words such that typically $L < K$. To address this one can infer a label-topic matrix $A \in \{0,1\}^{L \times K}$ such that topics are sampled to $\thd \sim \dir{\vv{t}_d A \vv{\alpha}}$, where the distribution over A is represented by the product of $K \times L$ independent Bernoulli distributions (\cite{Ramage2009} again).

An interesting extension of this is to consider the labels as partially observed random variables instead of observations. A model inference procedure could then predict the absent labels. This in turn requires learning a distribution over labels. The simple choice is the Dirichlet, but one can use use mixture models or LDA to capture latent relationships between labels, and then using a replicate of the LDA model for the words. This nested LDA approach was used with some success in\cite{Rubin2011}. 

This model is still limited to the case of binary label vectors only. A more general model would seek to take any vector $\xd \in \mathbb{R}$. A simple way to address this is to solve K independent regression problems such that $\thd \sim \dir{\vv{\alpha}_{d}}$ where $\alpha_{dk} = \exp(\vv{w}_k\T\vv{x}_d)$\cite{Mimno2008}. This model is known as Dirichlet Multinomial Regression (DMR)

Extensions of this model have been investigated using Gaussian processes instead of plain linear regression, using a softmax function\cite{Hennig2012} and a Laplace approximation\cite{WilliamsBarber1998} to handle the non-conjugacy in the model. Unsurprisingly, such a model does not scale to the size of datasets typical in topic models, for example the 8,000 document Reuters-21578 dataset is considered a toy, yet produces a kernel matrix with $64 \times 10^6$ elements.

An element of DMR is that the $K$ regression problems may be correlated, that is to say that the appearance of one topics may make others more or less likely. This in turn suggests the usual of a multi-task learning approach may improve inference.

\input{../footer.tex}
