\input{../header.tex}

\section{Future Work}
\subsection{Sparsity Inducing Priors}
In the two examples given we've used sparse, binary features. Low rank covariance decompositions may not be a good fit for such methods.

With that in mind we may instead choose to sparsify the features, by learning a sparse precision matrix.

In the previous section on multi-task learning we described how using either L1 regularisation or a Laplace prior on the a vectorized precision matrix will lead to a preference for sparse solutions. An alternate approach, which is slightly more robust, and encompasses a broader range of distributions is the use of a Gaussian Scale mixture.

For example, if one places a Gaussian-Exponential prior on each dimension of a weight vectors $\vv{w}_f \sim \nor{0}{\alpha_f}$ and places an exponential prior on the variance $\tau_f \sim \mathcal{E}\left(\gamma\right)$ then the marginal distribution over $w_f$ is a zero-mean Laplace distribution, where $\gamma$ controls the degree of sparsity \cite{Figueiredo2003}.

Similarly, if one employs a Gaussian-Gamma prior instead, the resulting distribution is a Student-T distribution, but now in a form amendable to inference using EM where with a latent variable $u$

\begin{align}
u & \sim \mathcal{G}\left(\halve{\nu}, \halve{\nu} \right) ,
& \vv{z}|u \sim \nor{\vv{\mu}}{\nu \inv{\Sigma}} 
& \implies & \vv{z} & \sim \mathcal{S}\left(\vv{\mu}, \inv{\Sigma}, \nu \right)
\end{align}

As the Student-T distribution has ``heavier" tails than the Gaussian, it is more accommodating of outliers, and so replacing Gaussian likelihood with Student-T likelihoods can lead algorithms which are more ``robust" to outliers, such as PCA and CCA\cite{Archambeau2006a}, and also matrix factorization\cite{Balaji2011}.

Likewise if one places a different Gaussian-Gamma mix on every individual component of a matrix $W \in \MReal{L}{F}$ of the form
\begin{align}
w_{ij} &\sim \nor{0}{\alpha_{ij}} & \alpha_{ij} \sim \mathcal{G}\left(\frac{a}{LF}, b\right)
\end{align}
one obtains a sparsifying inference scheme\cite{Archambeau2009a} similar automatic relevance-determination (ARD)

Such a model does not directly correspond to any known distribution. If one sets $a=0$ the marginal over $w_{ij}$ is the Laplace distribution, while the marginal approaches a Normal-Jeffry's prior as both $a$ and $b$ approach zero. Finally, if $\frac{a}{LF} = b$ then one recovers a product of student-T distributions, which is also sharply peaked around zero and so sparsity inducing. The posterior distribution over the $\alpha_{ij}$ is a produce of generalised inverse Gaussian distributions.

This permits an alternative, and more robust,  approach to sparse, correlated multi-task regression than that described earlier using the Glasso approach. Placing a matrix-variate prior over the multi-task weights matrix, and then placing a Gaussian-Gamma prior on the covariance over features, allows for inference of sparse weights. However, there are a few extensions. First, if the features have a block structure (e.g. multiple 1-of-F categorical encodings), one can place priors over weights corresponding to \emph{blocks} of features $W_i$. At the same time, if there are many tasks, one can learn a \emph{low-rank} distribution over tasks, using the the same derivation of as PCA. The resulting model\cite{Archambeau2011} is:

\begin{align}
\vv{t}_n|W,\vv{x}_n & \sim \nor{W\vv{x}_n}{\sigma^2 I_L} &
V & \sim \mnor{0}{\tau I_L}{I_Q} \\
W_i | V, Z_i, \Omega_i,\alpha_i & \sim \mnor{VZ_i}{\alpha_i^{-1} \Omega_i}{I_L} &
\Omega_i & \sim \mathcal{W}^{-1}\left(\nu, \lambda I_F \right) \\
Z_i | \Omega_i & \sim \mnor{0}{\alpha_i^{-1} \Omega_i}{I_Q} &
\alpha_i & \sim\mathcal{N}^{-1}\left(\omega, \chi, \phi\right)
\end{align}

Where $Q \ll L$ is the size of the low-rank task space, and $\mathcal{N}^{-1}\left(\omega, \chi, \phi\right)$ denotes the generalized inverse Gaussian distribution with PDF
\begin{align}
p(\alpha_i) = \frac{\chi^{-\omega}\left(\sqrt{\chi \phi}\right)^\omega}{2 K_w \left(\sqrt{\chi \phi}\right)} \alpha_i^{\omega-1} e^{-\half \chi \alpha_i^{-1} + \phi \alpha_i}
\end{align}
with $K_w(\cdot)$ being a modified Bessel function of the second kind. Inference, using variational EM, is complex, with gradient based methods required to learn the parameters of the GIG distribution.

The use of a generalised inverse Gaussian (GIG) distribution as a prior over \emph{prior} Gaussian-Scale mixture means that the marginal distribution over the weight matrix -- depending on its inferred parameterisation -- can be a matrix-variate Laplace, Student-T or Gamma distribution. The identifiability issues caused by the Kronecker matrix are handled by the shrinkage priors.

This model can be extended\cite{Yang2011} by defining a \emph{matrix-variate} generalised inverse Gaussian (MGIG) distribution $G\sim \mathcal{MGIG}\left(\Psi, \Phi, \nu\right), G \in \MReal{F}{F}$
\begin{align}
\frac{|G|^{\omega - (F + 1)/2}}{|\half \Psi|^\omega K_\omega \left(\frac{1}{4}\Phi \Psi\right)} \text{etr}\left( -\half \Psi \inv{G} - \half \Phi G \right)
\end{align}
where $K_\omega(\cdot)$ is a matrix-variate Bessel function. The Wishart and inverse-Wishart distributions are a special case of the MGIG distribution. Combined with a Gaussian-scale mixture this leads to a Gaussian MGIG (GMGIG) prior over weights.
\begin{align}
W & \sim \mnor{VZ}{\Omega}{\Sigma} &
V & \sim \mnor{V_0}{\kappa_v I_P}{\Sigma} &
Z & \sim \mnor{Z_0}{\Omega}{\kappa_z I_P} \\
\Omega & \sim \mathcal{MGIG}\left( \Psi_\Omega, \Phi_\Omega, \nu_\Omega \right) &
\Sigma & \sim \mathcal{MGIG}\left( \Psi_\Sigma, \Phi_\Sigma, \nu_\Sigma \right) &
\end{align}
which as can be seen, is a prior that assumes W is of low rank, with latent features $P \ll F$. Like the GIG prior of this, depending on the parameterisation, encompasses matrix-variate Student-T, Laplace and Bessel distributions. 

With this in mind, the model could be transformed to use a sparse prior on the features covariance by incorporating a Gauss-Scale mixture prior on the features covariance.

%\fixme{This paper notes you can add a condition that $\tr{\Epsilon} \leq 1$ to avoid it going to infinity, which would have been good to know.}


\input{../footer.tex}

