%%
% Please see https://bitbucket.org/rivanvx/beamer/wiki/Home for obtaining beamer.
%%
\documentclass[xcolor=dvipsnames]{beamer} 
\usepackage{graphicx}
\usetheme{Frankfurt}
\setbeamertemplate{items}[circle]
\setbeamertemplate{bibliography item}[text]

% Auto creates the dots for each slide in a section
\usepackage{remreset}
\makeatletter
\@removefromreset{subsection}{section}
\makeatother
\setcounter{subsection}{1}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts} 
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{subfigure}
\usepackage{graphicx,xcolor}
\usepackage{pifont,mdframed}
\usepackage{tikz}
\usepackage{bm}
\usetikzlibrary{fit,positioning}


\newcommand \etr[0] {
    \text{etr}
}

\newcommand \Etr[1] {
    \text{etr}\left( { #1 } \right)
}

%
% Macros
%
\newcommand \cashort[1] { {\todo[color=yello]{#1 -- Cedric}} }
\newcommand \calong[1]  { { \todo[inline,color=yellow]{#1 -- Cedric} } }
\newcommand \gbshort[1] { {\todo[color=cyan!40]{#1 -- Guillaume}} }
\newcommand \gblong[1]  { { \todo[inline, color=cyan!40]{#1 -- Guillaume} } }
\newcommand \mgshort[1] { {\todo{#1 -- Mark}} }
\newcommand \mglong[1]  { { \todo[inline]{#1 -- Mark} } }
\newcommand \bfshort[1] { {\todo[color=green!40]{#1 -- Bryan}} }
\newcommand \bflong[1]  { { \todo[inline,color=green!40]{#1 -- Bryan} } }


% Adds a plus const to the end of a math expression
\def \pcst{+\text{const}}

% A fancy version for capital R
\def \Rcal{\mathcal{R}}

% A fancy version for r
\def \rcal{\mathbf{r}}

% Loss function / log likelihood as appropriate
\def \L{\mathcal{L}}

% KL divergence [Math Mode]
\newcommand{\kl}[2] {
	\text{KL}\left[#1||#2\right]
}

\newcommand \vecf[1] {
    \text{vec}\left(#1\right)
}

\newcommand \ent[1] {
    \text{H} \left[ #1 \right]
}

\newcommand \mut[2] {
    \text{I} \left[ #1 ; #2 \right]
}

\newcommand \dvi[2] {
    \text{D}_\text{VI} \left[ #1; #2 \right]
}

% Starts an expected value expresses [Math Mode]
\newcommand{\starte}[1] {%
	\mathbb{E}_{#1}\left[
}

% Ends an expected value expression [Math Mode]
\def \ende{\right]}

% Starts an varianc expresses [Math Mode]
\newcommand{\startv}[1] {%
	\mathbb{V}\text{ar}_{#1}\left[
}

% Ends an variance expression [Math Mode]
\def \endv{\right]}

%\newcommand \ex[2] {
%    \bigl\langle #1 \bigr\rangle_{#2}
%}
\newcommand \ex[2] {
    \mathbb{E}_{ { #2 } }\left[ #1 \right]
}
\newcommand \var[2] {
    \mathbb{V}ar_{ { #2 } }\left[ #1 \right]
}

\newcommand \halve[1] {
	\frac{#1}{2}
}

\newcommand \half {
    \halve{1}
}

\newcommand \tr { \text{tr} } 

\newcommand \T { ^\top } 

\newcommand \fixme[1] {
    {\color{red} FIXME: #1}
}

\newcommand \vv[1] { \boldsymbol #1 }

\newcommand{\mbeq}{\overset{!}{=}}

\newcommand \diag[1] { \text{diag} \left( {#1} \right) }
\newcommand \diagonal[1] { \text{diagonal} \left( {#1} \right) }

\newcommand \Ed {{ \vv{\xi}_d}}
\newcommand \Edj {{\xi_{dj}}}
\newcommand \Edk {{\xi_{dk}}}
\newcommand \AEdj {{\Lambda(\xi_{dj})}}
\newcommand \AEdk {{\Lambda(\xi_{dk})}}
\newcommand \AEd  {{ \bm{\Lambda}(\bm{\xi}_d) }}

\newcommand \Axi { { \Lambda_{\xi} } }
\newcommand \bxi { { \vv{b}_{\xi} } }
\newcommand \cxi { { c_{\xi} } }


\newcommand \wdoc      { { \vv{w}_d } }
\newcommand \wdt[0]  { { w_{dt} } }
\newcommand \wdn[0]  { { \vv{w}_{dn} } }
\newcommand \wdnt[0]  { { w_{dnt} } }
\newcommand \wdd[0]   { { \vv w_{d} } }
\newcommand \zd[0]   { { \vv z_{d} } }
\newcommand \zdn[0]  { { \vv{z}_{dn} } }
\newcommand \zdnk[0] { { z_{dnk} } }
\newcommand \zdk[0]  { { z_{dk} } }
\newcommand \thd[0]  { { \vv \theta_d } }
\newcommand \thdk[0] { { \theta_{dk} } }
\newcommand \thdj[0] { { \theta_{dj} } }
\newcommand \epow[1] { { e^{#1} } }
\newcommand \pkt     { { \phi_{kt}  } }
\newcommand \pk      { { \vv \phi_k } }
\newcommand \lmd     { { \vv \lambda_d } }
\newcommand \lmdk    { { \lambda_{dk} } }
\newcommand \xd      { { \vv x_d } }
\newcommand \atxd     { A ^\top \bm x_d}
\newcommand \axd     { A\bm x_d}
\newcommand \tsq      { { \tau^2 } }
\newcommand \ssq      { { \sigma^2 } }
\newcommand \tmsq     { { \tau^{-2} } }
\newcommand \asq      { { \alpha^2 } }
\newcommand \amsq     { { \alpha^{-2} } }
\newcommand \sgsq     { { \sigma^2 } }
\newcommand \xvec     { { \vv{x} } }
\newcommand \omk      { { \bm \omega _k } }
\newcommand \omkt     { { \omega_{kt} } }
\newcommand \oma     { { \Omega_A } }
\newcommand \gdn      { { \vv{\gamma}_{dn} } }
\newcommand \gdnk     { { \gamma_{dnk} } }
\newcommand \gdk      { { \gamma_{dk} } }
\newcommand \isigt   { { \Sigma^{-1}_{\bm \theta} } }




\newcommand \halfSig { \frac{1}{2\sigma^2} }

\newcommand \nor[2]   { \mathcal{N} \left( {#1}, {#2} \right) }
\newcommand \nord[3]   { \mathcal{N}_{#1} \left( {#2}, {#3} \right) }
\newcommand \mnor[3]  { \mathcal{N} \left(#1, #2, #3\right) }
\newcommand \norp[3]  { \mathcal{N} \left(#1; #2, #3\right) }
\newcommand \mnorp[4] { \mathcal{N} \left(#1; #2, #3, #4\right) }
\newcommand \mul[1]   { \mathcal{M} \left( {#1} \right) }
\newcommand \muln[2]  { \mathcal{M} \left( {#1},{#2} \right) }
\newcommand \dir[1]   { \mathcal{D} \left( {#1} \right) }
\newcommand \pois[1]  { \mathcal{P} \left( {#1} \right) }
\newcommand \gp[2]    { \mathcal{GP} \left( {#1}, #2 \right) }
\newcommand \dir[1]   { \mathcal{D} \left( {#1} \right) }
\newcommand \gam[2]   { \mathcal{G} \left( {#1}, {#2} \right) }
\newcommand \beta[1]  { \mathcal{B}eta \left( {#1}, {#2} \right) }

\newcommand \lne[1]  { { \ln \left( 1 + e^{ #1 } \right) } }
\newcommand \Tr[1]   { \tr \left(  {#1}  \right) }

\newcommand \roud  { \vv{\rho}_{d}  }
\newcommand \rodk { \rho_{dk} }

\newcommand \exA[1]  { \ex{#1}{q(A)} }
\newcommand \exV[1]  { \ex{#1}{q(V)} }
\newcommand \exT[1]  { \ex{#1}{q(\Theta)} }
\newcommand \extd[1] { \ex{#1}{q(\thd)} }
\newcommand \exTV[1] { \ex{#1}{q(\Theta)q(V)} }

\newcommand \Real[0]  { { \mathbb{R} } }
\newcommand \VReal[1] { { \mathbb{R}^{#1} } }
\newcommand \MReal[2] { { \mathbb{R}^{#1 \times #2} } }
\newcommand \Nat[0]  { { \mathbb{N} } }
\newcommand \VNat[1] { { \mathbb{N}^{#1} } }
\newcommand \MNat[2] { { \mathbb{N}^{#1 \times #2} } }

\newcommand \inv[1] { {#1}^{-1} }
\newcommand \invb[1] { \inv{\left( #1 \right)} }

\newcommand \cn { \textsuperscript{\texttt{[{\color{blue}Citation Needed}]}} }

\newcommand \const { { \text{c} } }

\providecommand \floor [1] { \left \lfloor #1 \right \rfloor }
\providecommand \ceil [1] { \left \lceil #1 \right \rceil }


\newcommand \vt[2] { { #1^{(#2)} } }

\newcommand \hashtag[1] { { \ttfamily \##1 } }

\newcommand \mvy  { \vv{m}_{\vv{y}} }
\newcommand \sigvy { { S_Y } }

\newcommand \mmy  { M_Y      }
\newcommand \md   { \vv{m}_d }
\newcommand \phin { \vv{\phi}_n }
\newcommand \isigma { { \inv{\Sigma} } }

\newcommand \sigv     { { \Sigma_V } }
\newcommand \isigv     { { \Sigma^{-1}_V } }

\newcommand \sigy { { \Sigma_Y } }
\newcommand \isigy { { \Sigma_{-1}_Y } }


\newcommand \omy  { { \Omega_Y } }
\newcommand \iomy { { \inv{\Omega_Y} } }

\newcommand \siga     { { \Sigma_A } }
\newcommand \isiga     { { \Sigma^{-1}_A } }
\newcommand \diagv { { \diag{\nu_1,\ldots,\nu_P} } }

\newcommand \ma { \vv{m}_a }
\newcommand \my { \vv{m}_y }

\newcommand \VoU { V \otimes U }

%\newcommand \one { \mathbb{1} }
\newcommand \one  {{  \mathds{1} }}

\newcommand \lse { \text{lse} }
%\newcommand \lse[0] { \mathrm{lse} }

% Conditional independence 
\def\ci{\perp\!\!\!\perp} % from Wikipedia



% ------ For the eval section

% Multinomial PDF [Math Mode]
% params: 1 - the variable
%         2 - the value
%         3 - the state indicator (e.g. k for a distro with K values)
%         4 - any additional subscript
\newcommand{\mpdf}[4] {
	\prod_{#3} {#1}_{{#4} {#3}} ^ {#2}
}

% Dirichlet PDF [Math Mode]
% params: 1 - the variable
%         2 - the hyper-parameter
%         3 - the state indicator (e.g. k for a distro with K values)
%         4 - any additional subscript
\newcommand{\dpdf}[4] {
	\frac{1}{B({#2})} \prod_{#3} {#1}_{{#4} {#3}} ^ {({#2}_{#3} - 1)}
}

% To simplify the sampling equations, this is indicates
% that the given value has had datapoint "m" stripped out
%
\newcommand{\lm}[1] {
	#1^{\setminus m}
}

\newcommand \model[0] {
    \mathcal{M}
}

\newcommand \perplexity[1] {
    \mathcal{P} \left( { #1 } \right)
}

\newcommand \WTrain {
    \mathcal{W}^{(t)}
}

\newcommand \WQuery {
    \mathcal{W}^{(q)}
}

\newcommand \oneover[1] {
    \frac{1}{ {#1} }
}

\newcommand \samp[1] {
    { #1 }^{(s)}
}

\newcommand \etd[0] {
    \vv{\eta}_d
}



\author{Bryan Feeney, \\
Supervisor: Dr. Ricardo Silva} 
\title[Multitask Admixture Prediction]{Multi-Task Learning of Component Strengths in Non-Conjugate Admixture Models}
\institute[University College London]{
 University College London
}
\date[July 23, 2015]{July 23, 2015}

\begin{document}

%--- the titlepage frame -------------------------%


\begin{frame}[plain]
  \titlepage
\end{frame}




%--- Research - MTL -------------------------%


\section{Introduction}
\begin{frame}{Outline of the Talk}

\begin{itemize}
    \item<1-> Introduction
    \item<2-> Background
    \only<2> {
        \begin{itemize}
            \item Latent-Variable Models of Textual Corpora
            \item Choice of Priors for Topic Models
            \item Multi-Task Learning
        \end{itemize}
    }
    \item<3-> Model
    \only<3> {
        \begin{itemize}
            \item Model Specification
            \item Variational Inference
            \item Non-Conjugate Variational Inference with Bounds
        \end{itemize}
    }
    \item<4-> Results
    \item<5->Conclusions \& Future Work
\end{itemize}


\end{frame}

%--- Admixture Modelling -------------------------%

\section{Background}

\begin{frame}{Latent Variable Models of Text}

\only<1,2> {
    \begin{figure}[t]
    \centering
    \resizebox{6cm}{!}{
        \input{Images/mom-diagram.tex}
    }
    \end{figure}
}

\only<1,2,6,7,8>{
Classical Mixture Model of Text - One topic per document
\begin{align*}
\vv{\theta} & \sim \dir{\alpha} & z_d & \sim \muln{\vv{\theta}}{1} & w_{dn} \sim \muln{\vv{\phi}_{z_d}}{1}
\end{align*}
}
\only<1,2> {
    Where each of the component vocabularies is drawn $\vv{\phi}_k \sim \dir{\vv{\beta}}$.
}

\medskip

\only<2>{
However in practice this is not what people actually do...
}

\only<3>{ \includegraphics[width=1\textwidth]{images/BookTags.png}}
\only<4>{ \includegraphics[width=1\textwidth]{images/NetflixTags.png}}
\only<5>{ \includegraphics[width=1\textwidth]{images/lastfmTags.png}}

\only<6> {
Mixture models struggle to generalise.
\begin{itemize}
    \item Fewer clusters mean coarser estimates of cluster centroids
    \item But more clusters mean fewer datapoints per cluster, and thus sparser estimates of cluster centroids, due to the assumption of one cluster per document
\end{itemize}
}



\only<7,8> {
Admixture models assign a \emph{mixture} of topics to each document\cite{BleiNgJordan2003} 

\begin{align*}
\vv{\theta}_d & \sim \dir{\alpha} & z_{dn} & \sim \muln{\vv{\theta}_d}{1} & w_{dn} \sim \mul{\vv{\phi}_{z_{dn}},1}
\end{align*}

}

\only<7> {
    \begin{figure}[t]
    \centering
    \resizebox{6cm}{!}{
        \input{Images/lda-diagram-2.tex}
    }
    \end{figure}
}

\only<9> {
    Comparing mixture and admixture models for four newsgroups from 20-news.

    \includegraphics[trim=12.3cm 7.3cm 0.5cm 7.7cm, clip=true]{Images/20news-2013-03-25.pdf}
}


\end{frame}


%--- Topic Model Research -------------------------%


\begin{frame}{Choice of Priors for Topic Models}
\only<1> { 
    The Logistic Normal Distribution
    \begin{itemize}
            \item Captures Correlations between Topics\cite{Blei2006}
        \end{itemize}

        \begin{align*}
        \vv{\theta}_d & \sim \nor{\mu}{\Sigma} & z_{dn} &\sim \muln{\sigma(\vv{\theta}_d)}{1}\\
        & & \sigma(\vv{\theta}_d) & = \frac{\exp(\theta_{dk})}{\sum_j \exp(\theta_{dj})} 
        \end{align*}
    }
    
    \only<2> { 
    The Use of Covariates: $p(\wdoc | \xd) = \int p(\wdoc|\thd) p(\thd|\xd) d\thd$
    \begin{itemize}
        \item Ad-hoc Models: topics over time\cite{Wang2006}, author-topics\cite{MacCallum2007}, regional topics\cite{Eisenstein2010}
        \item Generic approach: Dirichlet Multinomial Regression (DMR)\cite{Mimno2008}.
        \begin{align*}
        \vv{\theta}_d & \sim \dir{\vv{\alpha}_d} & \alpha_{dk} = \exp(\vv{w}_k\top\xd)
        \end{align*}

    \end{itemize} 
    }

\end{frame}


%--- ---- MTL -------------------------%


\begin{frame}{Multi-Task Learning}
Problem: Making many predictions from the same data
\begin{enumerate}
    \item Predict exam scores in $L$ subjects for several children\cite{Bonilla2008}
    \item Predict customers affinity to $L$ observed aspects of a product\cite{Allenby1999}
    \item Propose image captions by predicting $p(\text{word}|\text{image})$ from image features for every individual word (in this case $L > 10,000$)\cite{Archambeau2011}
\end{enumerate}

\medskip 
\pause

Three general approaches:\cite{Caruana1997}
\begin{enumerate}
    \item Learn correlations between tasks.
    \item Learn structure of features by how we use them - via regularization
    \item Learn a low-rank projection of the tasks themselves
\end{enumerate}
\end{frame}


%--- Regularization -------------------------%



\begin{frame}{Regularization}

\only<1->{
Learn $L$ vectors $\vv{w}_l$. How to \emph{transfer} knowledge from inferring $\vv{w}_1, \ldots, \vv{w}_{l-1}$ to the task of inferring $\vv{w}_l$
}

\medskip 

\only<1>{
    Bayesian approach - learn the prior\cite{BakkerHeskes2003}
    
    \begin{align*}
        y_{nl}|\vv{w}_l & \sim \nor{\vv{w}_l\T\vv{x}_n}{\sigma^2 I} & \qquad
        \vv{w}_l & \sim \nor{\vv{m}}{\Sigma} \\
        \vv{m}|\Sigma & \sim \nor{\vv{m}_0}{\oneover{\lambda}\Sigma} & \qquad
        \Sigma & \sim \mathcal{W}^{-1}\left(\Sigma_0, \nu\right)
    \end{align*}
    }



\only<2>{
    Low-Rank Projections of the Feature Space  
    \begin{align*}
        y_{nl}|\vv{w}_l & \sim \nor{\vv{w}_l\T\vv{x}_n}{\sigma^2 I} & \qquad
        \vv{w}_l|\vv{z} & \sim \nor{U \vv{z}_l}{\alpha^2 I} \\
        & & \qquad \vv{z}_l & \sim \nor{\vv{0}}{I} \\
        \implies \vv{w}_l & \sim \nor{\vv{0}}{\alpha^2 I + U U\T}  
    \end{align*}
    For heterogeneous tasks one can use a mixture model as a prior.
}

\end{frame}


%--- Regularization and Correlation -------------------------%

\begin{frame}{Regularization and Correlation}
Once can also take into account task correlation by setting $W = \left\{ \vv{w}_l \right\}_{l=1}^L$ (note $W \in \MReal{L}{F}$), and defining

\begin{align*}
    \vv{y}_n|W & \sim \nor{W\vv{x}_n}{\Sigma} & \vv{w}_l \sim \nor{0}{\Omega}
\end{align*}

where $\Sigma \in \MReal{L}{L}$ is the \emph{task} covariance, $\Omega \in \MReal{F}{F}$ is the \emph{feature} covariance, and these are learnt in the usual manner.


\end{frame}

%%--- Matrix Variate Prior -------------------------%
%
%\begin{frame}{Matrix-Variate Priors}
%Matrix-Variate Normal Distribution \cite{Gupta1999} :
%\begin{align*}
%W \sim \mnor{M}{\Omega}{\Sigma} & \implies \vecf{W} \sim \nor{\vecf{M}}{\Sigma \otimes \Omega}
%\end{align*}
%
%{ \fontsize{9}{11}\selectfont
%\begin{align*}
%\ln p(W) = -\halve{FL} \ln 2\pi - \halve{F}\ln |\Omega| - \halve{L} \ln | \Sigma | - \half \text{etr} \left( \inv{\Sigma} (W - M) \inv{\Omega} (W - M)\T \right)
%\end{align*}
%}
%where $\text{etr}(X) = \exp \left( \Tr{X} \right)$\\
%
%\bigskip
%\pause
%Matrix Variate Priors for Multi-Task Learning\cite{Bonilla2008}\cite{Archambeau2011}
%, infer $Y \in \MReal{D}{L}$ from $X \in \MReal{D}{F}$
%
%\begin{align*}
%Y | W & \sim \mnor{XW}{I_D}{\Sigma} & \quad W \sim \mnor{0}{\Omega}{\Sigma} \\
%Y &\sim \mnor{0}{I_D + X \Omega X\T}{\Sigma}
%\end{align*}
%Defining a multi-task Gaussian Process regression model\cite{Bonilla2008}.
%
%\end{frame}
%




%--- Research -------------------------%

\section{Model}
\begin{frame}{Motivation}
Our Model

\begin{itemize}
    \item Predicting component strengths for topic-models with the aid of observed covariates
    \item Use a matrix-variate prior over weights to jointly
    \begin{itemize}
        \item Capture correlations between topics
        \item Capture a low-rank projection of the feature-covariance      
    \end{itemize}
    \item The topic-model itself captures a low-rank representation of word-counts  
\end{itemize}

\pause
Use-Case: Micro-Documents
\begin{itemize}
    \item Use covariates to help predict topics in an otherwise over-parameterised model
    \item Use topics to expand documents: e.g. predict additional words, notably hashtags, for tweets 
\end{itemize}

\end{frame}


%--- Matrix Variate ----------------%

\begin{frame}{Matrix-Variate Normal Distribution}
\begin{align*}
X & \sim \mnor{M}{\Omega}{\Sigma} & \implies \vecf{X} & \sim \nor{\vecf{M}}{\Sigma \otimes \Omega}
\end{align*}


    {\small
        \begin{align*}
        X & \in \MReal{K}{F} &
        M & \in \MReal{K}{F} &
        \Sigma & \in \MReal{K}{K} &
        \Omega & \in \MReal{F}{F}
        \end{align*}
    }



    {\small
        \begin{align*}
        \ln p(X) = \frac{KD}{2}\ln 2\pi - \frac{K}{2}\ln |\Omega| - \frac{F}{2}\ln |\Sigma| -\half \Tr{\inv{\Sigma}(X - M)\inv{\Omega}(X - M)\T}
        \end{align*}
    }


\end{frame}



%--- Model -------------------------%

\begin{frame}{Model}

\only<1> { The Correlated Topic-Model }
\only<2> { Predicting the Mean from Features}
\only<3->{Our Model: Correlation with Low-Rank Feature Covariance}
\only<1> {
    \begin{align*}
    \vv{\theta}_d & \sim \nor{\vv{\mu}}{\Sigma} & z_{dn} & \sim \muln{\sigma(\thd)}{1} \\
    & & w_{dn} & \sim \muln{\vv{\phi}_{z_{dn}}}{1} 
    \end{align*}
}
\only<2> {
    \begin{align*}
    \vv{\theta}_d & \sim \nor{A\xd}{\Sigma} & z_{dn} & \sim \muln{\sigma(\thd)}{1} \\
    & & w_{dn} & \sim \muln{\vv{\phi}_{z_{dn}}}{1} 
    \end{align*}
}

\only<3-> {
    \begin{align*}
    V & \sim \mnor{0}{\rho^2 I_P}{\Sigma} & A|V & \sim \mnor{UV}{\alpha^2 I_F}{\Sigma} \\
    \vv{\theta}_d & \sim \nor{A\xd}{\Sigma} & z_{dn} & \sim \muln{\sigma(\thd)}{1} \\
    & & w_{dn} & \sim \muln{\vv{\phi}_{z_{dn}}}{1} 
    \end{align*}
}

\only<3> {
    \begin{align*}
    A & \sim \mnor{0}{I + UU\T}{\Sigma} 
    \end{align*}
}

\only<4> {
    \begin{itemize}
        \item Learns a low-rank covariance over features $I + UU\T$
        \item Learns correlations between topics, $\Sigma$
        \item Learns a low-rank projection of words via the vocabularies $\vv{\phi}_k$
    \end{itemize}
}

\end{frame}

\begin{frame}{Variational Inference}
\only<1-> {
    Learn a posterior $q(V,A,\Theta,Z,\Phi)$ and parameters $\Sigma, U$. \\
    \medskip
}\only<2> {
    For concave functions $f(x)$ Jensen's inequality states that
    \begin{align*}
    f \left( \ex{X}{q} \right) \geq \ex{f(X)}{q}
    \end{align*}
}
\only<2-> {
    Using Jensen's bound, we derive a bound (the ``free-energy") on the marginal log-likelihood
    \begin{align*}
    \ln p(W|X,U,\Sigma) \geq \ex{\ln p(W,X,V,A,\Theta,Z,\Phi | \Sigma, U)}{q} + \ent{q}
    \end{align*}
}
\only<3-> {
    As the posterior is not tractable, we use the ``mean-field factorization" with posteriors conjugate to the prior
    \begin{align*}
    q(V,A,\Theta,Z,\Phi) = q(V)q(A)\prod_d q(\thd)\prod_n q(z_{dn}) \prod_k q(\vv{\phi}_k)
    \end{align*}
}
\only<4-> {
    Most posterior updates follow the usual mean-field form, e.g.
    \begin{align*}
    \ln q(A) \propto \ex{\ln p(W,X,V,A,\Theta,Z,\Phi}{q(W,X,V,\Theta,Z,\Phi)}
    \end{align*}
}
\end{frame}


%--- Non-Conjugate Bounds -------------%

\begin{frame}{Non-Conjugate Inference with Quadratic Bounds}
\only<1,2> {
    \begin{align*}
    \ln \sigma(\vv{\theta}) & = \thd - \lse(\vv{\theta}) & \lse(\vv{\theta}) = \ln \sum_k \theta_{k}
    \end{align*}
}

\only<2-> {
    \begin{align*}
    \lse(\vv{\theta}) & \leq \half \vv{\theta}\T \Axi \vv{\theta} - \bxi\T\vv{\theta} + \cxi
    \end{align*}
}

\only<3> {
    Bohing Bound\cite{Bohning1988a}
    \begin{align*}
    \Axi & = \half \left( I - \frac{1}{K+1}\one \one\T \right) \\
    \bxi & = \Axi \vv{\xi} - \sigma(\vv{\xi})\\
    \cxi & = \half \vv{\xi}\T \Axi \vv{\xi} - \sigma(\vv{\xi})\T\vv{\xi} + \lse(\vv{\xi})
    \end{align*}
    
    A second order Taylor approximation where the Hessian is replaced with $B \preceq H$, where $B = \frac{1}{2}\left( I - \frac{1}{K+1}\one \one\T \right)$
}

\only<4> {
    Bouchard Bound\cite{Bouchard2007}
    \begin{align*}
    \Axi & = \diag{\frac{1}{2\xi_k} \left( \frac{1}{1 + e^{-\xi_k}} - \half\right) }_{k=1}^K  \\
    \bxi & = \half - 2 s \text{ }\diag{\Axi} \\
    \cxi & = s - \half (s + \vv{\xi})\T\one + s^2\Axi \one\T\one + \vv{\xi}\Axi\vv{\xi} + \text{vec}_k(\ln (1 + e^{\xi_k}))
    \end{align*}
Obtained by approximating the $\lse(\cdot)$ with a product of sigmoids $\lse(\vv{\theta}) \approx s + \sum_k \ln (1 + e^{s + \theta_k})$ which are in turn bounded using Jaakkola's bound\cite{Jaakkola1997}
}

\end{frame}

\begin{frame}{Algorithm}
\only<1,4>{
Variational E-Step
{\tiny
    \begin{align*}
        q(V): \quad S_V = & \Sigma &
        R_V = & \invb{\rho^{-2} I_P + \alpha^{-2}U U \T} &
        M_V = & R_Y \left( \alpha^{-2} U M_A \right) & \\
        q(A): \quad S_A = & \Sigma &
        R_A = & \invb{\alpha^{-2} I_F + \sigma^{-2} X\T X} &
        M_A = & \left(\alpha^{-2} YV + \sum_d \md \inv{\Sigma} \xd\T\right) R_A
    \end{align*}
    \begin{align*}
         q(\thd): \quad S_d = & \diag{\invb{\inv{\Sigma} + N_d \Axi}}\\
         \md = & \invb{\inv{\Sigma} + N_d \Axi} \left(\inv{\Sigma} M_A \xd  + N_d \bxi + Z^{(d)}\vv{w}_d \right )\\
        q(z_{dv}): \quad \gamma_{dvk} \propto & \exp\left(m_{dk} + \Psi(\lambda_{kv}) - \Psi(\sum_t \lambda_{kt})\right) \\ 
        q(\phi_{kv}): \quad \lambda_{kv} \propto & \beta_v + \sum_d \gamma_{dvk} w_{dv} 
    \end{align*}
}
}
\only<2> {
    M-Step
    {\small 
    \begin{align*}
        U = & \invb{K \times R_V + M_V\T \inv{\Sigma} M_V}M_V \inv{\Sigma} M_A \\
        \Sigma = & \frac{1}{P+F+D} \left(M_V M_V\T + (M_A - U V)(M_A - U V)\T \right. \\
            & \qquad \qquad \left. + \sum_d \left[ S_d + (\md - M_A \xd)(\md - M_A\xd)\T \right] \right)\\
    \end{align*}
    }
}
\only<3> {
    Bohning Bound Update
    {\small 
    \begin{align*}
        \xi_d = \md 
    \end{align*}
    }
    Bouchard Bound Updates
    {\small 
    \begin{align*}
    \Axi & = 2 \text{diag}_k
    \left(
        \frac{1}{2\Edk}\left(\frac{1}{1 + e^{\Edk}} - \half\right)
    \right) \\
    \bxi & = \half I_K - s_d \Axi \\
    \Edk = & \sqrt{m_{dk}^2 - 2s_d m_{dk} + s_d^2 + V^{(d)}_{kk}} \\
    s_d = & \frac{\frac{K}{4} - \half + \sum_k m_{dk} \AEdk}{\sum_k \AEdk}
    \end{align*}
    }
}

\end{frame}



%--- Datasets -------------------------%

\section{Results}
\begin{frame}{Datasets}

Two Datasets
Tweets from May 1st to September 30th 2013 (inclusive)
\begin{itemize}
    \item 578,408 tweets from 572 users. Vocabulary of 21,366 words
    \item Median tweet length is 10 words, total of 5,313,438 word observations across the corpus 
    \item Features are: author; time at various granularities (hour, day, week, month)
\end{itemize}

Association for Computational Linguistics papers published from 1965 to 2013
\begin{itemize}
    \item 13,554 Documents. Vocabulary of 25,477 words
    \item Median document length of 2,941 words, total of 41,009,480 words across the corpus
    \item Features are: authors; publication venue, year
\end{itemize}

\end{frame}

\begin{frame}{Evaluation Methodology}
\only<1-> {
    Perplexity
    \begin{align*}
    \mathcal{P}(W|X) = \exp \left( - \frac{\ln p(W|X)}{\sum_d n_d}   \right)
    \end{align*}
}
\only<2> {
Baseline Models
\begin{description}
\item[Tweets]: Author Topic Models: LDA and CTM with Bohning and Bouchard Bounds
\item[ACL]: Standard Topic Models: LDA and CTM with Bohning and Bouchard Bounds
\end{description}

}

\end{frame}


\begin{frame}{Tweets}
\only<1> {
    \begin{center}      \includegraphics[height=2.5in]{Images/tweet-all.eps}    \end{center}
}
\only<2> {
    {\tiny
        \resizebox{\textwidth}{!}{
            \begin{tabular}{| l | l | l | l | l | l |}
            \hline
            {\strong Tennis (RG)} &
            {\strong Tennis (Wim)} &
            {\strong US Politics (Left)} &
            {\strong US Politics (Right)} &
            {\strong Beer Fans} &
            {\strong Markets} \\
            \hline
            {\tiny set} & 	{\tiny murrai} & 	{\tiny bill} & 	{\tiny obama} & 	{\tiny beer} & 	{\tiny morn} \\
            {\tiny break} & 	{\tiny andi} & 	{\tiny white} & 	{\tiny fine} & 	{\tiny man} & 	{\tiny bui} \\
            {\tiny point} & 	{\tiny draw} & 	{\tiny hous} & 	{\tiny gop} & 	{\tiny video} & 	{\tiny updat} \\
            {\tiny serv} & 	{\tiny down} & 	{\tiny obama} & 	{\tiny listen} & 	{\tiny \#craftbeer} & 	{\tiny pleas} \\
            {\tiny match} & 	{\tiny fact} & 	{\tiny american} & 	{\tiny hous} & 	{\tiny \#beer} & 	{\tiny sign} \\
            {\tiny second} & 	{\tiny line} & 	{\tiny presid} & 	{\tiny experi} & 	{\tiny review} & 	{\tiny bank} \\
            {\tiny up} & 	{\tiny set} & 	{\tiny program} & 	{\tiny republican} & 	{\tiny brew} & 	{\tiny perfect} \\
            {\tiny winner} & 	{\tiny note} & 	{\tiny abort} & 	{\tiny senat} & 	{\tiny breweri} & 	{\tiny rais} \\
            {\tiny hold} & 	{\tiny \#wimbledon} & 	{\tiny arriv} & 	{\tiny congrat} & 	{\tiny al} & 	{\tiny million} \\
            {\tiny doubl} & 	{\tiny surpris} & 	{\tiny former} & 	{\tiny obamacar} & 	{\tiny nation} & 	{\tiny pt} \\
            {\tiny \#rg13} & 	{\tiny tough} & 	{\tiny snowden} & 	{\tiny \#tcot} & 	{\tiny releas} & 	{\tiny initi} \\
            {\tiny first} & 	{\tiny hewitt} & 	{\tiny west} & 	{\tiny stupid} & 	{\tiny fox} & 	{\tiny upgrad} \\
            {\tiny from} & 	{\tiny 1st} & 	{\tiny nsa} & 	{\tiny immigr} & 	{\tiny craft} & 	{\tiny weather} \\
            \hline
            \end{tabular}
        }
    }
    \\
    \medskip
    {\small Taken from a STM/Bohning run with K=100 and P=50}
}
\only<3> {
{\small Sample of absent hashtags whose probability $> 0.04$.} \\
\medskip 
    {\small
        \resizebox{\textwidth}{!}{
            \begin{tabular}{| l || p{9cm} | } \hline 
            {\small Hashtag} & %\bf{User (Internal)} & 
            {\small Tweet Words} \\ \hline
    {\tiny \tt \#egypt} & 	{\tiny Egyptian street party \#tamarod \#june30 http://instagram.com/p/bU51EcHEBx/} \\ \hline
    {\tiny \tt \#egypt} & 	{\tiny \#BREAKING: Clashes erupt between pro- and anti-Morsi crowds in Cairo http://f24.my/1cZMakS} \\ \hline
    %{\tiny \tt \#syria} & 	{\tiny Gunfire and explosion heard from \#Nairobi \#WestgateMall as security forces push to end siege - http://aje.me/1dBkvsx} \\ \hline
    {\tiny \tt \#news} &     {\tiny Cargill Invests \$200M in Volgograd Sunflower Oil Factory http://tmt-go.ru/484191 \#business} \\ \hline
    {\tiny \tt \#syria} & 	{\tiny Syrian official blames rebels for purported chemical weapons attack near Damascus: http://apne.ws/19MMurk -DC} \\ \hline
    {\tiny \tt \#news} & 	{\tiny Disgraced politician Bo Xilai hints at love triangle as trial ends in China: http://apne.ws/19IRzNQ -KH} \\ \hline
    {\tiny \tt \#syria} & 	{\tiny UK-based watchdog says Syrian rebels shot down a fighter jet in \#Aleppo http://aje.me/11hGLG6} \\ \hline
    {\tiny \tt \#syria} & 	{\tiny French intel says regime launched CW attack because it feared rebel offensive in Damas suburbs, wanted to protect Mezze military airport.} \\ \hline
    {\tiny \tt \#craftbeer} & 	{\tiny Enjoying Southern Tier Brewing Co. Pumking Imperial Pumpkin Ale - 2013 using @brewvault. pic.twitter.com/yxJE7nBkcF} \\ \hline
    {\tiny \tt \#gp3} & 	{\tiny Free Practice Results: http://www.gp2series.com/Results/ \#GP2 \#KeepCalmAndWin} \\ \hline
        \end{tabular}
        }
    }
}
\end{frame}

\begin{frame}{ACL}
\only<1> {
    \begin{center}      \includegraphics[height=2.5in]{Images/acl-all.eps}    \end{center}
}

\only<2> {
    {\tiny
        \resizebox{\textwidth}{!}{
            \begin{tabular}{| l | l | l | l | l | l | l | l |}
            \hline
            {\strong Parsing} &
            {\strong Topic} &
            {\strong Machine} &
            {\strong Kernel} &
            {\strong Parts Of} &
            {\strong Word-Sense}\\
             &
            {\strong Models} &
            {\strong Translation} &
            {\strong Methods} &
            {\strong Speech (POS)} &
            {\strong Disambiguation}\\
            \hline
            {\tiny pars} & 	{\tiny topic} & 	{\tiny align} & 	{\tiny kernel} & 	{\tiny tag} & 	{\tiny sens} \\
            {\tiny grammar} & 	{\tiny model} & 	{\tiny word} & 	{\tiny tree} & 	{\tiny word} & 	{\tiny word} \\
            {\tiny rule} & 	{\tiny word} & 	{\tiny model} & 	{\tiny featur} & 	{\tiny morpholog} & 	{\tiny us} \\
            {\tiny tree} & 	{\tiny document} & 	{\tiny translat} & 	{\tiny us} & 	{\tiny us} & 	{\tiny disambigu} \\
            {\tiny can} & 	{\tiny distribut} & 	{\tiny sentenc} & 	{\tiny structur} & 	{\tiny po} & 	{\tiny wordnet} \\
            {\tiny parser} & 	{\tiny each} & 	{\tiny pair} & 	{\tiny can} & 	{\tiny languag} & 	{\tiny system} \\
            {\tiny algorithm} & 	{\tiny us} & 	{\tiny english} & 	{\tiny svm} & 	{\tiny arab} & 	{\tiny wsd} \\
            {\tiny from} & 	{\tiny our} & 	{\tiny us} & 	{\tiny learn} & 	{\tiny morphem} & 	{\tiny from} \\
            {\tiny which} & 	{\tiny from} & 	{\tiny parallel} & 	{\tiny base} & 	{\tiny form} & 	{\tiny context} \\
            \hline
            \end{tabular}        
        }
    }
    \\
    \medskip
    {\small Taken from a STM/Bohning run with K=100 and P=50}
}
\end{frame}

\section{Conclusions}
\begin{frame}{Conclusions}
\begin{itemize}
    \item We have a presented a topic-model conditioned on features
    \item This model performs much better than simple author-topic models for tweets
    \item The use of a low-rank projection of the feature-space improves performance
    \item The model is not generally applicable to all datasets however: given sufficiently large documents, features may not be useful.
\end{itemize}
\end{frame}

\begin{frame}{Future Work}
    \begin{itemize}
    \item Investigate datasets with generalisable feature-sets, such as images.
    \item Investigate the ability to scale using stochastic variational optimization \cite{Hoffman2012}
    \item Investigate the low-rank decomposition of both topic and features covariances simultanesouly
    \item Investigate the compound case $p(W,Y|X) = \int p(W|\Theta) p(Y|\Theta) p(\Theta|X) d\Theta$
        \begin{itemize}
            \item For example, given a corpus of academic papers, generate words and citations, using authors and time of publication as features
        \end{itemize}
     \end{itemize}
\end{frame}


\begin{frame}

\end{frame}





%--- the Bibliography frame -------------------------%

\section{Reference}
\begin{frame}[allowframebreaks]{Reference}


\bibliographystyle{plain}
\bibliography{/Users/bryanfeeney/Documents/library.bib}


\end{frame}



\end{document}