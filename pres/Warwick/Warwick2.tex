\documentclass{sciposter}
\usepackage{lipsum}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{multicol}
\usepackage{graphicx,url}  
\usepackage[utf8]{inputenc}
\usepackage[absolute]{textpos}
\usepackage{bm}
%\usepackage{fancybullets}



\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\newcommand \etr[0] {
    \text{etr}
}

\newcommand \Etr[1] {
    \text{etr}\left( { #1 } \right)
}

\newcommand \tinymath[1] {{  \mbox{\tiny ${#1}$ }  }}

%
% Macros
%
\newcommand \cashort[1] { {\todo[color=yello]{#1 -- Cedric}} }
\newcommand \calong[1]  { { \todo[inline,color=yellow]{#1 -- Cedric} } }
\newcommand \gbshort[1] { {\todo[color=cyan!40]{#1 -- Guillaume}} }
\newcommand \gblong[1]  { { \todo[inline, color=cyan!40]{#1 -- Guillaume} } }
\newcommand \mgshort[1] { {\todo{#1 -- Mark}} }
\newcommand \mglong[1]  { { \todo[inline]{#1 -- Mark} } }
\newcommand \bfshort[1] { {\todo[color=green!40]{#1 -- Bryan}} }
\newcommand \bflong[1]  { { \todo[inline,color=green!40]{#1 -- Bryan} } }


% Adds a plus const to the end of a math expression
\def \pcst{+\text{const}}

% A fancy version for capital R
\def \Rcal{\mathcal{R}}

% A fancy version for r
\def \rcal{\mathbf{r}}

% Loss function / log likelihood as appropriate
\def \L{\mathcal{L}}

% KL divergence [Math Mode]
\newcommand{\kl}[2] {
	\text{KL}\left[#1||#2\right]
}

\newcommand{\symmkl}[2] {
	\text{KL}^{symm}\left[#1||#2\right]
}

\newcommand \vecf[1] {
    \text{vec}\left(#1\right)
}

\newcommand \ent[1] {
    \text{H} \left[ #1 \right]
}

\newcommand \mut[2] {
    \text{I} \left[ #1 ; #2 \right]
}

\newcommand \dvi[2] {
    \text{D}_\text{VI} \left[ #1; #2 \right]
}

% Starts an expected value expresses [Math Mode]
\newcommand{\starte}[1] {%
	\mathbb{E}_{#1}\left[
}

% Ends an expected value expression [Math Mode]
\def \ende{\right]}

% Starts an varianc expresses [Math Mode]
\newcommand{\startv}[1] {%
	\mathbb{V}\text{ar}_{#1}\left[
}

% Ends an variance expression [Math Mode]
\def \endv{\right]}

%\newcommand \ex[2] {
%    \bigl\langle #1 \bigr\rangle_{#2}
%}
\newcommand \ex[2] {
    \mathbb{E}_{ { #2 } }\left[ #1 \right]
}
\newcommand \var[2] {
    \mathbb{V}\text{ar}_{ { #2 } }\left[ #1 \right]
}
\newcommand \cov[1] {
    \mathbb{C}\text{ov}[ #1 ]
}

\newcommand \halve[1] {
	\frac{#1}{2}
}

\newcommand \half {
    \halve{1}
}

\newcommand \tr { \text{tr} } 

\newcommand \T { ^\top } 

\newcommand \fixme[1] {
    {\color{red} FIXME: #1}
}

\newcommand \vv[1] { \bm #1 }

\newcommand{\mbeq}{\overset{!}{=}}

\def\app#1#2{%
  \mathrel{%
    \setbox0=\hbox{$#1\sim$}%
    \setbox2=\hbox{%
      \rlap{\hbox{$#1\propto$}}%
      \lower1.1\ht0\box0%
    }%
    \raise0.25\ht2\box2%
  }%
}
\def\approxpropto{\mathpalette\app\relax}

\newcommand \diag[1] { \text{diag} \left( {#1} \right) }
\newcommand \diagonal[1] { \text{diagonal} \left( {#1} \right) }

\newcommand \Ed {{ \vv{\xi}_d}}
\newcommand \Edj {{\xi_{dj}}}
\newcommand \Edk {{\xi_{dk}}}
\newcommand \AEdj {{\Lambda(\xi_{dj})}}
\newcommand \AEdk {{\Lambda(\xi_{dk})}}
\newcommand \AEd  {{ \bm{\Lambda}(\bm{\xi}_d) }}

\newcommand \Axi { { \Lambda_{\xi} } }
\newcommand \bxi { { \vv{b}_{\xi} } }
\newcommand \cxi { { c_{\xi} } }


\newcommand \wdoc      { { \vv{w}_d } }
\newcommand \wdt[0]  { { w_{dt} } }
\newcommand \wdn[0]  { { \vv{w}_{dn} } }
\newcommand \wdnt[0]  { { w_{dnt} } }
\newcommand \wdd[0]   { { \vv w_{d} } }
\newcommand \zd[0]   { { \vv z_{d} } }
\newcommand \zdn[0]  { { \vv{z}_{dn} } }
\newcommand \zdnk[0] { { z_{dnk} } }
\newcommand \zdk[0]  { { z_{dk} } }
\newcommand \thd[0]  { { \vv \theta_d } }
\newcommand \thdk[0] { { \theta_{dk} } }
\newcommand \thdj[0] { { \theta_{dj} } }
\newcommand \epow[1] { { e^{#1} } }
\newcommand \pkt     { { \phi_{kt}  } }
\newcommand \pk      { { \vv \phi_k } }
\newcommand \lmd     { { \vv \lambda_d } }
\newcommand \lmdk    { { \lambda_{dk} } }
\newcommand \xd      { { \vv x_d } }
\newcommand \atxd     { A ^\top \bm x_d}
\newcommand \axd     { A\bm x_d}
\newcommand \tsq      { { \tau^2 } }
\newcommand \ssq      { { \sigma^2 } }
\newcommand \tmsq     { { \tau^{-2} } }
\newcommand \asq      { { \alpha^2 } }
\newcommand \amsq     { { \alpha^{-2} } }
\newcommand \sgsq     { { \sigma^2 } }
\newcommand \xvec     { { \vv{x} } }
\newcommand \omk      { { \bm \omega _k } }
\newcommand \omkt     { { \omega_{kt} } }
\newcommand \oma     { { \Omega_A } }
\newcommand \gdn      { { \vv{\gamma}_{dn} } }
\newcommand \gdnk     { { \gamma_{dnk} } }
\newcommand \gdk      { { \gamma_{dk} } }
\newcommand \isigt   { { \Sigma^{-1}_{\bm \theta} } }

\newcommand \nd { { n_{d\cdot\cdot} } }


\newcommand \halfSig { \frac{1}{2\sigma^2} }

\newcommand \nor[2]   { \mathcal{N} \left( {#1}, {#2} \right) }
\newcommand \nord[3]   { \mathcal{N}_{#1} \left( {#2}, {#3} \right) }
\newcommand \mnor[3]  { \mathcal{N} \left(#1, #2, #3\right) }
\newcommand \norp[3]  { \mathcal{N} \left(#1; #2, #3\right) }
\newcommand \mnorp[4] { \mathcal{N} \left(#1; #2, #3, #4\right) }
\newcommand \mul[1]   { \mathcal{M} \left( {#1} \right) }
\newcommand \muln[2]  { \mathcal{M} \left( {#1},{#2} \right) }
\newcommand \dir[1]   { \mathcal{D} \left( {#1} \right) }
\newcommand \pois[1]  { \mathcal{P} \left( {#1} \right) }
\newcommand \gp[2]    { \mathcal{GP} \left( {#1}, #2 \right) }
\newcommand \dir[1]   { \mathcal{D} \left( {#1} \right) }
\newcommand \gam[2]   { \mathcal{G} \left( {#1}, {#2} \right) }
\newcommand \beta[1]  { \mathcal{B}eta \left( {#1}, {#2} \right) }
\newcommand \DP[1]    { \text{DP} \left( {#1} \right) }
\newcommand \GP[2]    { \text{GP} \left( {#1}, {#2} \right) }

\newcommand \lne[1]  { { \ln \left( 1 + e^{ #1 } \right) } }
\newcommand \Tr[1]   { \tr \left(  {#1}  \right) }

\newcommand \roud  { \vv{\rho}_{d}  }
\newcommand \rodk { \rho_{dk} }

\newcommand \exA[1]  { \ex{#1}{q(A)} }
\newcommand \exV[1]  { \ex{#1}{q(V)} }
\newcommand \exT[1]  { \ex{#1}{q(\Theta)} }
\newcommand \extd[1] { \ex{#1}{q(\thd)} }
\newcommand \exTV[1] { \ex{#1}{q(\Theta)q(V)} }

\newcommand \Real[0]  { { \mathbb{R} } }
\newcommand \VReal[1] { { \mathbb{R}^{#1} } }
\newcommand \MReal[2] { { \mathbb{R}^{#1 \times #2} } }
\newcommand \Nat[0]  { { \mathbb{N} } }
\newcommand \VNat[1] { { \mathbb{N}^{#1} } }
\newcommand \MNat[2] { { \mathbb{N}^{#1 \times #2} } }

\newcommand \inv[1] { {#1}^{-1} }
\newcommand \invb[1] { \inv{\left( #1 \right)} }

\newcommand \cn { \textsuperscript{\texttt{[{\color{blue}Citation Needed}]}} }

\newcommand \const { { \text{c} } }

\providecommand \floor [1] { \left \lfloor #1 \right \rfloor }
\providecommand \ceil [1] { \left \lceil #1 \right \rceil }


\newcommand \vt[2] { { #1^{(#2)} } }

\newcommand \hashtag[1] { { \ttfamily \##1 } }

\newcommand \mvy  { \vv{m}_{\vv{y}} }
\newcommand \sigvy { { S_Y } }

\newcommand \mmy  { M_Y      }
\newcommand \md   { \vv{m}_d }
\newcommand \phin { \vv{\phi}_n }
\newcommand \isigma { { \inv{\Sigma} } }

\newcommand \sigv     { { \Sigma_V } }
\newcommand \isigv     { { \Sigma^{-1}_V } }

\newcommand \sigy { { \Sigma_Y } }
\newcommand \isigy { { \Sigma_{-1}_Y } }


\newcommand \omy  { { \Omega_Y } }
\newcommand \iomy { { \inv{\Omega_Y} } }

\newcommand \siga     { { \Sigma_A } }
\newcommand \isiga     { { \Sigma^{-1}_A } }
\newcommand \diagv { { \diag{\nu_1,\ldots,\nu_P} } }

\newcommand \ma { \vv{m}_a }
\newcommand \my { \vv{m}_y }

\newcommand \VoU { V \otimes U }

%\newcommand \one { \mathbb{1} }
\newcommand \one  {{  \mathds{1} }}

\newcommand \lse { \text{lse} }
%\newcommand \lse[0] { \mathrm{lse} }

% Conditional independence 
\def\ci{\perp\!\!\!\perp} % from Wikipedia



% ------ For the eval section

% Multinomial PDF [Math Mode]
% params: 1 - the variable
%         2 - the value
%         3 - the state indicator (e.g. k for a distro with K values)
%         4 - any additional subscript
\newcommand{\mpdf}[4] {
	\prod_{#3} {#1}_{{#4} {#3}} ^ {#2}
}

% Dirichlet PDF [Math Mode]
% params: 1 - the variable
%         2 - the hyper-parameter
%         3 - the state indicator (e.g. k for a distro with K values)
%         4 - any additional subscript
\newcommand{\dpdf}[4] {
	\frac{1}{B({#2})} \prod_{#3} {#1}_{{#4} {#3}} ^ {({#2}_{#3} - 1)}
}

% To simplify the sampling equations, this is indicates
% that the given value has had datapoint "m" stripped out
%
\newcommand{\lm}[1] {
	#1^{\setminus m}
}

\newcommand \model[0] {
    \mathcal{M}
}

\newcommand \perplexity[1] {
    \mathcal{P} \left( { #1 } \right)
}

\newcommand \WTrain {
    \mathcal{W}^{(t)}
}

\newcommand \WQuery {
    \mathcal{W}^{(q)}
}

\newcommand \oneover[1] {
    \frac{1}{ {#1} }
}

\newcommand \samp[1] {
    { #1 }^{(s)}
}

\newcommand \etd[0] {
    \vv{\eta}_d
}

\newcommand \thdo { { \vv{\theta}_{d\cdot} } }
\newcommand \thok { { \vv{\theta}_{\cdot k} } }
\newcommand \phok { { \vv{\phi}_{\cdot k} } }
\newcommand \phdo { { \vv{\phi}_{d\cdot} } }



\newtheorem{Def}{Definition}


\title{\color{white} Hierarchical Models of Document Networks}
%Título do projeto

\author{\color{white}Bryan Feeney, Ricardo Silva}
%nome dos autores

\institute 
{\color{white}University College, London}
%Nome e endereço da Instituição

\email{{\color{white}bryan.feeney,ricardo.silva},{\color{white}(@ucl.ac.uk})}
% Onde você coloca os emails dos integrantes


%\date is unused by the current \maketitle


% Exibe os logos (direita e esquerda) 
% Procure usar arquivos png ou jpg, e de preferencia mantenha na mesma pasta do .tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Begin of Document

\noleftlogo
\norightlogo

\definecolor{BoxCol}{rgb}{0.33,0.12,0.17}\definecolor{SectionCol}{rgb}{1,1,1}


\begin{document}

%define conference poster is presented at (appears as footer)

\conference{{\bf NSCML 2015}, NSCML Workshop on Autonomous Citizens: Algorithms for Tomorrow's Society}

%\LEFTSIDEfootlogo  
% Uncomment to put footer logo on left side, and 
% conference name on right side of footer

% Some examples of caption control (remove % to check result)

%\renewcommand{\algorithmname}{Algoritme} % for Dutch

%\renewcommand{\mastercapstartstyle}[1]{\textit{\textbf{#1}}}
%\renewcommand{\algcapstartstyle}[1]{\textsc{\textbf{#1}}}
%\renewcommand{\algcapbodystyle}{\bfseries}
%\renewcommand{\thealgorithm}{\Roman{algorithm}}

\maketitle
\begin{textblock}{10}(-5,-1)
\includegraphics[height=200mm]{banners/DarkRed.pdf}
\end{textblock}

%%% Begin of Multicols-Enviroment
\begin{multicols}{3}

%%% Abstract
\section{Introduction}
As they grow in size and number, the problem of how to navigate large document networks such as Wikipedia, Arxiv and others is increasingly pressing. \\

One avenue is be provided by algorithms which recommend can links for give nodes. Such algorithms may consider graph structure on its own\cite{Gopalan2013b}, but one can learn more about each node, and also predict links for new nodes in the graph, if one also takes into account the content of document-nodes. \\

Typically the latent representation which describes a node's content does not perfectly describe its linkage however. Therefore previous approaches have relied on the ad hoc addition of extra noise factors\cite{Chang2009a}\cite{Neiswanger2014}. \\

We present a model for recommending links in document networks. We eschew noise factors by using a hierarchical scheme to share information between distinct representations of document's content and linkage. We take into account edge-weights by extracting the number of times a document $p$ is linked to by document $d$. We present a deterministic inference algorithm using softmax bounds and variational Bayes which is linear in the number of network links.


%%% Introduction
\section{Matrix-Variate Normal Distribution}
Given a random matrix $X \in \MReal{D}{K} \sim \mnor{M}{\Sigma}{\Omega}$ with row covariance $\Sigma \in \MReal{K}{K}$ and column covariance $\Omega \in \MReal{D}{D}$ its log-pdf under a matrix-variate normal distribution is
\begin{align*}
\halve{DK}\ln 2\pi - \halve{D}\ln|\Sigma| - \halve{K} \ln|\Omega| -\Tr{\inv{\Omega}(X - M)\inv{\Sigma}(X - M)\T}
\end{align*}
This is mathematically identical writing, $\vecf{X} \sim \nor{\vecf{M}}{\Omega \otimes \Sigma}$; however this is far more parsimonious model than the naive approach of $\vecf{X} \sim \nor{\vecf{M}}{S}$. The separability assumption,that $S = \Omega \otimes \Sigma$ means that the covariance is approximated in the following ways:

\begin{align*}
\cov{X_{dk}, X_{pj}} & = \Omega_{dp} \Sigma_{kj} &
\cov{X_{d-}} & = \Omega_{dd} \Sigma \\
\cov{X_{-k}} & = \Sigma_{kk} \Omega 
\end{align*}


\newcommand{\imsize}{0.45\columnwidth}
%\begin{figure}
%\begin{center}
%\begin{tabular}{c c}
%
%\end{tabular}
%\end{center}
%\caption{ Parts (a) through (c) show three images consisting of squares of
%different sizes;
%(d) shows the pattern spectra, denoting the number of foreground pixels 
% removed by openings by reconstruction by $\lambda \times \lambda$ squares. No 
%granulometry is capable of separating the patterns, because the only 
%differences between the images lie in the distributions of the 
%connected components. }\label{fig:blocks}
%\end{figure}


\section{The Model}




Similar to the correlated topic model\cite{Blei2006} each document is modelled as a mixture of $K$ topics, with topic strengths denoted by $\thdo$. These are collected into a matrix $\Theta \in \MReal{D}{K}$. 
\begin{align*}
\Theta &\sim \mnor{\vv{\mu}\one\T}{\Sigma}{\alpha I_D}
\end{align*}

Denoting the softmax function as $\sigma_k(\vv{\theta}) = \frac{\theta_k}{\sum_j \theta_j}$ and the vector of softmax scores as $\vv{\sigma}(\vv{\theta})$ we 
the draw topic and then word for the $n$-th of the $N^w_d$ tokens in document $d$ as
\begin{align*}
\zdn & \sim \muln{\vv{\sigma}(\thdo)}{1} &
\wdn & \sim \muln{\vv{\beta}_{z_{dn}}}{1}
\end{align*}
And likewise for the $m$-th of the $N^l_d$ out-links in document $d$
\begin{align*}
y_{dm} & \sim \muln{\vv{\sigma}(\thdo)}{1} &
l_{dp} & \sim \muln{\vv{\phi}_{- y_{dm}}}{1}
\end{align*}

We link the probability of a document being generated according to topic $k$ with the probability of that same document generating a word or link according to topic $k$ using this prior over $\Phi$.
\begin{align*}
\Phi|\Theta & \sim \mnor{\Theta}{\Sigma}{\diag{\vv{\rho}}}
\end{align*}
where $\vv{\rho} \in \VReal{D}$ is the column covariance of all documents, and takes into account our uncertainty with regard to individual documents' probabilities of being the targets of links.


\section{Bounding the Softmax}
The log-probability of the emission probabilities for words is given by
\begin{align*}
\ex{p(Z)}{q} = \sum_d \sum_n \sum_k \zdnk \ex{\thdk}{q} - \sum_d N^w_d \text{ }\ex{\lse(\thdo)}{q}
\end{align*}
where the log-sum-exp function is $\lse(\thd) = \ln (\sum_k e^\thdk)$. This expectation cannot be evaluated analytically

Rather than use a Taylor approximation\cite{Wang2013} we choose to bound it with using the Bohning bound\cite{Bohning1988}. This guarantees convergence, and due to its quadratic form, allows for closed form updates.

\begin{align*}
\lse(\thdo) \leq \half \thdo\T A_K \thdo - \vv{b}_d\T\thdo + c_d \label{eqn:lse-def}
\end{align*}
where
\begin{align*}
A_K & = \half \left( I_K - \frac{1}{K} \one \one\T \right) \\
b_d & = A_K \Ed - \vv{\sigma}(\Ed)  \\
c_d & = \frac{1}{2} \Ed\T A_K \Ed - \vv{\sigma}(\Ed)\T\Ed + \lse(\Ed)
\end{align*}

Employing this bound, and denoting $N^o_d = N^w_d + N^l_d$ we can lower-bound the log probability of the emission topics as

\begin{equation*}
\begin{aligned}
\ex{p(X,Y)}{q} & \geq \sum_d  (\sum_n \vv{z}_{dn} + \sum_m \vv{y}_{dm}) \thdo \\
   & - \sum_d N^o_d \left(\half \thdo\T A_K \thdo - \vv{b}_d\T\thdo + c_d\right)
\end{aligned}
\end{equation*}
and derive the expectations of the linear and quadratic terms in the usual way.\\

Taking derivatives, we obtain the solution to the bound parameter is $\Ed = \ex{\thdo}{q}$. This indicates that our bound simplifies to $\ex{\lse(\thdo)}{q} \leq \lse(\ex{\thdo}{q})$, however the longer form using $A$, $\vv{b}$ and $c$ avoids the need for nested Newton-Raphson iterations.\\

The distribution over links is similarly bounded by $A_D$, $\vv{f}$ and $g$, but we can rotate it from an equation over $\vv{\phi}_k$ into an equation over $\vv{\phi}_d$ by letting $\hat{L}_{dk} = \sum_p \sum_m y_{pmk} l_{pmd}$ and $N^i = \diag{\sum_d \hat{L}_{d\cdot}}$
\begin{equation*}
\begin{aligned}
p(L|Y) & \geq \sum_k \hat{L}_{\cdot k}\T \phok  - \sum_k N^i_{kk} \left(\half \phok\T A_D \phok - \vv{f}_k\T \phok + g_k\right) \\
& = \Tr{L \Phi} - \half \Tr{A_D \Phi N^i \Phi} + \Tr{F \Phi} - \one\T G \one \\
& = \sum_d \hat{L}_{d\cdot} \phdo - \half \sum_{p \neq d} A_{dp} \vv{\phi}_{p\cdot}\T N^i \phdo \\
&-\half A_{dd} \phdo\T N^i \phdo + F_{d\cdot}\T\phdo + G_{d\cdot}
\end{aligned}
\end{equation*}

The matrix $F \in \MReal{D}{K}$ and vector $G \in \MReal{1}{K}$ simply collect the vectors $\vv{f}_k$ and scalars $g_k$. This gives us a lower bound, which we denote $\hat{p}(\Theta, Z, Y, W, L, \Phi)$  on the complete probability

\section{Variational Inference}

\newcommand \mtd { { \vv{m}^{\theta}_d } }
\newcommand \std { { S^\theta_d } }
\newcommand \mpd { { \vv{m}^{\phi}_d } }
\newcommand \spd { { S^\phi_d } }

Having lower-bounded the complete log likelihood using Bohning's bound, we can then employ an approximate posterior distribution $q(\Theta, Z, Y, \Phi, \Sigma, \vv{\rho}, \alpha)$ in conjunction with Jensen's inequality to obtain a lower-bound on the marginal log-likelihood:

\begin{align*}
\ln p(W, L) \geq \ex{\hat{p}(\Theta, Z, Y, W, L, \Phi)}{q} - \ent{q}
\end{align*}

We use the following mean-field factorisation
\begin{align*}
q(\ldots) = q(\alpha)q(\Sigma)\prod_d q(\thd)q(\vv{\phi}_d)q(\rho_d)\prod_n q(\vv{z}_{dn})q(\vv{y}_{dn}).
\end{align*}
where 
\begin{align*}
q(\thdo) &= \mathcal{N}\left(\thdo; \mtd, \std \right) &
q(\phdo) &= \mathcal{N}\left(\phdo; \mpd, \spd\right) 
\end{align*}
All other posteriors have the same functional form as the priors.\\

The factorisation is firstly tractable, since matrix-variate posteriors are unavailable for $\Theta$ and $\Phi$, but it also allows us to perform inference a document at time which in future work will facilitate online learning over larger dataset using stochastic gradient descent. \\

The distribution $q(\thdo)$ gives the probability of document $d$ originating a word or link according to topic $k$, while the probability $q(\vv{\phi}_p)$ gives the probability of document $p$ being the target of a link emitted according to topic $k$\\

Denoting the sum of expected topic assignments for document $d$ as $\hat{\vv{z}}_{d\cdot} = \sum_n \ex{\vv{z}_{dn}}{q}$ and $\hat{\vv{y}}_{d\cdot} = \sum_m \ex{\vv{y}_{dm}}{q}$ we obtain the following updates for both means:
\begin{align*}
\mtd &= \invb{ \invb{(\alpha + \rho_d)\Sigma} + N^o_d A_K } \\
    & \times
            \left(
                \invb{\rho_d \Sigma} \mpd
                + \invb{\alpha \Sigma}\vv{\mu}
                + \vv{b}_d 
                + \hat{\vv{z}}_{d\cdot}
                + \hat{\vv{y}}_{d\cdot}
            \right) \\
 \mpd & = \invb{\invb{\rho_d \Sigma} + A_{dd}N^i} \\
  & \times
             \left(
                 \invb{\rho_d \Sigma}\mtd + N^i F_{d\cdot} -\half \sum_{p \neq d} A_{dp} N^i \vv{m}^\phi_p + \hat{L}_{d\cdot}
             \right)
 \end{align*}
 
One can see the similarity between the summands in the covariance terms on the right-hand side, and the covariance of a vector in a matrix-variate distribution. Note that $\invb{A\otimes B} = \inv{A} \otimes \inv{B}$, so one expectats the precision of a matrix vector to similarly be of the form $\inv{a}\inv{B}$

\section{Experimental Results}

We use the Association for Computational Linguistics (ACL) corpus of academic papers, which we have augmented by extracting the number of times each link appears in the document text. Excluding documents with fewer than three words or two out-going links, we arrive at a corpus of 4,264 documents. The total word-count of that corpus is 14,864,709 words drawn from a dictionary of 23,769 unique terms. 

The evaluation methodology consists of removing from each document half its links, training on the full corpus, and then evaluating the rank of the held out links.

We use three ranking metrics:
\begin{description}
\item[Mean Reciprocal Rank]: $\frac{1}{D} \sum_d \frac{1}{P} \sum_p \frac{1}{r_p}$, where $r_p$ is the rank of linked document $p$ in the ordered list of links for document $d$ 
\item[Recall at M]: The number of correct links found in the top $m$ links for document $d$ as a proportion of the total number of correct links. 
\item[Precision at M]: The proportion of correct links found in the top $m$ links returned for a document $d$.
\item[Mean Average Precision]: Given the ranks of all correct links for a document, average the precision at m for m equal to each of those ranks. 
\end{description}
These all then averaged across all documents in the corpus. We additionally plot the perplexity of the observed words and links in the \emph{training} set. Perplexity is the marginal log-likelihood scaled by document length according to the formula $\exp\left(\sum_d -\ln p(\vv{w}_d,\vv{l}_d) / \sum_d N^o_d\right)$.

\includegraphics[scale=1.1]{images/RPlot.pdf}

Our results, on the ACL dataset, are given above. Perplexity is seen to improve even as ranking metrics degrade. This is likely due to the greater preponderance of words versus links.

\section{Discussion}
We have presented a model which links the emission of topics per document, to the emissions of documents per topic, and therefore shares information between these two related tasks. \\

Our model is the first, to our knowledge, which uses the \emph{incidence} rather than mere \emph{existence} of links when modelling relationships between documents. By construction it avoids the quadratic scaling problem of \cite{Chang2009a} and instead uses a deterministic inference algorithm which is linear in the number of links.\\

Our model can be used to predict links for existing and new documents, in the latter case by inferring $\thdo$ from document words. Our ranking results, on existing documents is competitive with published work, notably \cite{Chang2009a}\cite{Neiswanger2014} and we are actively experimenting with implementations of competing models and other datasets.

 
%%% References

%% Note: use of BibTeX als works!!

\bibliographystyle{plain}

\bibliography{/Users/bryanfeeney/Documents/library.bib}

\end{multicols}

\end{document}