\input{../header.tex}

\newcommand \thdo { { \vv{\theta}_{d\cdot} } }
\newcommand \thok { { \vv{\theta}_{\cdot k} } }
\newcommand \phok { { \vv{\phi}_{\cdot k} } }
\newcommand \phdo { { \vv{\phi}_{d\cdot} } }


\section{The Model}
Following the example of the correlated topic model (CTM)\cite{Blei2006} we try to capture correlations between topics. We collect the individual document topic-score vectors $\thdo$ into a matrix $\Theta \in \MReal{D}{K}$. 
\begin{align}
\Theta &\sim \mnor{\vv{\mu}\one\T}{\Sigma}{\alpha I_D}
\end{align}

Denoting the softmax function as $\sigma_k(\vv{\theta}) = \frac{\theta_k}{\sum_j \theta_j}$ and the vector of softmax scores as $\vv{\sigma}(\vv{\theta})$ we 
the topic for the $n$-th of the $N^w_d$ words in document $d$ as
\begin{align}
\zdn & \sim \muln{\vv{\sigma}(\thdo)}{1} &
\wdn & \sim \muln{\vv{\lambda}_{z_{dn}}}{1}
\end{align}
And likewise for the $m$-th of the $N^l_d$ out-links in document $d$
\begin{align}
y_{dm} & \sim \muln{\vv{\sigma}(\thdo)}{1} &
l_{dp} & \sim \muln{\vv{\phi}_{- z_{dn}}}{1}
\end{align}

For the distributions over words we simply use the Dirichlet prior such that $\vv{\ lambda}_k \sim \dir{\vv{\beta}}$ for all $k$. Were we to do likewise for the distributions over out-links, we would just have a simple multi-field (or ``multi-modal") topic-model\cite{Salomatin2009}. However we instead wish to transfer our knowledge of emission topics assigned with each document $\Theta$ to the parameter $\Phi = \{ \vv{\phi}_k\T \}_{k=1}^K$ governing the emission of documents for each topic $k$. Thus we use the following prior:

\begin{align}
\Phi|\Theta & \sim \mnor{\Theta}{\Sigma}{\diag{\vv{\rho}}}
\end{align}
where $\vv{\rho} \in \VReal{D}$ is the column covariance associated with each document. 

\subsection{Inference with Non-Conjugate Bounds}
\subsubsection*{The Matrix-Normal Distribution}
Given a random matrix $X \in \MReal{D}{K} \sim \mnor{M}{\Sigma}{\Omega}$ with row covariance $\Sigma \in \MReal{K}{K}$ and column covariance $\Omega \in \MReal{D}{D}$ its log-pdf is given by
\begin{align}
\halve{DK}\ln 2\pi - \halve{D}\ln|\Sigma| - \halve{K} \ln|\Omega| -\Tr{\inv{\Omega}(X - M)\inv{\Sigma}(X - M)\T}
\end{align}
This is mathematically identical to the following distribution over the vectorized matrix X, $\vecf{X} \sim \nor{\vecf{M}}{\Omega \otimes \Sigma}$. It should be noticed that this approach, due to the assumed separability of the covariance, is a considerably more parsimonious model than naively assuming $\vecf{X} \sim \nor{\vecf{M}}{S}$. The separability assumption, i.e. that $S = \Omega \otimes \Sigma$ means that the covariance is approximated in the following ways:

\begin{align}
\cov{X_{dk}, X_{pj}} & = \Omega_{dp} \Sigma_{kj} &
\cov{X_{d-}} & = \Omega_{dd} \Sigma &
\cov{X_{-k}} & = \Sigma_{kk} \Omega 
\end{align}

\subsubsection*{Non-Conjugate Bounds}
The log-probability of the emission probabilities for words and links is given by
\begin{align}
\ex{p(Z)}{q} = \sum_d \sum_n \sum_k \zdnk \ex{\thdk}{q} - \sum_d N^w_d \text{ }\ex{\lse(\thdo)}{q}
\end{align}
where the log-sum-exp function is $\lse(\thd) = \ln (\sum_k e^\thdk)$. As this expectation cannot be evaluated analytically, we choose to bound it with an analytically tractable expression. While Taylor series expansions have been used the past\cite{Blei2006}\cite{Wang2013a}, these involve iterating though Newton-Raphson steps which are both expensive, and not guaranteed to converge. Therefore we use the Bohning bound approximation\cite{Bohning1988} which is both guaranteed to converge, and has a quadratic form which allows for closed form updates.

The bound is given by
\begin{align}
\lse(\thdo) \leq \half \thdo\T A \thdo - \vv{b}_d\T\thdo + c_d \label{eqn:lse-def}
\end{align}
where
\begin{align}
A_K & = \half \left( I_K - \frac{1}{K} \one \one\T \right) \\
b_d & = A \Ed - \vv{\sigma}(\Ed) \\
c_d & = \frac{1}{2} \Ed\T A \Ed - \vv{\sigma}(\Ed)\T\Ed + \lse(\Ed)
\end{align}

Employing this bound, and denoting $N^o_d = N^w_d + N^l_d$ we can lower-bound the log probability of the emission topics as

\begin{align}
\ex{p(X,Y)}{q} = \sum_d  (\sum_n \vv{z}_{dn} + \sum_m \vv{y}_{dm}) \thdo
   - \sum_d N^o_d \left(\half \thdo\T A_K \thdo - \vv{b}_d\T\thdo + c_d\right)
\end{align}

Taking derivatives of the complete log-likelihood and solving for zero, we see that the solution to the bound parameter is $\Ed = \ex{\thdo}{q}$. Substituting this for $\ex{\thdo}{q}$ in \eqref{eqn:lse-def} and simplifying we see that the bound reduces to $\ex{\lse(\thdo)}{q} \leq \lse(\ex{\thdo}{q})$; however in our inference scheme we retain the longer form for analytical tractability.

Similarly, the distribution over out-links can be bounded as

\begin{align}
\ex{p(L|Y)}{q} & = \sum_p \sum_m \sum_k \sum_d y_{pmk} l_{pmd} \ex{\phi_{dk}}{q} \\
 & - \sum_p \sum_m \sum_k \sum_d y_{pmk} l_{pmd} \left(\half \ex{\phok}{q}\T A_D \ex{\phok}{q} - \vv{f}_k\T \ex{\phok}{q} + g_k\right)
\end{align}
where the bound parameter in this case $\vv{\xi}_k = \ex{\phok}{q}$. Letting $\hat{L}_{dk} = \sum_p \sum_m y_{pmk} l_{pmd}$ and the diagonal matrix $N^i = \diag{\sum_d \hat{L}_{d\cdot}}$, we can see that $N^i$ is the diagonal matrix of how often an \emph{``in-link"} is generated by topic $k$. With this notation we can write:
\begin{align}
p(L|Y) & = \sum_k \hat{L}_{\cdot k}\T \ex{\phok}{q} - \sum_k N^i_{kk} \left(\half \ex{\phok}{q}\T A_D \ex{\phok}{q} - \vv{f}_k\T \ex{\phok}{q} + g_k\right) \\
& = \ex{\Tr{L \Phi}}{q} - \half \ex{\Tr{A_D \Phi N^i \Phi}}{q} + \ex{\Tr{F \Phi}}{q} - \one\T G \one \\
& = \sum_d \hat{L}_{d\cdot} \phdo - \half \sum_{p \neq d} A_{dp} \vv{\phi}_{p\cdot}\T N^i \phdo -\half A_{dd} \phdo\T N^i \phdo + F_{d\cdot}\T\phdo + G_{d\cdot}
\end{align}

\subsubsection*{Variational Inference}
Using an approximate posterior $q(\Theta, Z, Y, \Phi, \vv{\lambda}_1, \ldots \vv{\lambda}_k)$ we can and Jensen's inequality we can lower-bound the marginal log-likelihood as

\begin{align}
\ln p(W, L) \leq \ex{p(\Theta, Z, Y, W, L, \Phi, \vv{\lambda}_1, \ldots \vv{\lambda}_k}{q} - \ent{q}
\end{align}

We use a mean-field factorization with conjugate posteriors
\begin{align}
q(\Theta, Z, Y, \Phi, \vv{\lambda}_1, \ldots \vv{\lambda}_k) = \prod_d q(\thd)q(\vv{\phi}_d)\prod_n q(\vv{z}_{dn})q(\vv{y}_{dn}) \prod_k q(\vv{\lambda}_k).
\end{align}




\section{Experiments}
The dataset is the Associated for Computational Linguistics (ACL) corpus of academic papers. Each document has associated with it a list of links to other documents in the corpus, the list of authors, the venue and the year published. Excluding all documents with fewer than three unique words, there were 13,533 documents. Excluding from that set all documents with fewer than 2 out-links resulted in a corpus of 4,264 documents. The total word-count of that corpus was 14,864,709 words drawn from a dictionary of 23,769 unique terms.




\input{../footer.tex}