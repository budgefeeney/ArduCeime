\input{../header.tex}

\newcommand \thdo { { \vv{\theta}_{d\cdot} } }
\newcommand \thok { { \vv{\theta}_{\cdot k} } }
\newcommand \phok { { \vv{\phi}_{\cdot k} } }
\newcommand \phdo { { \vv{\phi}_{d\cdot} } }


\section{The Model}
Following the example of the correlated topic model (CTM)\cite{Blei2006} we try to capture correlations between topics. We collect the individual document topic-score vectors $\thdo$ into a matrix $\Theta \in \MReal{D}{K}$. 
\begin{align}
\Theta &\sim \mnor{\vv{\mu}\one\T}{\Sigma}{\alpha I_D}
\end{align}

Denoting the softmax function as $\sigma_k(\vv{\theta}) = \frac{\theta_k}{\sum_j \theta_j}$ and the vector of softmax scores as $\vv{\sigma}(\vv{\theta})$ we 
the topic for the $n$-th of the $N^w_d$ words in document $d$ as
\begin{align}
\zdn & \sim \muln{\vv{\sigma}(\thdo)}{1} &
\wdn & \sim \muln{\vv{\lambda}_{z_{dn}}}{1}
\end{align}
And likewise for the $m$-th of the $N^l_d$ out-links in document $d$
\begin{align}
y_{dm} & \sim \muln{\vv{\sigma}(\thdo)}{1} &
l_{dp} & \sim \muln{\vv{\phi}_{- z_{dn}}}{1}
\end{align}

For the distributions over words we simply use the Dirichlet prior such that $\vv{\ lambda}_k \sim \dir{\vv{\beta}}$ for all $k$. Were we to do likewise for the distributions over out-links, we would just have a simple multi-field (or ``multi-modal") topic-model\cite{Salomatin2009}. However we instead wish to transfer our knowledge of emission topics assigned with each document $\Theta$ to the parameter $\Phi = \{ \vv{\phi}_k\T \}_{k=1}^K$ governing the emission of documents for each topic $k$. Thus we use the following prior:

\begin{align}
\Phi|\Theta & \sim \mnor{\Theta}{\Sigma}{\diag{\vv{\rho}}}
\end{align}
where $\vv{\rho} \in \VReal{D}$ is the column covariance associated with each document. 

\subsection{Inference with Non-Conjugate Bounds}
\subsubsection*{The Matrix-Normal Distribution}
Given a random matrix $X \in \MReal{D}{K} \sim \mnor{M}{\Sigma}{\Omega}$ with row covariance $\Sigma \in \MReal{K}{K}$ and column covariance $\Omega \in \MReal{D}{D}$ its log-pdf is given by
\begin{align}
\halve{DK}\ln 2\pi - \halve{D}\ln|\Sigma| - \halve{K} \ln|\Omega| -\Tr{\inv{\Omega}(X - M)\inv{\Sigma}(X - M)\T}
\end{align}
This is mathematically identical to the following distribution over the vectorized matrix X, $\vecf{X} \sim \nor{\vecf{M}}{\Omega \otimes \Sigma}$. It should be noticed that this approach, due to the assumed separability of the covariance, is a considerably more parsimonious model than naively assuming $\vecf{X} \sim \nor{\vecf{M}}{S}$. The separability assumption, i.e. that $S = \Omega \otimes \Sigma$ means that the covariance is approximated in the following ways:

\begin{align}
\cov{X_{dk}, X_{pj}} & = \Omega_{dp} \Sigma_{kj} &
\cov{X_{d-}} & = \Omega_{dd} \Sigma &
\cov{X_{-k}} & = \Sigma_{kk} \Omega \label{eqn:sep-cov-forms}
\end{align}

\subsubsection*{Non-Conjugate Bounds}
The log-probability of the emission probabilities for words is given by
\begin{align}
\ex{p(Z)}{q} = \sum_d \sum_n \sum_k \zdnk \ex{\thdk}{q} - \sum_d N^w_d \text{ }\ex{\lse(\thdo)}{q}
\end{align}
where the log-sum-exp function is $\lse(\thd) = \ln (\sum_k e^\thdk)$. As this expectation cannot be evaluated analytically, we choose to bound it with an analytically tractable expression. While Taylor series expansions have been used the past\cite{Blei2006}\cite{Wang2013a}, these involve iterating though Newton-Raphson steps which are both computationally expensive and lack convergence guarantees. Therefore we use the Bohning bound approximation\cite{Bohning1988} which is both guaranteed to converge, and has a quadratic form that allows for closed form updates.

The bound is given by
\begin{align}
\lse(\thdo) \leq \half \thdo\T A_K \thdo - \vv{b}_d\T\thdo + c_d \label{eqn:lse-def}
\end{align}
where
\begin{align}
A_K & = \half \left( I_K - \frac{1}{K} \one \one\T \right) \\
b_d & = A_K \Ed - \vv{\sigma}(\Ed) \label{eqn:bohning-b} \\
c_d & = \frac{1}{2} \Ed\T A_K \Ed - \vv{\sigma}(\Ed)\T\Ed + \lse(\Ed) \label{eqn:bohning-c}
\end{align}

Employing this bound, and denoting $N^o_d = N^w_d + N^l_d$ we can lower-bound the log probability of the emission topics as

\begin{equation}
\begin{aligned}
\ex{p(X,Y)}{q} & \geq \sum_d  (\sum_n \vv{z}_{dn} + \sum_m \vv{y}_{dm}) \ex{\thdo}{q} \\
   & - \sum_d N^o_d \left(\half \ex{\thdo}{q}\T A_K \ex{\thdo}{q} - \vv{b}_d\T\ex{\thdo}{q} + c_d\right)
\end{aligned}
\end{equation}

Taking derivatives of the complete log-likelihood and solving for zero, we see that the solution to the bound parameter is $\Ed = \ex{\thdo}{q}$. 

Substituting this for $\ex{\thdo}{q}$ in \eqref{eqn:bohning-b} and \eqref{eqn:bohning-c} we see that equation \eqref{eqn:lse-def} simplifies to $\ex{\lse(\thdo)}{q} \leq \lse(\ex{\thdo}{q})$ which is equivalent to a zero-order Delta method approximation (i.e. a zero-order Taylor series approximation around $\ex{\thdo}{q}$). However by using the longer form we avoid the need for a nested iterative inference scheme (e.g. Newton-Raphson) in determining the posterior mean of $\thdo$.

Similarly, the distribution over out-links can be bounded as

\begin{align}
\ex{p(L|Y)}{q} & \geq \sum_p \sum_m \sum_k \sum_d y_{pmk} l_{pmd} \ex{\phi_{dk}}{q} \\
 & - \sum_p \sum_m \sum_k \sum_d y_{pmk} l_{pmd} \left(\half \ex{\phok}{q}\T A_D \ex{\phok}{q} - \vv{f}_k\T \ex{\phok}{q} + g_k\right)
\end{align}
where the bound parameter in this case $\vv{\xi}_k = \ex{\phok}{q}$. Letting $\hat{L}_{dk} = \sum_p \sum_m y_{pmk} l_{pmd}$ and the diagonal matrix $N^i = \diag{\sum_d \hat{L}_{d\cdot}}$, we can see that $N^i$ is the diagonal matrix of how often an \emph{``in-link"} is generated by topic $k$. With this notation we can write:
\begin{align}
p(L|Y) & \geq \sum_k \hat{L}_{\cdot k}\T \ex{\phok}{q} - \sum_k N^i_{kk} \left(\half \ex{\phok}{q}\T A_D \ex{\phok}{q} - \vv{f}_k\T \ex{\phok}{q} + g_k\right) \\
& = \ex{\Tr{L \Phi}}{q} - \half \ex{\Tr{A_D \Phi N^i \Phi}}{q} + \ex{\Tr{F \Phi}}{q} - \one\T G \one \\
& = \sum_d \hat{L}_{d\cdot} \phdo - \half \sum_{p \neq d} A_{dp} \vv{\phi}_{p\cdot}\T N^i \phdo -\half A_{dd} \phdo\T N^i \phdo + F_{d\cdot}\T\phdo + G_{d\cdot}
\end{align}
where the matrices $F \in \MReal{D}{K}$ and $G \in \MReal{D}{K}$ simply collect the vectors $\vv{f}_k$ and $\vv{g}_k$

\subsubsection*{Variational Inference}
Having lower-bounded the complete log likelihood using Bohning's bound, we can then employ and approximate posterior distribution $q(\Theta, Z, Y, \Phi, \vv{\lambda}_1, \ldots \vv{\lambda}_k)$ in conjunction with Jensen's to obtain a lower-bound the marginal log-likelihood:

\begin{align}
\ln p(W, L) \geq \ex{\hat{p}(\Theta, Z, Y, W, L, \Phi, \vv{\lambda}_1, \ldots \vv{\lambda}_k)}{q} - \ent{q}
\end{align}

We use the following mean-field factorisation
\begin{align}
q(\Theta, Z, Y, \Phi, \vv{\lambda}_1, \ldots \vv{\lambda}_k) = \prod_d q(\thd)q(\vv{\phi}_d)\prod_n q(\vv{z}_{dn})q(\vv{y}_{dn}) \prod_k q(\vv{\lambda}_k).
\end{align}

In most cases, the approximate posterior is naturally conjugate to the prior and likelihood, and the posteriors are easily obtained. However, the case of the topic-distribution matrix $\Theta$ and the document-emission matrix $\Phi$ also require special attention as the posteriors do not have a matrix-variate Normal form: specifically while one can obtain a multivariate Normal posterior over $\vecf{\Theta}$, its covariance is not separable into a Kronecker product of two matrices. Thus we take the aggressive form of the mean-field factorisation outlined above where we consider individual rows of the matrices individually.

\newcommand \mtd { { \vv{m}^{\theta}_d } }
\newcommand \std { { S^\theta_d } }
\newcommand \mpd { { \vv{m}^{\phi}_d } }
\newcommand \spd { { S^\phi_d } }

\begin{align}
q(\Theta) &= \prod_d \mathcal{N}\left(\thdo; \mtd, \std \right) &
q(\Phi) &= \prod_d \mathcal{N}\left(\phdo; \mpd, \spd\right) 
\end{align}
and futher assume that the covariances $\std$ and $\spd$ are diagonal. Denoting the sum of expected topic assignments for document $d$ as $\hat{\vv{z}}_{d\cdot} = \sum_n \ex{\vv{z}_{dn}}{q}$ and $\hat{\vv{y}}_{d\cdot} = \sum_m \ex{\vv{y}_{dm}}{q}$ we obtain the following updates for both means:
\begin{align}
\mtd &= \invb{ \invb{(\alpha + \rho_d)\Sigma} + N^o_d A_K }
            \left(
                \invb{\rho_d \Sigma} \mpd
                + \invb{\alpha \Sigma}\vv{\mu}
                + \vv{b}_d 
                + \hat{\vv{z}}_{d\cdot}
                + \hat{\vv{y}}_{d\cdot}
            \right) \\
 \mpd & = \invb{\invb{\rho_d \Sigma} + A_{dd}N^i}
             \left(
                 \invb{\rho_d \Sigma}\mtd + N^i F_{d\cdot} -\half \sum_{p \neq d} A_{dp} N^i \vv{m}^\phi_p + \hat{L}_{d\cdot}
             \right)
 \end{align}
 
As can be expected, the first inverted term on the right-hand sides is the expression for the (full) posterior covariance. In both cases it is a sum of two terms each of which takes the form of a scalar covariance times a covariance matrix, as one would expect given the matrix-variate form. 

In the case of $\mtd$, the matrix $A_K$ is a component of the posterior row-covariance and the diagonal matrix $N^o = \diag{N^o_1, \ldots, N^o_D}$ is a component of the column covariance, while in the case of $\mpd$ these roles are reversed, due to the bound operating column-wise. Note that $A_{dd} = \frac{D-1}{2D}$ while $A_{dp} = - \oneover{2D}$

However it is not possible to express this matrix as a Kronecker product of terms involving $\alpha I_D, \diag{\vv{\rho}}$ and $N^o$ on the left-hand side and $\Sigma$ and $A_K$ on the right, which is why we cannot straightforwardly use a matrix-variate posterior.

The interpretation of these two distributions is that $q(\thd)$ is the distribution of document $d$ being the \emph{origin} of a word or link according to topic $k$, for all $k \in \{1, \ldots, K\}$, whereas $q(\vv{\phi}_d)$ is the distribution of a document being the \emph{target} of a link emitted according to topic $k$, for all $k \in \{1, \ldots, K\}$.

\begin{algorithm}
\caption{Self-Referential Algorithm}
\label{alg:sra_generic}
    \begin{align*}
        \mtd &= 
            \invb{ (\inv{\alpha} + \inv{\rho_d}) + N^o_d A_K }
            \left(
                \invb{\alpha \Sigma}\vv{\mu}
                + \invb{\rho_d \Sigma} \mpd + \vv{b}_d 
                + \sum_n \ex{\vv{z}_{dn}}{q} 
                + \sum_m \ex{\vv{y}_{dn}}{q}
            \right)
    \end{align*}
\end{algorithm}

\subsubsection*{Non-Identifiability of the prior Covariances}
Careful attention needs to be applied to the case of the prior covariances: factors in Kronecker products are inherently non-identifiable as $A \otimes B = c A \otimes \frac{1}{c} B$ for any constant $c$. In practice this means that maximum likelihood estimates of A and B can diverge below and over machine precision respectively.

In the case of maximum likelihood estimation this is addressed by means of ``flip-flop" algorithms\cite{Srivastava2009} which fix an element of one matrix to a constant value and adjust the other accordingly. In our case, we use priors to address this computation issue, drawing $\alpha$ and $\rho_d$ from the same inverse-gamma prior $\inv{\Gamma}\left(a, b\right)$ and $\Sigma \sim \inv{\mathcal{IW}}\left(\Sigma_0, \nu\right)$. This means that our estimates of the covariances are only weakly identifiable.




\section{Experiments}
The dataset is the Associated for Computational Linguistics (ACL) corpus of academic papers. Each document has associated with it a list of links to other documents in the corpus, the list of authors, the venue and the year published. Excluding all documents with fewer than three unique words, there were 13,533 documents. Excluding from that set all documents with fewer than 2 out-links resulted in a corpus of 4,264 documents. The total word-count of that corpus was 14,864,709 words drawn from a dictionary of 23,769 unique terms.

We used precision-at-m and recall-at-m as our metric. For a document $d$ this is simply the precision and recall evaluated on the top $m$ documents, ranked by their probability of being and out-link of document d, $p(l_{dp} | \thd)$. We average these across all documents. The other metric mentioned in perplexity, which is the marginal log-likelihood normalized by document length, according to 

\begin{equation}
\mathcal{P}erp(W,L) = \exp\left( - \frac{\ln p(W,L)}{\sum_d N^w_d + N^l_d} \right)
\end{equation}
Smaller perplexity scores are better.

The evaluation methodology consists of removing from each document half of its links, training on the full corpus, and then evaluating precision and recall at m for each document on the held-out links, and averaging.

The baseline models currently consist of the RTM model, which as established earlier, performs extremely poorly, and a previous version of this model, ``MTM-Old" which re-used $\Theta$ for link emissions
\begin{align}
\Theta & \sim \mnor{0}{I}{\Sigma} &
z_{dn} & \sim \muln{\vv{\sigma}(\thdo)}{1} &
w_{dn} & \sim \prod_k \muln{\vv{\lambda}_k}{1}^{z_{dnk}} \\
& &
y_{dm} & \sim \muln{\vv{\sigma}(\thdo)}{1} &
l_{dp} & \sim \prod_k \muln{\vv{\sigma}(\thok)}{1}^{y_{dmk}}
\end{align}
As noted in previous work, MTM-Old exhibits the property that the model prefers to adapt $\Theta$ to fit words rather than out-links, which is unsurprising since each document has approximate 300 words for every out-link. Consequently, as training continues, perplexity scores improved at the expense of recall and precision scores. The current model, where the emission distribution for links $\Phi$ is separate from $\Theta$ seeks to address this.

\subsubsection*{Model Variants}
I looked into initialising the $\vv{\mu}$, $\Sigma$, $\Theta$ and $\vv{\lambda}_1, \ldots, \vv{\lambda}_k$ with the output of a CTM run, and then drawing $\Phi \sim \mnor{\Theta}{\diag{\vv{\rho}}}{\Sigma}$ however while that improved perplexity it caused the precision and recall scores to degrade.

I also looked at the effect of $\alpha$ and $\vv{\rho}$. Fixing these at 1 led to poorer perplexity, precision and recall metrics. The more interesting effect however that convergence was substantially slower (taking almost twice as many iterations) with $\rho_d$ fixed at 1. This suggests that there is significant benefit to taking into account variance at the document level during training.




\input{../footer.tex}