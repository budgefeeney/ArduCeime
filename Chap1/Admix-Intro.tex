\input{../header.tex}

\subsection{Admixture Models of Text}
The Hidden Words language mode is an example of a projection of a vocabulary into a latent space, to simplify inference, and also address the issues of synonymy and polysemy. There has been considerable research into the derivation of such projections.. Typically these have involved the idea of projecting words into some low-dimensional space, where each dimension corresponds to some semantic meaning, or ``topic". 

The earliest approach is latent semantic analysis\footnote{Sometimes the term latent semantic indexing (LSI) is used, due to the original application to document retrieval. Similarly pLSA is sometimes referred to as pLSI} (LSA)\cite{Deerwester1990} which takes a $D\times T$ document-term matrix $W$ using TF-IDF encodings of words, and decomposes it using a singular value decomposition $W = USV\T$. By ranking the singular values one can identify the subspaces which account for most of the variance, which it is assumed correspond to semantically meaningful topics. A peculiarity of LSA however is that documents exhibit ``negative" scores for topics, which seems counter-intuitive. While one can imagine that a document for which the golf topic has a high score will feature golfing terms, and that a document for which the golf topic has a score of zero will feature no such terms, it's difficult to conceive of a document having a strongly \emph{negative} score for the golf topic.


%
%An alternate approach, the Hyperspace Analogue to Language\cite{Lund1996} represents a corpus by means of a matrix denoting how often two words co-occur within a sliding window which is moved one word at a time throughout all words in the corpus. Word counts are scaled inversely by the number of words between them such that an entry in the matrix is given by 
%
%\begin{equation*}
%HAL_{t,t'} = \sum_{l=0}^L w(l) n(l, t, t')
%\end{equation*}
%Where $t$ and $t'$ are distinct words, $l$ is the window length, $w(l)$ is the window-specific weight and $n(l, t, t')$ is the count of how often words $t$ and $t'$ co-occur with exactly $l$ words in-between.
%
%In general, words with similar meanings should have similar co-occurence vectors, and one can therefore use, for example, multidimensional scaling to establish semantically meaningful clusters of words\cite{Lund1996}.
%
%The HAL as defined above does not however define a probabilistic model. The following probabilistic formulation was provided by \cite{Azzopardi2005}
%
%\begin{equation*}
%p_h(t'|t) = \sum_l p(l) p(t'|t,l)
%\end{equation*}
%where $p(t'|t,l) = \frac{n(l, t, t')}{\sum_u n(l, u, t')}$ and the two choices of $p(l)$ are $\frac{1}{2L}$ or $\frac{L - l + 1}{\sum_m L - m + 1}$. In many cases smoothing techniques such as those used in language modelling are necessary to handle cases where $n(l,t,t') = 0$.
%
%\fixme{
%\begin{itemize}
%    \item How does this compare to association rules?
%    \item Tweets are small enough that the entire tweet could be a window
%    \item Single tweets have been assigned single topics in the past.
%    \item How does one \emph{generate} words from HALs?
%\end{itemize}}
%


\begin{figure}[t]
\centering
    \subfigure[]{
        \resizebox{4cm}{!}{
            \input{plots/mom-diagram.tex}
        }
    }
    \subfigure[]{
        \resizebox{4cm}{!}{
            \input{plots/lda-diagram-2.tex}
        }
    }
    \caption{Plate diagrams for (a) a mixture of multinomials and (b) latent dirichlet allocation}
\label{fig:plates}
\end{figure}

In the field of Bayesian inference, this does not arise, as topics are all given a non-zero probability. The simplest model of text is a mixture of multinomial distributions\cite{Nigam2000}:

\begin{align}
p(W) = p(\vv{\theta}|\vv{\alpha}) \prod_d \sum_{z_d} p(z_d|\vv{\theta})\prod_n p(w_{dn} | z_d, \Phi)
\end{align}
where $\vv{\theta}$ is the corpus-wide distribution over topics, $z_d$ is the single topic with which the whole of document $d$ is modelled, and $\Phi$ is the $K \times T$ matrix of $K$ topic-specific $T$-dimensional word-distributions. In practice the assumption that each document can be described by a single topic is often too restrictive. Further, it precludes better fit by increasing the number of topics, K, as topics defined by fewer training examples will have more elements of vocabulary with zero (or near-zero) probability.

Probabilistic latent semantic analysis\cite{Hofmann1999a} addresses this by enabling each word in a document to have its own topic, allowing for many topics per document, and thereby allowing better fit with greater numbers of topics:

\begin{align}
p(W) = \prod_d p(d) \prod_n \sum_{z_{dn}} p(w_{dn}|z_{dn})p(z_{dn}|d)
\end{align}
The number of parameters - the $K$ topic assignments per document - grows linearly with the size of the training set $W$, and so this model suffers severely from overfitting. In the original presentation, this was addressed by using a variant of EM known as ``tempered" EM which raises the likelihood to the power of some temperature parameter when evaluating the posterior over topic-assignments in the E-Step. Unlike the mixture of multinomials, this is not a generative model, as the topic is sampled based on $d$, an index into the training set, which makings the handling of unseen data difficult. The suggested approach is to use the fold-in heuristic which evaluates the likelihood of a new document $\vv{w}^*$ as

\begin{align}
p(\vv{w}^*|\Phi) = \sum_d p(d) \prod_n \sum_{z_{dn}} p(w_{dn}^*|z_{dn}, \Phi) p(z_{dn}|d)
\end{align}
An issue with this heuristic is that is necessarily requires that at least one document in the training set have the same topic proportions as the query document, otherwise the probability will collapse to zero, and as with the mixture of multinomials, such pathological cases will become more likely as K increases.

The Latent Dirichlet Allocation model\cite{BleiNgJordan2003} (LDA) addresses by creating a fully generative model, exploiting the exchangeability assumptions not only of words, but also documents.

\begin{align}
\thd & \sim \dir{\vv{\alpha}} & \pk & \sim \dir{\vv{\beta}} \\
z_{dn} & \sim \muln{\thd}{1} & w_{dn} & \sim \muln{\vv{\phi}_{z_{dn}}}{1}
\end{align}
For each document LDA samples a distribution $\thd$ over topics, then samples $n_{d\cdot\cdot}$ topics $z_{dn}$ from that distribution, which are then used to sample words from the appropriate topic-specific word-distribution $\vv{\phi}_{z_{dn}}$. We denote $\Phi = \{ \vv{\phi}_k\T \}_{k=1}^K$ and $\Theta = \{ \vv{\theta}_d\T \}_{d=1}^D$. The likelihood of a corpus is therefore defined as

\begin{align}
p(W | \vv{\alpha}, \vv{\beta}) = \prod_d \int \prod_n \sum_{z_{dn}} p(w_{dn}|z_{dn}, \Phi)p(z_{dn}|\thd)p(\thd|\vv{\alpha})p(\Phi|\vv{\beta}) d\thd d\Phi
\end{align}

By defining a complete generative model, LDA can make predictions on unseen documents. As shown in \cite{BleiNgJordan2003} it outperformed pLSA and mixtures of multinomials on held-out likelihood and also in prediction experiments where the inferred document-level topic distributions were used as features instead of bag-of-word vectors.

It worth nothing that in practice, the main difference between the two is regularisation: given symmetric Dirichlet priors parameterised by $\vv{\alpha} = \vv{\beta} = \vv{1}$ LDA simply defines a proper Bayesian estimator for the pLSA model\cite{GiKa2003}.

\subsubsection{Comparison with other Models}
More precisely, LDA can be described as an admixture of multinomials, in that it models a corpus as a mixture of document-level mixtures. In fact \cite{BleiNgJordan2003} was not the first time such a model had been presented,  admixtures of multinomials had been used in Bioinformatics previously\cite{Pritchard2000} though that result was little known in machine learning fields. 

The Multinomial PCA algorithm\cite{Buntine2002} is also an independent invention of the LDA model published in advance of \cite{BleiNgJordan2003} It is a word-level mixture, if one treats $\Phi$ as a parameter with no distribution one can equivalently rewrite the model as
\begin{align}
\wdoc & \sim \muln{\Phi\T\thd}{n_{d\cdot\cdot}} & \thd & \sim \dir{\vv{\alpha}}
\end{align}
from which the connection with PCA, where one typically write $x \sim \nor{W\T\vv{z}}{\sigma^2 I}$ for $\vv{z} \sim \nor{\vv{0}}{I}$, becomes clear.

Finally it is worth noting that while the presentation has used text as a motivating example, LDA can be applied to any case where an admixture of multinomials is an appropriate distribution for the observed data. In the original presentation\cite{BleiNgJordan2003} one example given was that of collaborative filtering, where users and the movie scores corresponded to documents and word-counts. Given that in an LDA model $W = \Theta \Phi$ one can see the correspondence between LDA and matrix factorization, a popular method of implementing recommender systems\cite{Salakhutdinov2007}, and indeed this correspondence has been investigated further in\cite{Agarwal2010}. In particular LDA can be seen to be a probabilistic form of non-negative matrix factorisation\cite{Lin2007}

Other examples have included interactions between users on social networks  according to certain latent communities, analoguous to words, documents and latent-topics respectively\cite{Zhang2007}, or applied to features derived from images, i.e. ``visual words", to infer latent image topics.\cite{Philbin2008}

\subsubsection{Capturing Correlation between Admixture Components}

As the Dirichlet prior assumes its components are near-independent\footnote{But not fully independent due to the requirement that samples exist on the simplex}, it does not capture correlations between topics, and so other priors have been suggested.

The correlated topic model\cite{Blei2006} (CTM) replaces the Dirichlet distribution with a logistic-normal distribution.  The logistic-normal distribution is a standard Gaussian, whose samples are mapped onto the simplex by a softmax transformation. The model therefore samples document-level mixture distributions according to
\begin{align}
\etd & \sim \nor{\vv{\mu}}{\Sigma} & \thd = \vv{\sigma}\left(\etd\right) = \left\{\frac{e^{\eta_{dk}}}{\sum_j e^{\eta_{dj}}}\right\}_{k=1}^K
\end{align}
after which the model follows as for LDA. Unlike LDA, where many implementers choose not to learn the hyper-parameters, in CTM it is essential in order to capture topic correlations. The non-conjugacy of the multinomial likelihood with the Gaussian prior makes inference complex and typically requires approximations, discussed in section \ref{sec:nonconj}

An issue with learning correlated topic-model is the quadratic growth in covariance parameters as the number of topics increase: given large numbers of topics. A straightfoward way to address this is to learnt a low-rank projection of the covariance, giving rise to the prior over topics:\cite{Putthividhya2009}:

\begin{align}
\vv{z} & \sim \nor{\vv{0}}{I} & \etd & \sim \nor{A\vv{z} + \vv{\mu}}{\Sigma_0} & \thd = \vv{\sigma}\left(\etd\right) 
\end{align}
In this case $\Sigma_0$ is a fixed hyper-parameter and correlations are learnt via the factor-analysis model. This is equivalent to CTM where $\Sigma = AA\T + \Sigma_0$, however the addition of the auxiliary variable $\vv{z}$ allows for the straightforward derivation of an inference algorithm using standard techniques. Once can also use a Laplacian prior over the latent space $\vv{z}$ to promote sparse topic-assignments\cite{Putthividhya2009}, and also to aid in the interpretability of the matrix A.

One can also explore low-rank equivalents when using Dirichlet priors, using Pachinko Allocation\cite{Li2006}. A Pachinko allocato is a tree-structured, directed, acyclic, graphical model, of which LDA is a special case,  where a single root note, corresponding to a document, gives rise to $K$ intermediate nodes, corresponding to topics, each of which in turn gives rise to $T$ observations, corresponding to words. This can be arbitrarily extended to a model of depth $m+1$ by writing:

\begin{align}
z^{(1)}_{dn} \sim & \muln{\vv{\theta}_{d,0}^{(1)}}{1} &
z^{(2)}_{dn} \sim & \muln{\vv{\theta}^{(2)}_{d, z^{(1)}_{dn}}}{1} & \ldots \quad
z^{(m)}_{dn} \sim & \muln{\vv{\theta}^{(m)}_{d, z^{(m-1)}_{dn}}}{1} \\
w_{dn} \sim & \muln{\vv{\phi}_{z^{(m)}_{dn}}}{1}
\end{align}
Clearly $w_{dn}$ can be viewed as the $(m+1)$-th instance of $z_{dn}$, particularly since all values of $\vv{\theta}$ and $\vv{\phi}_k$ all share Dirichlet priors. An advantage of this approach over CTM is that it is not limited to capturing pairwise correlations via a covariance matrix, instead it can be thought of as learning a low-rank topic-space using multinomial PCA. A disadvantage is the increase in parameters that can be learnt for each document.

\input{../footer.tex}
