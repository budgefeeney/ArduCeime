\input{../header.tex}

\subsection{Language Models of Text}

Language modelling is the problem of predicting the next word in a sequence (e.g a sentence) given all previous words. Common use-cases are predictive keyboards, spell-checkers and speech recognition. Typically one makes the Markov assumption, with first order (``bigram") and second-order (``trigram") markov models being popular.

As vocabularies are normally quite large (typically of the order of 30,000 terms), raising this to the power of two or three can lead to models that are very overparameterised: simple maximum likelihood estimates therefore will frequently include n-grams with high variance, or whose estimate simply collapses to zero in the absence of training data. The simple solution to this problem  of ``Laplace smoothing", equivalent to a symmetric Dirichlet prior, is unsatisfying in that it does not take into account the information available in the lower-order n-grams.

Most approaches instead use lower-order n-gram statistics, using information from unigrams when no information exists for bigrams and so forth. One approach is interpolation: where the probability of an n-gram is a weighted combination of the probabilities of all m-grams for $m \in \{n, (n-1), \ldots, 1\}$. Hence for a bigram we'd say its probability $p(w_i|w_{i-1}) = \lambda p(w_i|w_{i-1}) + (1 - \lambda) p(w_i)$. Ideally a different $\lambda$ is learnt for each distinct context (sequence of words $w^i_{i-n+1}$) however for performance reasons interpolation terms are bucketed according to the counts for their respective probabilities. he intuition is that probabilities defined by high counts should have low-variance so so have high weight $\lambda$. This is known as Jelinek-Mercer smoothing.

A second approach are "back-off" models, where one ``backs off" to a simpler estimate if no data exists to support a higher order estimate. Back-off models typically use a form of discounting, removing a portion of counts from frequently occuring n-grams and assigning the resultant spare probability mass to lower-order n-grams. The best performing back-off model is Knesser-Ney, which defines the probability of a word as 

The most successful language model, Knesser-Ney assigns to each unigram a ``probability" proportional to the number of \emph{distinct word sequences it follows} (often referred to as contexts) rather than the number of occurrences. 

\begin{align}
p(w_i | w_{i-1}) = \left\{ \begin{array}{lr}
     \frac{c(w_i, w_{i-1}) - D}{c(w_i-1)} & \text{if } c(w_i, w_{i-1}) > 0 \\
     \lambda(w_{i-1})\frac{ | \{t | c(t, w_i) > 0\} }{\sum_v \{v | c(v, w_i) > 0\}} & \text{otherwise}
 \end{array}
\right.
\end{align}
where $c(\cdot)$ denotes how often the word sequence was observed in the data and $\lambda(w_{i-1})$ is a normaliser that ensures that the probabilities sum to one (note that the discount $D$ on the left-hand size means that there is some probability mass to be accounted for by the left hand side). $D$ is normally learnt on held-out data via cross-validation.

An interpolated variant of Knesser-Ney was derived in\cite{Goodman2001} called ``modified Knesser-Ney" which is one of the best performing language models currently in use. This is naturally defined as

\begin{align}
p(w_i | w_{i-1}) = \frac{c(w_i, w_{i-1}) - D}{c(w_i-1)} + \lambda(w_{i-1})\frac{ | \{t | c(t, w_i) > 0\} }{\sum_v \{v | c(v, w_i) > 0\}}
\end{align}

Many other interpolation and backoff models exist, notably Katz-Smoothing, Witten-Bell Smoothing, Good-Turing Estimation and Absolute Discounting, which are all described in \cite{Goodman2001}

Probabilistic language models include the language model of \cite{MacKay1995} which uses a hierarchical Dirichlet model of text:

\begin{align}
w_i|w_{i-1} \sim \dir{\vv{\theta}_{i-1}} & \vv{theta}_i & \sim \dir{\vv{\alpha}} \forall i 
\end{align}

By learning they prior, the resulting model interpolated between unigram probability ($\vv{\alpha}$) and bigram probabilities $\Theta = \{ \vv{\theta}_i \}_{i}$. Further the update for the prior was shown to be a function of the number of unique contexts the term followed, rather than the simple Jelinek interpolation mode. Despite this, results in \cite{Teh} show that this performs worse than both interpolated and modified Knesser Ney.

One possible reason for this is that the Dirichlet distribution does not adequately capture the power-law distribution of text. Therefore, the model was amended in \cite{Teh} to use Pitman Yor processes (PYPs) instead, which, \emph{do} provide power-law distributions through the addition of a discount parameter. For a given context (i.e. a sequence of words) $\vv{w}$ of length $n = |\vv{w}|$,let $\pi(\vv{W})$ denote the prefix of length $n-1$ of $\vv{w}$. In that case the PYP language model is recursively defined as

\begin{align}
G\vv{w} & \sim \text{PYP}\left(d_{|\vv{w}|}, \alpha_{|\vv{w}|}, G_{|\vv{w}|} \right) \\
& \vdots \\
G_{\emptyset} &\sim \text{PYP}\left(d_0, \alpha_0, G_0\right)
\end{align}

with the the discount parameter $d$ and concentration parameter $\alpha$ being functions of the length of the context $\vv{w}$. When the discount $d=0$ the PYP reduces to the Dirichlet Process. A fuller description of Dirichlet processes is given in section \fixme{section}.

The resulting sampling-based inference scheme was a nested Chinese restaurant process, sometimes called a ``Chinese restaurant franchise". It can be shown that interpolated Knesser-Ney is equivalent to this model when we ensure that each word is ``served" at only one ``table" in the restaurant.\footnote{As a PYP represents an infinite number of tokens, but there are only a finite number of words in the vocabulary, a single word may be associated with more than one table in any ``restaurant"}. A high-performance variant of the PYP language model, which avoided redundant calculation via memoization was presented in \cite{Wood2011}.

A novel Bayesian approach to language modelling is the hidden words model\cite{Deschacht2012}. This posits that in addition to the observed words, with a known vocabulary, there are hidden words, with unknown vocabulary, such that each word is generated from a hidden word. The hidden vocabulary is not necessarily the same size as the observed word vocabulary. A simple language model (e.g. a second-order markov model) is used to model the hidden words, while observed words remaining conditionally independent given the hidden words that generated them. The likelihood of an observed trigram is therefore:

\begin{align}
p(\vv{w}^i_{i-2}|\vv{\phi}) = \sum_{\vv{h}^i_{i-2}} p(\vv{h}^i_{i-2}|\vv{\phi}) \prod_{n=i-2}^i p(w_n | h_n)
\end{align}

As hidden word counts presumably will never reduce to zero, there is no need for advanced interpolation techniques. In experiments on Reuters and NIPS datasets, this gave better predictive likelihood scores than modified Knesser-Ney.

\input{../footer.tex}
