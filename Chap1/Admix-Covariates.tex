\input{../header.tex}

\subsubsection{The Use of Covariates}
Often times documents $\vv{w}_d$ are accompanies by features $\vv{x}_d$ which can be used to improve inference. The methods of incorporating covariates into LDA fall into two categories, as described in\cite{Mimno2008}: ``downstream" models and ``upstream" models.

In ``downstream" models a single topic distribution $\thd$ is assigned to a document and then multiple classes of outputs such as words $\vv{w}_d$ and features $\vv{x}_d$ are generated, with most models extending to many classes of output $\vv{w}_d, \vv{x}_d^{(1)}, \ldots, \vv{x}_d^{(m)}$. These classes of model are sometimes known as ``multi-modal"\cite{Virtanen2012a} or multi-field\cite{Salomatin2009} models.

The earliest example is the CorrLDA model\cite{Blei2003} which generates one topic distribution for each document, then from that generates $n^{(m)}_{d\cdot\cdot}$ topics for each modality, and thereby generate the document and associated features. A variant only generates topics from the topic distribution for one modality, and then samples from those topics to generate topics for all other modality. An example given in the paper was using image features in conjunction with their related captions. The first variant was independently presented in \cite{Erosheva2004} for the case of scientific papers and references, and extended to the non-parameteric case in \cite{Yakhnenko2009}. An interesting variation of this was \cite{Zhao2001} in which both modalities were over words, but in different languages, the intent being to learn topic specific translations of words.

While acknowledged by neither, when one considers the relationship between LDA and Multinomial PCA\cite{Buntine2002}, one can see how this construction could be used to generalise CCA to the case of many heterogeneous distributions neither which are necessarily Gaussian, something explored in \cite{Virtanen2012a}. 

A slightly different approach is presented in \cite{Salomatin2009} where following the CTM model\cite{Blei2006}, unnormalised topic strengths are sampled from a learnt Gaussian prior instead of a Dirichlet prior. This single unnormalised vector $\etd$ is then split into $M$ disjoint segments $\vv{etd}_d^{(m)}$, one for each modality, which via a softmax transformation is converted into $M$ topic distributions $\vv{theta}_d^{(m)}$, each with $K^{(m)}$ components. This allows different numbers of topics per modality, while the covariance matrix in the prior captures correlations between different topics in these different modalities.

This particular method was then extended in \cite{Virtanen2012a}, which instead of using a softmax transformation, used the sparsifying stick-breaking construction of \cite{Paisley2012} when converting unnormalised vectors into probability vectors. An obvious advantage of this approach was the ability to specify an upper bound on topics and have the model infer the appropriate number of topics. A second advantage however was the ability to infer ``private" topics per modality, topics which have no direct correlation with topics in other modalities: an example given in discussion is that given  a dataset of Wikipedia pages and images, one may need to infer additional topics to model the page which have no corresponding topic among the image features.

A special case of these models are topics over time\cite{Wang2006}. This addresses the issue of corpora which span a large time-period, such that the distribution over topics for one early document should differ to the that for a much later document. To achieve this a time period is compressed to a $[0\ldots 1]$ interval, and then a Beta distribution over this time period is associated with each of the $K$ topics, representing the time period in which that topic's occurrence is most likely. The document is then just a tuple $(\vv{w}_d, x_d)$ with $x_d \in [0\ldots 1]$ being the scalar time-stamp jointly dependant on the topic-distribution and the learnt topic-time Beta priors.

While not strictly a ``downstream" model, or indeed an ``upstream" model, it is worth nothing that this approach was extended in the dynamic topic model of \cite{Blei2006a}. In this the prior over topics is represented by a Gaussian as in CTM, but so too are the per-topic vocabulary distributions, which a softmax transformation used to link them to the multinomial distribution over words.. By implemented Kalman filters, one can cause the learnt prior over topics to change other time, making some topics more likely at time $t + \Delta_t$ than at time $t$, while simultaneously, using the same procedure, also permute the topic vocabularies. An example given is that the topic corresponding to neuroscience articles may assign high-probability to words such as "movement" and low probability to "neurone" in articles published in 1905, and then assign high probability to "neurone" and low probability to "eye" in 1995.

A final example of a down-stream model is\cite{Eisenstein2010} which sought to use geographic information to predict not only topic, but topic vocabulary. A three-level Gaussian hierarchy was used to represent topic vocabularies: a single prior, $K$ topic-specific vocabularies, and for each topic $k$, $R$ region specific variants. The Gaussian distributions each had isotropic variances which were learnt as part of the model. 

The second approach to covariates are the ``upstream" methods. In these methods the model only generates text, but the generative story for the document-level topic-distribution is dependant on covariates. For example, in the author-topic model\cite{RosenZvi2004} each author has their own topic-distribution, and the generative story is that each word is generated by one of the documents authors, according to their corresponding topic-distribution. An extension of this is the author-recipient topic model\cite{MacCallum2007} in which a variety of ways of associating author-recipient combinations with topic-distributions were investigated. This idea of capturing network relationships and communities by shared topics among authors and recipients has been further explored in \cite{Sachan2012} and \cite{Kang2013}.

While downstream models have been generalisable by design, the upstream models are mostly ad-hoc. An early attempt to define a generalised model was LabelledLda\cite{Ramage2009}. As the name suggests, the binary vector $\vv{t}_d \in \{0,1\}^L$ indicated which of $L$ labels had been assigned to the document. The model defined a matrix $A \in \{0,1\}^{K \times L}$, where the distribution over A was represented by the product of $K \times L$ Bernoulli distributions, which was designed to turn on and off certain topics. The topic distribution for a document was then sampled according to $\thd \sim \dir{A\vv{t}_d}$. A simpler variant of this model, called FlatLDA in\cite{Rubin2011} just uses a one-to-one correspondence between labels and topics, eliminating the need to learn A.

A more flexible model model was Dirichlet Multinomial Regression\cite{Mimno2008}. Given a feature vector $\vv{x}_d$ each document's topic distribution was sampled from $\thd \sim \dir{\vv{\alpha}_{d}}$ where $\alpha_{dk} = \exp(\vv{w}_k\T\vv{x}_d)$, with the rest of the model following normally.

A variation of this was the Kernel Topic Model\cite{Hennig2012} where a Gaussian Process was used to predict topic-strengths, which were then converted to probabilties using a softmax transformation. Inference followed \cite{WilliamsBarber1998} in using a Laplace approximation to handle the non-conjugacy in the model. Unsurprisingly, they experienced significant difficulty scaling this to the size of datasets typical in topic models, where the 8,000 document Reuters-21578 dataset is considered a toy, yet produces a 64,000,000 element kernel matrix.

This section thus far has been focussed on observed covariates used to improve assignment of topics, oftentimes with a view to generating text. One can also consider the case where one has to predict one or many labels given discrete observations such as words in a document, with LDA used to create low-rank representations to improve learning. The earliest approach is a ``downstream" model, SupervisedLDA\cite{Blei2008}, which, for a regression problem estimating $t_d|\vv{w}_d \in \Real$, learns topics $\vv{z}_{dn}$ for the $n_{d\cdot\cdot}$ words in $\vv{w}_d$, then makes a prediction using $t = \vv{w}\T\left(\oneover{n_{d\cdot\cdot}}\sum_n \vv{z}_{dn}\right)$. This can be extended to categorical predictions in the usual way by employing the framework of generalised linear models.

The most recent approach is \cite{Rubin2011}, which extended LabelledLDA\cite{Ramage2009} by learning a prior over labels. The simplest approach, PriorLDA, placed a Dirichlet prior over labels, which then had a corresponding Multinomial distribution, with the hyperparameters of the Dirichlet learnt from the data. The more complex DependencyLDA, models the labels using LDA, then given the labels proceed to model the text using LDA in turn. Observing that LDA can simply be viewed as multinomial PCA, this is equivalent to a multitask prediction problem where low-rank projections are learnt for both the features (the document words) and the tasks (the document labels)

\input{../footer.tex}
