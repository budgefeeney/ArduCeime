\input{../header.tex}

\subsubsection{The Use of Covariates}
Oftentimes documents $\vv{w}_d$ are accompanied by features $\vv{x}_d$ which can be used to improve inference. The methods of incorporating covariates into LDA fall into two categories, as described in\cite{Mimno2008}: ``downstream" models and ``upstream" models.

In ``downstream" models a single topic distribution $\thd$ is assigned to a document and then multiple classes of outputs such as words $\vv{w}_d$ and features $\vv{x}_d$ are generated, with most models extending to many classes of output $\vv{w}_d, \vv{x}_d^{(1)}, \ldots, \vv{x}_d^{(m)}$. These classes of model are sometimes known as multi-modal\cite{Virtanen2012a} or multi-field\cite{Salomatin2009} models.

The earliest example is the CorrLDA model\cite{Blei2003} which generates one topic distribution for each document, then from that generates topics $z_{dn}^{(m)}|\thd \sim \muln{\thd}{1}$ for all $n_{d\cdot\cdot}^{(m)}$ in each modality, identifying the component distribution from which the $n$-th observation is generated.

A variant also presented in\cite{Blei2003} generates topics from the topic distribution for one modality only, and then samples from those topics to generate topics for all other modality, such that $z_{dn}^{(m)}|\vv{z}_d^{(0)} = z_{di}^{(0)}$ for $i \sim \mathcal{U}(0, n_{d\cdot\cdot}^{(0)})$. An example given in the paper is using image features in conjunction with their related captions. The first variant was independently presented in \cite{Erosheva2004} for the case of scientific papers and references, and extended to the non-parameteric case in \cite{Yakhnenko2009}. An interesting variation of this is \cite{Zhao2001} in which both modalities are over words, but in different languages, the intent being to learn topic specific translations of words.

While acknowledged by neither, when one considers the relationship between LDA and Multinomial PCA\cite{Buntine2002}, one can see how this construction could be used to generalise CCA to the case of many heterogeneous distributions neither which are necessarily Gaussian, something discussed in \cite{Virtanen2012a}. 

A slightly different approach is presented in \cite{Salomatin2009} where following the CTM model\cite{Blei2006}, unnormalised topic strengths are sampled from a learnt Gaussian prior instead of a Dirichlet prior. This single unnormalised vector $\etd$ is then split into $M$ disjoint segments $\vv{\eta}_d^{(m)}$, one for each modality, which via a softmax transformation is converted into $M$ topic distributions $\vv{\theta}_d^{(m)}$, each with $K^{(m)}$ components. This allows different numbers of topics per modality, while the covariance matrix in the prior captures correlations between different topics in these different modalities.

This particular method has been then extended in \cite{Virtanen2012a}, which instead of using a softmax transformation, used the sparsifying stick-breaking construction of \cite{Paisley2012} when converting unnormalised vectors into probability vectors. An obvious advantage of this approach is the ability to specify an upper bound on topics and have the model infer the appropriate number of topics. A second advantage however is the ability to infer ``private" topics per modality, topics which have no direct correlation with topics in other modalities. An example given in discussion is a dataset of Wikipedia pages and images: one may need to infer additional topics to capture the distribution of words in the page which have no corresponding topics among the distributions over image features.

A special case of ``downstream" models, where there are two modalities, of which the second has only one observation, is the topics over time model\cite{Wang2006}. This addresses the issue of corpora which span a large time-period, where the distribution over topics for one early document should differ to the that for a much later document. To achieve this a time period is compressed to a $[0\ldots 1]$ interval, and then a Beta distribution over this time period is associated with each of the $K$ topics, representing the time period in which that topic's occurrence is most likely. The document is then just a tuple $(\vv{w}_d, x_d)$ with $x_d \in [0\ldots 1]$ being the scalar time-stamp jointly dependant on the topic-distribution and the learnt topic-time Beta priors.

While not strictly a ``downstream" model, or indeed an ``upstream" model, it is worth nothing that this approach was extended in the dynamic topic model of \cite{Blei2006a}. In this the prior over topics is represented by a logistic-normal distribution, as in CTM, but so too are the per-topic vocabulary distributions. By implementing Kalman filters, one can cause the learnt prior over topics to change other time, making some topics more likely at time $t + \Delta_t$ than at time $t$, while simultaneously, using the same procedure, also permute the topic vocabularies. An example given is that the topic corresponding to neuroscience articles may assign high-probability to words such as ``movement" and low probability to ``neurone" in articles published in 1905, and yet assign high probability to ``neurone" and low probability to ``movement" in 1995.

A final example of a down-stream model is\cite{Eisenstein2010} where the inputs are words and a latitude-longitude coordinate pair. The model uses to use geographic information to predict not only topic, but region-specific variants of topic-vocabularies. A three-level Gaussian hierarchy is used to represent topic vocabularies: a single Gaussian prior, $K$ topic-specific Gaussian vocabularies, and then for each topic $k$, $R$ region-specific Gaussian variants. This is used as a prior for the multinomial word observations by means of a softmax transform. The Gaussian distributions each have isotropic variances which are learnt as part of the model. The model additionally attempts to cluster X-Y coordinates into latent regions. On held-out data, with missing coordinates, the model exhibited 24\% accuracy when identifying from which of 49 US states (Hawaii was excluded) a tweet was sent.

The second approach to covariates are ``upstream" models. In these methods the model only generates text, but in the generative story, the document-level topic-distribution is dependant on the associated covariates. For example, in the author-topic model\cite{RosenZvi2004} each author has their own topic-distribution, and the generative story is that each word is generated by one of the document's authors, according to their corresponding topic-distribution, hence $z_{dn} | \Theta \sim \muln{\thd_{a_{dn}}}{1}$, where $a_{dn}$ is sampled uniformly from the document's authors.

An extension of this is the author-recipient topic model\cite{MacCallum2007} in which a variety of ways of associating author-recipient combinations with topic-distributions were investigated. One posited that every possibly author-recipient pair had their own topic distribution $\vv{\theta}_{ar}$, with the author-pair combination for each token creating by separately sampling uniformly from the authors and the recipients. A modification assumed created a topic-distribution $\vv{\theta}_o$ for each \emph{role}, $o$, with the role itself being sampled on a distribution over $R$ roles that every author-recipient pair could take. 

This idea of capturing network relationships and communities by shared topics among authors and recipients has been further explored in \cite{Sachan2012} and \cite{Kang2013}.

While downstream models have been generalisable by design, the upstream models described thus far have been ad-hoc. An early attempt to define a generalised model was LabelledLda\cite{Ramage2009}. As the name suggests, it is used in the case of labelled data, where a vector $\vv{t}_d \in \{0,1\}^L$ indicates which of $L$ labels have been assigned to a document. In the case where $L = K$, the topics are sampled from $\thd \sim \dir{\vv{t} .* \vv{\alpha}}$ where $.*$ indicates an elementwise product. Essentially this just turns off certain topics. This was named FlatLDA in\cite{Rubin2011}

To create a more general model where $L \neq K$, and in particular where $L < K$, a variant was defined were a matrix $A \in \{0,1\}^{L \times K}$ is used to sample the topics according to $\thd \sim \dir{\vv{t}_d A \vv{\alpha}}$, where the distribution over A is represented by the product of $K \times L$ independent Bernoulli distributions.

A simpler and more flexible model model is Dirichlet Multinomial Regression\cite{Mimno2008}. Given a feature vector $\vv{x}_d$ each document's topic distribution is sampled from $\thd \sim \dir{\vv{\alpha}_{d}}$ where $\alpha_{dk} = \exp(\vv{w}_k\T\vv{x}_d)$, with the rest of the model following normally. Inference follows a Gibbs-EM strategy, with a sampling step was used to infer the topic distributions for each token $n$ in document $d$ given $\vv{x}_d$ and $\vv{w}_1, \ldots \vv{w}_K$ ($\thd$ was collapsed out), which in turn were used to in an M-Step to update the weight parameters and topic-specific distributions.

A variation of this is the Kernel Topic Model\cite{Hennig2012} where a Gaussian Process is used to predict topic-strengths, which are then converted to probabilities using a softmax transformation. Inference follows \cite{WilliamsBarber1998} in using a Laplace approximation to handle the non-conjugacy in the model. Unsurprisingly, the authors experienced significant difficulty scaling this to the size of datasets typical in topic models, where the 8,000 document Reuters-21578 dataset is considered a toy, yet produces a kernel matrix with $64 \times 10^6$ elements.

This section thus far has been focussed on observed covariates used to improve assignment of topics, oftentimes with a view to generating text. One can also consider the case where one has to predict one or many labels given discrete observations such as words in a document, with LDA used to create low-rank representations to improve learning. The earliest approach is a ``downstream" model, SupervisedLDA\cite{Blei2008}, which, for a regression problem estimating $t_d|\vv{w}_d \in \Real$, learns topics $\vv{z}_{dn}$ for the $n_{d\cdot\cdot}$ words in $\vv{w}_d$, then makes a prediction using $t = \vv{w}\T\left(\oneover{n_{d\cdot\cdot}}\sum_n \vv{z}_{dn}\right)$. This can be extended to categorical predictions in the usual way by employing the framework of generalised linear models.

The most recent approach is \cite{Rubin2011}, which extends LabelledLDA\cite{Ramage2009} by learning a prior over labels. The simplest approach, PriorLDA, places a Dirichlet prior over labels, which then have a corresponding Multinomial distribution, with the hyperparameters of the Dirichlet label prior learnt from the data. The more complex DependencyLDA models the labels using LDA, then given the labels proceeds to model the text using LDA in turn. Observing that LDA can simply be viewed as multinomial PCA, this is equivalent to a multitask prediction problem where low-rank projections are learnt for both the features (the document words) and the tasks (the document labels)

\input{../footer.tex}
