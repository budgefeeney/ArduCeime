\input{../header.tex}

\subsection{Other Admixture Distributions of Text}


LDA-PYP\cite{Lindsey2012}. 

Recall that the LDA-HDLM\cite{Wallach2006} makes use of Minka's\cite{Minka2000} updates.

Topical keyphrase extraction on twitter\cite{Zhao2011a}

\subsubsection{Poisson}
An alternative to modelling corpora as a document at a time, is to model each word in each document independently as having a Poisson distribution, with mean parameter $\lambda_t$. 

A well-known result\cite{Bishop2006}\cite{Inouye2014} is that for T independent variables $x_1, x_2, \ldots, x_T$, each with a Poisson distribution characterised by mean parameter $\lambda_t$, their joint distribution is the product of two distributions: the first a multinomial, with mean parameter $\frac{1}{\sum_t \lambda_t} \{\lambda_1, \lambda_2, \ldots, \lambda_T\}$ and the second Poisson with mean parameter $\sum_t \lambda_t$. The traditional bag of words model typically employs a Multinomial/Poisson combination, where the Poisson represented document lenght, instead of T Poissons. Many models proceed to discard the Poisson over document length entirely, believing it to be uninformative, as can be seen in the above presentations of LDA.

An admixture of Poissons for discrete count data was proposed in \cite{Gopalan2013}. A deficiency of this model is that it does not place any constraints on the document length, thus the latent topic distribution inferred for a given document can only reconstruct that document up to a constant scaling factor. As with the multinomials, this model takes advantage of the ability to implicitly express the admixture via a weighted combination of parameters. Specifically, for each individual word (or other discrete measurement), the probability given the document mixture is:
\fixme{Where does this stand on counts}

\begin{align}
w_{dt} \sim \pois{\thd\T\Phi_{\cdot,t}}
\end{align}

This approach used independent Gamma priors for $\theta_{dk}$ and $\Phi_{kt}$, an analogue to the Dirichlet priors used in LDA. The result is essentially an unnormalized implementation of LDA. They claim that the Gamma distribution imposed sparsity (though this is true also of an appropriate parameterised Dirichlet), and that this model captures ``long-tail" results, though again again this is possible in the case of LDA by learning the topic distribution's hyperparameter\cite{Wallach2009a}. The fundamental advantage of this approach, particularly when compared against other \emph{matrix factorization} approaches, is that by employing the same class of tricks used for LDA in \cite{Mimno2012a} they can exploit sparsity in the input matrix to be factored for fast inference.

\subsubsection{Gaussian}
The Gaussian distribution encodes a scaled Euclidean distance from a centroid. Consequently, using our distance-metric intuition, we can see that while distances will degenerate at high-dimensionalities, it will at penalise missing values more than Poisson or Multinomial distributions. In practice, the covariance matrix $\Sigma \in \MReal{T}{T}$ is too large for efficient inference, and given limited data estimates may well be low rank.

One approach which used an admixture of Gaussians with diagonal covariances was for the particular case of Twitter data\cite{Eisenstein2010}. The use of Gaussians was primarily motivated by the desire to perturb vocabularies according to proximity to regional centre, thereby accounting for regional dialects. The model presented a three-level hierarchy of Gaussians (prior, base topic, regional topic) with adjustable isotropic covariances at each step, so the regional variation was captured by iterative shrinkage of of the global vocabulary to a topic-vocabulary and thence to regional topic vocabulary. Ultimately though, each regional topic's Gaussian word-distribution was converted to a Multinomial word-distribution by means of an unspecified softmax transformation, so missing words were ultimately not penalised.

To use the Gaussian distribution directly, we would need to use a term-frequency representation of the text, and use a truncated log-normal distribution to represent it.

Something which hasn't been used in the literature is investigating the use of low-rank covariance approximations. Perhaps this is simply because it may be easier to run PCA as a preprocessing step. However one could easily envisage a model such as

\begin{align}
\vv{z} \sim & \nor{0}{\rho^2 I_P} & P << T \\
\vv{\phi}_k | \vv{z}  \sim & \nor{W \vv{z} + \vv{m}}{\tau^2 I_T} \\
\vv{\phi}_k \sim & \nor{\vv{m}}{\tau^2 I_T + \rho^{-2} W W\T}
\end{align}

In the case of an admixture you could use matrix variate notation, though I'm not sure to what end, unless you wanted to present this as a matrix-factorization algorithm where $W \sim \mnor{\Theta \Phi}{I_T}{\Sigma_\Theta}$.

\begin{align}
\vv{z} \sim & \nor{0}{\rho^2 I_P}  \\
\Phi | \vv{z} \sim & \mnor{\one_K \vv{z}\T W\T + \one_K\vv{m}\T}{\tau^2 I_T}{\Sigma_\Theta} \\
\Phi \sim & \mnor{\one_K\vv{m}\T}{\tau^2 I_T + \rho^{-2} W W\T}{\Sigma_\Theta} \\
\end{align}

where $\Sigma_\Theta$ is the covariance matrix over topics. \fixme{The appearance of $\Sigma_\Theta$ here is strange. If topics are similar you'd ideally want them folded together into one}. 

A different approach might be to give each topic its own low-rank covariance $W_k$, drawn from a normal matrix prior with estimated mean $W$ parameter. This might account for the fact that, for example ``white" and ``house" are strongly correlated in a topic about politics, but weakly correlated in a topic about real-estate, while the estimate of the prior mean would capture global correlations such as ``united" with "states", "kingdom" and "airlines", which might not immediately obvious in the subset of documents assigned to a single topic. The model would be

\begin{align}
W \sim & \mnor{0}{I_P}{I_T} \\
W_k \sim & \mnor{W}{I_P}{I_T} \\
\vv{z} \sim & \nor{0}{\rho^2 I_P}  \\
\phi_k | W_k, \vv{z} \sim & \nor{W_k \vv{z} + \vv{m}}{I_T} \\
\phi_k | W_k \sim & \nor{\vv{m}}{I_T + W_k W_k\T}
\end{align}

\fixme{Is there some parametric way we could encode the perturbation, instead of explicitly inferring K+1 W matrices? Does the matrix-variate construction offer any worthwhile avenues of exploration in this regard? ? Lastly does an admixture low-rank vocabularies make sense I know mixtures of factor analysers exist, but since LDA is a sort of multinomial PCA, subsequently doing Gausian PCA on the multinomial basis-vectors seems excessive. }. 

\subsubsection{The Von Mises Fisher Distribution}

The probabilistic equivalent of a cosine distance for unit-normed vectors is the Von-Mises Fisher distribution, which is a probability distribution on the $\mathbb{S}^{T-1}$ unit-sphere in $\mathbb{R}^{T}$. Its density is given by:

\begin{equation}
c_T(\kappa)\exp(\kappa \vv{\mu}\T\vv{x})
\end{equation}

where $||\vv{\mu}||_2 = ||\vv{x}||_2 = 1$. The normaliser $c_T(\kappa)$ is defined as

\begin{equation}
\frac{\kappa ^{T\!/\!2-1}}{(2\pi)^{T-2} I_{T\!/\!2-1}(\kappa)}
\end{equation}

where $I_r(\cdot)$ is the modified Bessel function of the third kind and the r-th order.


An admixture of Von-Mises Fisher distributions was used in the Spherical Topic Models paper\cite{Reisinger2010}. The authors claim the von Mises Fisher Distribution is robust at high-dimensionalities and/or when the number of non-zero values is very small, particularly compared against Euclidean-based metrics, though this seems to be contradicted by the link between cosine and Euclidean distances described earlier in this report. Unlike the Gaussian approaches outlined above, it doesn't allow one to capture correlations between words.

The model is

\begin{align}
\vv{\mu} | \kappa_0 & \sim  vMF(\vv{m}, \kappa_0) \\
\vv{\phi}_k | \vv{\mu}, \xi & \sim  vMF(\vv{\mu}, \xi) \\
\thd & \sim  \dir{\vv{\alpha}} \\
\wdoc & \sim  vMF(\text{WAvg}(\thd, \Phi), \kappa) 
\end{align}

In their model they define the mixing function WAvg($\thd, \Phi$) as $\frac{\thd\T\Phi}{||\thd\T\Phi||}$. They don't specify the order of the norm in the denominator: one presumes by default it should be a 2-norm. They additionally note that this formula does not yield the vector that minimizes the weighted sum of geodesic distances to the mean. Buss \& Fillmore (2001) introduce the spherical average $BFAvg(\thd,\Phi) = \arg \min_{\vv{q}} \sum_k \thdk \delta_\mathbb{S}(\vv{\phi}_k, \vv{q})$ where $\delta_\mathbb{S}(\vv{x}, \vv{y})$ is the geodesic distance between $\vv{x}, \vv{y} \in \mathbb{S}^T$. 
Two variants were used: a straightforward admixture of vMF distributions, and an admixture of ``trucated" vMF distributions restricted to the positive othant of the unit sphere only. Despite the fact that the latter variant respects the support of the TF-IDF vectors used as an input to the algorithm, it was the plain admixture of vMFs that gave better results. However the paper explicitly refused to evaluate results using common metrics such as perplexity: instead conducted more convoluted experiments such as measuring the accuracy of logistic regression where the features are the topics generated by multiple topic-modelling algorithms and the like. It's hard to judge the efficacy of the model with such results.


\subsubsection{Markov Random Fields with Discrete Observations}
This following section looks into models which deal with sparse, high-dimensional data \emph{and} also handle correlations between words. The method is to consider the vocabulary as a fully connected graph over all possible terms, and by means of a sparsity-inducing regularization prune out edges, so that what is left is a sparse, disconnected graph of related words.

This is achieved by means of a Markov Random Field, a type of undirected graphical model.

Two methods have been considered: one uses a multivariate Bernoulli distribution for words, from which the standard Ising model is derived. In the case of microtexts, where repeated words are rare, little is lost by substituting a multivariate Bernoulli in place of a Multinomial. The other approach is more general and attempts to use multivariate Poisson distributions for words, for which of course the standard Multinomial/Poisson model is a special case. However by capturing graph structure, the Poisson MRF penalizes missing words and captures correlations.

In both cases scaling is hard, though in the former it is at least well-studied.

\subsubsection*{Bernoulli MRF}
The Bernoulli MRF model of \cite{Nallapati2007} assumes that text have been encoded in a binary bag-of-words vector, where for term $t$, if that term occurs \emph{at least once} in the document, then $w_{dt} = 1$ , otherwise $w_{dt} = 0$. For microtexts, where repeated word occurrences are rare in any event, nothing is lost by this construction.

In the case of LDA, words are distributed according to a Multinomial distribution, whose parameter $\phi_k$ is drawn from the appropriate Dirichlet according to the topic assigned to that particular word. In the Bernoulli MRF model words would be distributed according to a topic-specific Bernoulli MRF, with parameter-matrix $\Lambda^{(k)}$, where $\Lambda^{(k)}$ encodes the edge structure, and an L1 regularlizer encodes a preference for sparse graphs.

\newcommand \vertices { { \mathcal{V} } }
\newcommand \edges    { { \mathcal{E} } }

The actual model of the MRF, from \fixme{Wainright et al 2006} begins with the usual energy function over $T$ possible vertices in $\vertices$ and edges in $\edges$ is

\begin{equation}
p(\vv{w}_d|\Lambda^{(k)}) = \exp\left(
    \sum_{t\in\vertices}\Lambda^{(k)}_{\emptyset t} w_{dt}
    + \sum_{t,v \in \edges}\Lambda^{(k)}_{st} w_{dt} w_{dv}
    - A(\Lambda^{(k)})
\right)
\end{equation}
where $\Lambda^{(k)} \in \mathbb{R}^{(T+1) \times T}$ and $A(\Lambda^{(k)})$ is the log normalizer. 

With some re-arranging, we can see how this can be decomposed into T logistic regressions, one for each possible term $t \in {1\ldots T}$. 

\begin{align}
\hat{\Lambda}^{(k)}_t = & \arg \max_{\Lambda^{(k)}_t} \sum_d \ln p(w_{dt}|\vv{w}_{d \setminus t}, \Lambda^{(k)}_t) - \rho ||\Lambda^{(k)}_{\setminus t}||_1 \\
 = &  \arg \max_{\Lambda^{(k)}_t} \sum_d w_{dt} \Lambda^{(k)}_t \vv{w}_{d \setminus t} - \ln(1 + \exp(\Lambda_t^{(k)\top}\vv{w}_{d \setminus t}) - \rho ||\Lambda^{(k)}_{\setminus t}||_1
\end{align}

In a topic model, there will therefore by $T \times K$ T-dimensional logistic regressions. While expensive in pure computational terms, it's worth nothing how trivially parallelizable this model is.

In the above $\vv{w}_{d \setminus t}$ is $\vv{w}_d$ with the t-th element set to 1, which is equivalent to \emph{removing} the t-th value and adding an intercept value in its place. As the t-th value of $\Lambda^{(k)}$ now corresponds to a bias term, it is not regularized, so the regularizer operates on $\Lambda^{(k)}_{\setminus t}$ which is $\Lambda^{(k)}$ with $\Lambda^{(k)}_{tt}$ ``removed", i.e. it is not included in the evaluation of the norm.

\fixme{In light of the MTL focus on what I've done thus far, it would be interesting to consider this as an MTL learning problem, using a Gaussian scale mixture to enforce sparsity on $\Lambda^{(k)}$. The number of parameters is huge however.}

Reading the original Bernoulli MRF paper from \fixme{Wainwright et al} they claim that their convergence proof requires that the dataset grow logarithmically with the ``graph size", which is a condition most larger corpora will should meet. 

In the case of the admixture paper using this model, results were reported for just \emph{a single experiment} for which the number of topics $K=10$ and the the corpus (AP) contains $D=2,246$ documents with a vocabulary $T=10,473$. They ran LDA on this to estimate topics, then for each topic $k$ took those documents with $\theta_{dk} \geq 0.25$ and then ran the given algorithm to determine the word graph for each topic. This is evidently to work around performance issues involved in $K \times T$ logistic regressions of dimension $T$ on each iteration of the algorithm. The result, derived from $\Lambda^{(k)}$, is a graph of the already-determined topic-vocabulary. For the two of the ten topics they choose to present, they had 35,588 and 128,074 edges. The number of non-zero values in $\Lambda^{(k)}$ is twice that. There is no symmetry constraint on $\Lambda^{(k)}$.

\subsubsection*{Poisson}
\newcommand \vl {\vv{\lambda}}
\newcommand \bl {\Lambda}
\newcommand \blk {\Lambda^{(k)}}

The first attempt to use MRFs actively for topic modelling instead of just as a post-hoc visualization tool was by Inouye et al\cite{Inouye2014}.

This uses a formulation of an MRF with Poisson observations introduced and then refined by Yang et al in \cite{Yang2012} and \cite{Yang2013a}, which states that given independent parameters $\vv{\lambda}$ and $\Lambda$ the probability of an observation $\wdoc$ is:

\begin{align}
p(\wdoc|\vl,\bl) = \exp\left(\vl\T\wdoc + \wdoc\T\bl\wdoc - \sum_t \ln (w_t ! ) \right)
\end{align}

where $\diag{\bl} = 0$. Under this model, the distribution of a single observation given all other observations is:

\begin{align}
w_t | \vv{w}_{\setminus t}, \vl, \bl  \sim \pois{\exp(\lambda_t + \Lambda_t \vv{w})}
\end{align}

Knowing the relationship between the Poisson and its canonical definition as a member of the exponential family, we can see that this distribution's natural parameter is $\eta_t = \lambda_t + \Lambda_t \vv{w}$. 

The matrix $\bl$ is equivalent to the precision matrix in a Gaussian MRF. If it is set to zero, then the resulting distribution is a multivariate Poisson distribution, similar to the pure Poisson model discussed earlier. The matrix can contain negative or positive values (the latter the refinement Yang introduced in \cite{Yang2013a}). This allows us to capture the fact that words rarely co-occur, as well as the converse.

As with the straightforward multinomial distribution, they implement the admixture by portraying the parameters as a convex optimisation of the component parameters and the topic assignments.
\begin{align} 
\begin{split}
\thd, &\wdoc, \vl_1, \ldots, \vl_k, \bl^{(1)}, \ldots \blk  \sim \\
& \mathcal{P}_{\text{MRF}}\left(\wdoc;  \hat{\vl} = \sum_k \thdk \vl_k, \hat{\bl} = \sum_k \thdk \blk \right)\dir{\thd; \vv{\alpha}}\prod_k \pi_{\text{MRF}}(\vl_k, \blk; \vv{\beta}, \gamma, \rho_\lambda, \rho)
\end{split}
\end{align}

where $\pi_{\text{MRF}}(\vl_k, \blk; \vv{\beta}, \gamma, \rho_\lambda, \rho)$ is the conjugate prior over the parameters of the poisson MRF and is proportional to

\begin{align}
\begin{split}
\pi_{\text{MRF}}(\vl_k, &\blk; \vv{\beta}, \gamma, \rho_\lambda, \rho) \propto \\
& \exp\left( \vv{\beta}\T\blk\vv{\beta} - \gamma A(\vl_k, \blk) - \rho_\lambda||\vl_k||_2^2 - \rho||\vecf{\blk}||_1 \right)
\end{split}
\end{align}

where $\beta_t > 0$, $\gamma \geq 0$, $\rho_\lambda > 0$, $\rho > \max_{t,v}\beta_t\beta_v$. Note that this prior encodes a preference for a sparse graph structure.

The posterior update follows an intuitive pseudo-count scheme where
\begin{align}
\hat{\vv{\beta}} = \vv{\beta} + \vv{w}
\hat{\gamma}     = \gamma + 1
\end{align}

and the norm weightings $\rho_\lambda$ and $\rho$ are kept fixed.

One issue the the paper does not dwell on the consequences of mixing in the exponential space instead of the mean parameter space. As the section on the multinomial distribution illustrated, a weighted combination of the natural parameters is not the same as a weighted combination of the mean parameters.

For inference to be tractable, they optimise the pseudo-likelhood instead of the likelihood. This gives rise to the following approximate log-posterio

\begin{align}
\begin{split}
\ln p(\Theta, & \vl_1, \ldots, \vl_k, \bl^{(1)}, \ldots, \blk | W) \\
& \approx \sum_d \left[ \left( \sum_t \eta_{dt} \hat{w}_{dt} - (\gamma - 1)A(\eta_{dt}) \right) + (\vv{\alpha - 1})\T\ln(\thd) \right]
\end{split}
\end{align}

where $\hat{\wdoc} = \vv{\beta} + \wdoc$ and $\eta_{dt} = \sum_k \thdk (\lambda_{kt} + \Lambda^{(k)}_t \vv{w}_{d \setminus t})$ is the the canonical parameter of the univariate Poisson.


Once one incorporates the $l_1$ norm constraint on the $\blk$ matrices, the resulting objective is no longer differentiable. In the admixture of PMRF paper they therefore use a proximal optimization method.

An alternative to this, inspired by Cedric's work, would be to use Gaussian scale mixtures with the GMGIG prior \cite{Yang2013} on $\blk$ to attempt to enforce sparsity, but how we'd fit this in with the PMRF prior distribution is unclear.

Results are only reported for the case where $K=5$ topics and $T=500$ terms and $D=31,000$ documents. This is due to the fact that the algorithm scales quadratically with respect to vocabulary size. 

They don't really have a clear answer to the scaling problem.

\subsubsection{The Hyperspace Analogue to Language}
The Hyperspace Analogue to Language\cite{Lund1996} is, in plain terms, a matrix of how often two words co-occur within a sliding window which is moved one word at a time throughout all words in all documents in a corpus. Word counts are scaled inversely by the number of words between them such that an entry in the matrix is given by 

\begin{equation*}
HAL_{t,t'} = \sum_{l=0}^L w(l) n(l, t, t')
\end{equation*}
Where $t$ and $t'$ are distinct words, $l$ is the window length, $w(l)$ is the window-specific weight and $n(l, t, t')$ is the count of how often words $t$ and $t'$ co-occur with exactly $l$ words in-between.

In general, words with similar meanings should have similar co-occurence vectors, and one can therefore use, for example, multidimensional scaling to establish semantically meaningful clusters of words\cite{Lund1996}.

The HAL as defined above does not however define a probabilistic model. The following probabilistic formulation was provided by \cite{Azzopardi2005}

\begin{equation*}
p_h(t'|t) = \sum_l p(l) p(t'|t,l)
\end{equation*}
where $p(t'|t,l) = \frac{n(l, t, t')}{\sum_u n(l, u, t')}$ and the two choices of $p(l)$ are $\frac{1}{2L}$ or $\frac{L - l + 1}{\sum_m L - m + 1}$. In many cases smoothing techniques are necessary to handle cases where $n(l,t,t') = 0$. These discussed in the following section.


\fixme{
\begin{itemize}
    \item How does this compare to association rules?
    \item Tweets are small enough that the entire tweet could be a window
    \item Single tweets have been assigned single topics in the past.
    \item How does one \emph{generate} words from HALs?
\end{itemize}}



\subsubsection{Admixtures of Language Models}
In \cite{Wallach2006} an admixture of hierarchical Dirichlet language models (HDLMs)\cite{MacKay1995} was used on a tiny dataset of 150 NIPS abstracts and a subset of 20-newsgroup posts. Unlike the original HDLM paper, they used Minka's update algorithm\cite{Minka2002} for the vocabulary and topic hyperparameters. Topic inference was identical to LDA. Two variants of vocabulary distributions were employed, one which shared the vocabularly hyperprior amongst all topics, and one which had a different hyperprior for each topic which gave better results. Both models provided better perplexity results than LDA.

This method was revisited in the ``Topical N-gram Model"\cite{Wang2007}, which viewed the model as an admixture of bigrams instead of an admixture of language models. Their belief was that a model which viewed a vocabularly as a mixture of unigrams, bigrams, trigrams etc. would be a better fit. They therefore augmented LDA with a Bernoulli switch $c_{dn}$ which when zero indicated that word $w_{dn}$ was to be fused into a bigram with word $w_{d(n-1)}$ (i.e. sampled according to a transition matrix $\Phi_k$), and which when set to one indicated that the word was independently generated (i.e. sampled from a multinomial $\phi_k$). By tying together sequences where $c_dn=1$ they presented higher-order n-grams.

An issue with this model was that for each word a topic was sampled, such that the various words in an n-gram would have separate topic assignments. Empirically they found that retroactively setting all topics to the topic of the last word worked best, but there was no theoretical justification for this. A more theoretically sound approach was taken by \cite{Lindsey2012} which used hierarchical Pitman-Yor language models to generate text. In this case a single topic was generated, and fixed, until $c_dn=1$ which they described a Bayesian change-point detection. Further, the distribution of $c_dn$ was changed such that the posterior depended on the topic and the previous word. Using Amazon's Mechanical Turk they obtained superior performance in a phrase-intrusion test to LDA and the ``Topical N-Gram" model of \cite{Wang2007}, though it is conspicuous that neither reported results using the common perplexity metric.

Both of these methods are interested in ``collocations", i.e. the idea that a unique concept is represented by more than one word, but that such collocations only exist for certain topics. An example is that the words ``White" and "House" should always be fused together except in the case of a topic centred on real-estate say, where they should be kept separate. The most recent extension of these methods used for the purposes of topical collocation is that of \cite{Johnson2010} which uses an admixture of adaptor grammars, described as a hierarchical, non-parameteric Bayesian extension of probabilistic context-free grammars.

There are many other models using variants of these ideas. In \cite{Griffiths2005} a hidden markov model was employed, where the emission distributions consisted of $S-1$ multinomial distributions and one LDA model. For each word a topic $z_dn$ and a hidden state $c_dn$ were sampled, with the topic used only if the appropriate LDA emission distribution was used. The intent was that the other emission distributions would capture ``syntax" words. Equally, the idea of employing a markov property was used in \cite{Gruber2007} where a topic was assigned to an entire sentence, and a Bernoulli switch was used to determine whether a new topic be sampled for the next sentence or not. A variation of these methods have been used in author-topic models\cite{RosenZvi2004} of Tweets\cite{Zhao2011}\cite{Zhao2011a} where a single topic was assigned to each tweet, and for each word within that tweet a Bernoulli switch was used to distinguish topic words from ``syntax" words. In this case there was just one distribution for ``syntax words" the empirical distribution of words in the corpus.

The idea of a special distribution to capture stop-words is prevalent


\subsubsection{A Natural Parameterisation of the Multinomial Admixture}
As explained in \cite{Buntine2002}, one can re-state the standard LDA model, where the vocabularies are parameters with no distribution, as

\begin{align}
\thd & \sim \dir{\vv{\alpha}} & \text{\emph{Document d's topic distribution}} \\
\vv{w}_d & \sim \muln{\sum_k \thd\T\Phi}{N_d} & \text{\emph{The term-count vector for document $d$}} \label{eqn:lda_convex_combo}
\end{align}

where $\Phi = \{\vv{\phi}_k\T\}_{k=1}^{K}$.

With this, we can consider the \emph{natural parameterisation} of this model. In the current parameterisation, we require that $0 \leq \phi_{kv} \leq 1$ for all $v$ and $\sum_v \phi_{kv} = 1$. Adopting the natural parameterisation\footnote{As described on \url{http://qwone.com/~jason/writing/mixtureMultinomials.pdf}} we can eliminate all constraints on $\phi_{kv}$ potentially making it easier to model

\begin{align}
p(\vv{w}|\vv{\phi}) = \frac{(\sum_t w_t)!}{\prod_t w_t !} 
\prod_t \left(   
    \frac{\exp(\phi_t)}{\sum_t \exp(\phi_t)}
\right)^{w_t}
\end{align}

Were one to try to express the natural parameter as a convex combination, as we have previously with the mean parameter, then we obtain the following

\begin{align}
p(\wdoc|\Phi, \thd) = \frac{(\sum_t w_{dt})!}{\prod_t w_{dt} !} 
\prod_t \left(   
    \frac{\exp(\thd\T\Phi)}{\sum_t \exp(\thd\T\Phi)}
\right)^{w_{dt}}
\end{align}

A disadvantage of this approach is that this \emph{does not} correspond to the standard LDA admixture model. Whereas in \eqref{eqn:lda_convex_combo} we represent the mixture distribution as weighted arithmetic average of the mixture components, in this model we're using a weighted \emph{geometric} average of the components, which is less expressive.

\fixme{One issue that's not made clear is that in the note mentioned, they claim the natural mixture is related to the Dirichlet distribution}.

As described in the introduction, for a mean  $\vv{\mu} = \{ \frac{1}{3}, \frac{1}{3}, \frac{1}{3} \}$ both $\{1, 1, 1\}$ and $\{3, 0, 0\}$ will have the same probability, as the multinomial distribution will not penalize missing values. With regards to sparsity, for a single document, the multinomial is similar to the consine distance in that

\begin{align*}
\ln p(\vv{w_{dn}}|\vv{\phi}) & \propto \ln \prod_n\prod_t \phi_v^{w_{dnt}} = \sum_n\sum_t w_{dnt} \ln \phi_t & \text{ }\\
 & = \sum_t n_{dt} \ln \phi_t & n_{dt} = \sum_n w_{dnt} \\
 & = \vv{n}_{d}\T \hat{\vv{\phi}} =  ||\vv{n}_{d}||_2 || \hat{\vv{\phi}}||_2 \cos (\vv{n}_{d}, \hat{\vv{\phi}}) & \text{$\hat{\vv{\phi}} = \ln \vv{\phi}$}
\end{align*}
Neither $\vv{n}_d$ nor $\hat{\vv{\phi}}$ have unit norm, so the subsequent comparison between the cosine and Euclidean distance is not so straightforward. Moreover, in the case of admixtures, the mean parameter is usually much less sparse than the observation, so the problem of no vector components never intersecting is much less likely to arise.

%--- the TopicSeqMem 4 frame -------------------------%


\section{Conclusions}
When dealing with tweets, a particular instance of the class of microtexts, the distinguishing factors are
\begin{enumerate}
    \item Very short documents
    \item Documents are missing words that a topic model would ascribe to them.
    \item Few, and often no, repeated words
    \item A very large vocabulary in the case of Twitter
\end{enumerate}

Given (1) and (2) it seems the best method to employ is one which does not penalize missing words at all, which is the straightforward Multinomial model. The one which might be most problematic is the von Mises-Fisher model, which corresponds to a cosine distance.

Given (1) and (4)  it seems that norm-based distances and the Gaussian distribution should be avoided, as the combination of very high sparsity and large dimensionality could lead to the degenerate behaviour of all documents being roughly equiprobable. This is alleviated by the fact that the topic distributions shouldn't be very sparse. However a Gaussian will additionally penalize missing words, rendering it unsuitable in light of (2)

Were one interested in capturing correlations between words, one could follow Eisenstein's approach and transform a Gaussian distributed ``base topic" in a multinomially distributed topic distribution.

\input{../footer.tex}
