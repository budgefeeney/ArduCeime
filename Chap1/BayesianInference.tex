\input{../header.tex}

\newcommand \zd   { { \vv{z}_d } }
\newcommand \qfam { { \mathcal{Q} } }
\newcommand \xdat { { \mathcal{X} } }
\newcommand \zdat { { \mathcal{Z} } }
\newcommand \xnew { { \vv{x}^{*} } }
\newcommand \znew { { \vv{z}^{*} } }
\newcommand \param { { \vv{\phi} } }
\newcommand \params { { \Phi } }
\newcommand \ml[1] { { {#1}_{\text{ML}} } } 
\newcommand \map[1] { { {#1}_{\text{MAP}} } } 
\newcommand \quarter { { \oneover{4} } }
\newcommand \eighth { { \oneover{8} } }
\newcommand \fqt[1] { { \mathcal{F}\left( {#1} \right) } }
\newcommand \joint { { p(\xdat, \zdat | \params) } }
\newcommand \logjoint { { \ln \joint } }
\newcommand \exlogjoint[1] { { \ex{\logjoint}{{#1}} } }

\section{Inference in Probabilistic Models}
Probabilistic inference is concerned with two problems: inference, the estimation parameter values $\param$ from a set of observations $\vv{x} \in \xdat$; and prediction, the evaluation of the probability of unobserved variables $p(\xnew | \xdat) = \int p(\xnew | \param) p(\param | \xdat) d\param$. Various methods have been proposed for this purpose.

\emph{Maximum likelihood} finds the parameter value that maximises, $p(\xdat|\param)$, known as the likelihood. For reasons of algebraic convenience, one commonly finds the maximum of the log of the likelihood.\begin{align}
\ml{\param} = \arg \max_{\param} \ln p(\xdat | \param)
\end{align}
\bflong{Take Marks lecture not about the log and the motivation from information theory}
As the log of a monotonically increasing function, the log-likelihood and likelihood have the same maxima. For the particular case of independently and identically distributed (IID) observations, the log-likelihood factors into a simple sum.
\begin{align}
\ml{\param} = \arg \max_{\param} \sum_d \ln p(\xd | \param)
\end{align}
\bflong{Which is stronger: exchangeability or IID}
For prediction one typically employs the approximation

\begin{align}
p(\xnew | \xdat) 
&= \int p(\xnew | \param) p(\param | \xdat) d\param \\
&\approx \int p(\xnew | \ml{\param}) p(\param | \xdat) d\param
=  p(\xnew | \ml{\param})
\end{align}

\bflong{This is wrong as $\param$ has no distribution, check with Heinrich}

\emph{Maximum a Posteriori} (MAP) estimation, like maximum likelihood, finds a mode, but of the posterior distribution instead of the likelihood. Given a prior $p(\param)$, the posterior is given by Bayes rule:
\begin{align}
p(\param | \xdat) = \frac{p(\xdat | \param)p(\param)}{p(\xdat)} = \frac{\text{likelihood} \times \text{prior}}{\text{evidence}}
\end{align}

The MAP solution is:

\begin{align}
\map{\param} = \arg \max_{\param} \frac{p(\xdat | \param)p(\param)}{p(\xdat)}
 & = \arg \max_{\param} p(\xdat | \param)p(\param) \\
 & = \arg \max_\param \ln p(\xdat | \param) + \ln p(\param)
\end{align}
\bfshort{This is wrong as it may be solved directly}
Like Maximum Likelihood, prediction using MAP is approximated with a point-estimate $p(\xnew | \xdat) \approx p(\xnew | \map{\param})$. Such an approximation is appropriate if we assume the posterior is tightly concentrated around the mode.

The incorporation of the prior is motivated by two related reasons: firstly it allows a practitioner to supply some prior knowledge to the inference procedure and secondly it prevents a model from overfitting in the presence of limited or noisy data.

The alternative to finding modes is to take a fully \emph{Bayesian} approach and solve Bayes rule directly, and thereby derive a \emph{distribution} over $\param$. This involves evaluating the evidence $p(\xdat) = \int p(\xdat | \param) p(\param) d\param$. In many cases this is intractable, but there are families of prior and likelihood distributions where the prior and posterior have the same functional form for which exact inference is possible. In these cases we say the prior is \emph{conjugate} to the likelihood.

An example is learning a parameter $\param$ with a Dirichlet distribution given multinomial-distributed observations (e.g. throws of a weighted dice). The Dirichlet distribution is given by:
\begin{align}
p(\param) & = \oneover{B(\vv{\beta})} \prod_k \phi_k^{(\beta_k - 1)} &
B(\vv{\beta}) & = \frac{\prod_k \Gamma(\beta_k)}{\Gamma(\sum_k \beta_k)}
\end{align}
where $B(\cdot)$ is the multivariate beta function, and $\Gamma(\beta)$ is the Gamma function, which can be thought of as a continuous generalisation of the factorial function such that $x! = \Gamma(x+1)$. The multinomial is given by:
\begin{align}
p(\vv{x}|\param) = \frac{(\sum_k x)!}{\prod_k x!} \prod_k \phi_k^{x_k}
\end{align}
though for a single draw, the binomial coefficient evaluates to one. A single-draw multinomial distributions is known as a \emph{categorical} distribution. The joint distribution of several, exchangeable observations using a categorical likelihood is \bfshort{Used exchangeable before introducing it}
\begin{align}
p(\xdat|\param) & = \prod_d \prod_k \phi_k^{x_{dk}} = \prod_k \phi_k^{n_k} &
n_k = \sum_d x_{dk}
\end{align}
The evidence $p(\xdat)$ for a Categorical-Dirichlet mixture is therefore:
\begin{align}
p(\xdat) & = \frac{1}{B(\beta)} \int \prod_k \phi_k^{n_k} \prod_k \phi_k^{(\beta_k - 1)} d\param  = \frac{1}{B(\beta)} \int \prod_k \phi_k^{n_k + \beta_k - 1} \\
& = \frac{B(\vv{n} + \vv{\beta})}{B(\vv{\beta})}.
\end{align}
The analytic solution arises from observing that the integrand is an unnormalised Dirichlet distribution parameterised by $\vv{n} + \vv{\beta}$. This marginal distribution is sometimes known as the P\'{o}lya distribution, or the Dirichlet-compound-Multinomial distribution \cite{Madsen2005}.\label{polya}

Putting this together we can analytically derive an expression for the posterior:

\begin{align}
\begin{split}
p(\phi|\xdat) 
& = \frac{p(\phi|\xdat)p(\phi)}{p(\xdat)}
 = \left(\prod_k \phi_k^{n_k} \right)
   \left(\oneover{B(\vv{\beta})} \prod_k \phi_k^{\beta_k - 1} \right)
   \frac{B(\vv{\beta})}{B(\vv{n} + \vv{\beta})} \\
& = \oneover{B(\vv{n} + \vv{\beta})} \prod_k \phi_k^{n_k + \beta_k - 1}
\end{split}
\end{align}
which is a Dirichlet distribution parameterised by $\vv{n} + \vv{\beta}$, where $\vv{n} = \{ n_k \}_{n=1}^K$

It is useful to compare the expected value of the posterior with the MAP and maximum likelihood solutions.

\begin{align}
\ex{\param}{}  &= \frac{n_k + \beta_k}{\sum_j n_j + \beta_j} &
\map{\param} &= \frac{n_k + \beta_k - 1}{\sum_j n_j + \beta_j - 1} &
\ml{\param}  &= \frac{n_k}{\sum_j n_j}
\end{align}

The Bayesian approach is more affected by the prior than the MAP solution, where the prior is discounted, and the ML approach obviously incorporates no prior information whatsoever.
 
\subsection{Latent Variable Models}
Oftentimes it is appropriate to assume that there is some unobserved, latent process which determines the distributions of the observed variables. Denoting the latent variables as $\vv{z}$ this gives rise to the following likelihood
\begin{align}
p(\xdat|\param) = \int p(\xdat, \zdat|\param) d\zdat
\end{align}

A common case where latent variables arise is the mixture model, where it is assumed there are $K$ distributions, of which one, identified by a latent categorical variable $\vv{z}_d$ is used to sample an observation $\xd$. 

As an example, consider the problem of modelling a corpus of documents. Let $\xd \in \mathbb{N}^T$ be the count of how often each word occurred in document $d$, for all $T$ words. If we presume each document is about one of K topics, where each topic has a distribution over word-counts $\vv{\phi}_k$, we can associate a latent variable $\vv{z}_d$ with each document indicating which (unobserved) topic it generated its words. This gives rise to the mixture of multinomials model\cite{Nigam2000}
\begin{align}
\vv{\theta} & \sim \dir{\vv{\alpha}} &
\vv{z}_d|\vv{\theta} & \sim \muln{\vv{\theta}}{1} & 
\vv{x}_d|\vv{z}_d, \param & \sim \prod_k \muln{\param}{1}^{z_{dk}} & 
\param & \sim \dir{\vv{\beta}}
\end{align}

It should be noted that this model does not simply rely on obsevations being IID. It further relies on their joint distribution being \emph{exchangeable}

Letting $\params = \{ \vv{\phi}_{k=1}^K \}$, our MAP solution is
\begin{align}
\begin{split}
\params 
    &= \arg \max_\params \ln p(\params) + \ln p(\xdat|\params) \\
    &= \arg \max_\params \ln p(\params) + \ln \int \joint d\zdat
\end{split}
\end{align}



\bflong{Why does the integral have no analytic solution}
The integral has no analytic solution, so we approximate it, leading to the EM algorithm.
\subsubsection{Expectation Maximization}
Jensen's inequality states that for any \emph{concave} function $f(x)$, $f\left(\ex{x}{}\right) \geq \ex{f(x)}{}$. By introducing a new, wholly arbitrary distribution $q(\zdat)$ we can approximate the previous log-integral

\begin{align}
 \ln \int & \joint d\zdat = \ln \int \frac{q(\zdat)}{q(\zdat)} \joint d\zdat\\ 
     &=  \ln \ex{\oneover{q(\zdat)}\joint}{q} \geq \ex{\ln \oneover{q(\zdat)}\joint}{q}\\
     &= \ex{\ln \joint}{q} + \ent{q} \label{eqn:elbo}
\end{align}

where the entropy $\ent{q} = -\ex{\ln q(\zdat)}{q}$. Equation \eqref{eqn:elbo} is sometimes referred to as the \emph{free energy} or the \emph{evidence lower-bound} (``ELBO").

This leads two a two step maximisation process.

\begin{align}
\text{E-Step:} & & q^t(\zdat) & = \arg \max_{q(\zdat)} \ex{\ln p(\xdat, \zdat|\params^{t-1})}{q^{t-1}} + \ent{q} \label{eqn:estep} \\
\text{M-Step:} & & \params^t & = \arg \max_{\params} \ex{\ln \joint}{q^{t}} + \ln p(\params) \label{eqn:mstep}
\end{align}
To determine how to maximise $q(\zdat)$ we observe that, for a fixed $\params$, the difference between the ELBO and the marginal log-likelihood is given by
 the Kullback Leibler divergence between $q(\zdat)$ and the posterior $p(\zdat|\xdat, \params)$.
\begin{align}
\int q(\zdat) \ln \frac{p(\xdat, \zdat|\params)}{q(\zdat)} d\zdat
& = \int q(\zdat) \ln \frac{p(\zdat|\xdat, \params)p(\xdat | \params)}{q(\zdat)} d\zdat \\
& = \int q(\zdat) \ln p(\xdat | \para) q\zdat + \int q(\zdat)\ln\frac{p(\zdat|\xdat, \params)}{q(\zdat)} d\zdat \\
& = \ln p(\xdat | \params) + \kl{q(\zdat)}{p(\zdat|\xdat, \params)}
\end{align}
As the Kullback-Leibler divergence is always positive, we can deduce that reducing the divergence will increase the ELBO, whose maximum is $\ln p(\xdat | \params)$. Hence the E-Step maximisation procedure is to set $q(\zdat) = p(\zdat | \xdat, \param)$, since in this case $\kl{q(\zdat)}{p(\zdat|\xdat, \params)} = 0$.

This is the MAP variant of the Expectation Maximization (EM) algorithm\cite{Dempster1977}. 
Both E and M steps increase the bound. In the M-Step, we maximise the ELBO with respect to $\params$, increasing both the likelihood and the ELBO. In the E-Step by setting $q(\zdat) = p(\xdat|\zdat, \params)$ the KL divergence goes to zero, decreasing the gap between the ELBO and marginal likelihood. Consequently EM is guaranteed to find a local maximum of the marginal likelihood.\fixme{ref}

For a mixture of multinomials model, the updates can be derived using the method of partial derivatives with Lagrange multipliers and are as follows:

\begin{align}
\text{E-Step:} & & q(z_{dk}) 
& = \frac{p(\xd|z_d = k, \param)p(z_d = k)}{\sum_j p(\xd|z_d = j, \param)p(z_d = j)} 
= \frac{p(\xd | \param_k) \theta_k}{\sum_j p(\xd | \param_j) \theta_j} \label{eqn:mom-estep} \\
\text{M-Step:} 
& & \phi_{kt} & = \frac{1}{N_k} \sum_d \ex{z_{dk}}{q} x_{dt} + \beta_t - 1, \qquad N_k = \sum_d \ex{z_{dk}}{} \\
& & \theta_k & =  \frac{N_k + \alpha_k - 1}{\sum_j N_j + \alpha_j - 1} \label{eqn:mom-mstep}
\end{align}
where $\ex{z_{dk}}{q} = q(z_{dk})$. The predictive distribution for a new observation $\xnew$ is:
\bfshort{$\phi_{kt}$ denom looks wrong, it needs normalisation}
\begin{align}
p(\xnew | \xdat, \params)  = \sum_k p(\znew = k | \vv{\theta}) p(\xnew, \znew = k | \param_k) 
= \sum_k \theta_k \cdot p(\xnew | \param_k)
\end{align}


\subsubsection{Variational EM}
It is not always feasible to evaluate the posterior distribution $p(\zdat|\xdat, \param)$ exactly. In such cases one needs to either approximate the E-Step update to the distribution (e.g. by parameterising it and taking a gradient step), or approximate the distribution itself.

We consider the latter approach, where one assumes the true distribution $q(\zdat)$ can be approximated by a family of distributions $\qfam$. One popular choice is the family of factorized distributions $q(\zdat) = \prod_m q(\zdat_m)$. Note that this factorization is not derived from the IID principle, as the in prior discussion on EM, rather it is a modelling assumption made by the practitioner. The question then is how to evaluate the variational E-Step


\begin{align}
q(\zdat) & = \arg \max_{q(\zdat_1) \ldots q(\zdat_M)} \exlogjoint{\tinymath{\prod_m q(\zdat_m)}} + \ent{q}
\end{align}

Given this formulation, we can solve independently for each block of latent variables $\zdat_m$ in turn, using the following decomposition.

\begin{align}
\begin{split}
& \mathbb{E}_{\tinymath{\prod_m q(\zdat_m)}} \left[ \logjoint \right] + \ent{q} \\
& \qquad = \int q(\zdat_n) \exlogjoint{\tinymath{\prod_{m\neq n} q(\zdat_m)}} d\zdat_n + \ent{q_n} + \sum_{m\neq n} \ent{q_m}
\end{split}\\
\begin{split}
& \qquad = -\kl{q(\zdat_n)}{\tilde{p}(\xdat, \zdat|\param)} + \sum_{m\neq n} \ent{q_m} 
\end{split}
\end{align}

Where $\tilde{p}(\xdat, \zdat|\param)$ is the distribution corresponding to
\begin{align}
\ln \tilde{p}(\xdat, \zdat|\param) = \ex{\logjoint}{\tinymath{\prod_{m\neq n} q(\zdat_m)}} + \text{const}
\end{align}
As the KL divergence is always non-negative, the bound is maximised when it is set to zero, from which the following update is derived: 

\begin{align}
q(\zdat) \propto \exp\left( \exlogjoint{\tinymath{\prod_{m\neq n} q(\zdat_m)}}  \right)
\end{align}

Unlike the case of EM however, setting the KL divergence to zero does not cause the free-energy to become equal to the likelihood for a fixed $\params$. This is because the true posterior $p(\zdat | \xdat, \params)$ may not be in the family of distributions $\qfam$ which we have chosen. Consequently, the variational E-Step is not guaranteed to maximise the bound, and so the algorithm is not guaranteed to converge to a local maximum as with EM. Optimising the bound will still improve the likelihood however, and may therefore find useful parameter values. Fuller descriptions of variational learning are given in \cite{Jordan1999a}\cite{Bishop2006}\cite{Tzikas2008}. 



\subsubsection*{Further Methods of Inference}
For very large datasets, one can partition parameters and distributions into those ``local" parameters directly linked to individual observations, and ``global" parameters, and then infer local parameters one small batch of documents at a time, and use them to perform a gradient update\cite{RobbinsMonro1951} on the global variables, leading to a stochastic variational update scheme\cite{Hoffman2012}. This addresses the case where the whole dataset will not fit in memory. Additionally it allows for global parameters to be while inferring local parameters rather than waiting till all local parameters have been learnt.
 
%\subsection{Stochastic Variational Learning}
%In many models, in addition to the IID assumption, we can further consider the set of random variables to be partitioned into two disjoint subsets: one consisting of the ``local" latent variables $\zd$  associated with each datapoint $\xd$, and then ``global" variables $\param$ which are associated with the model overall.
%
%In more concrete terms, the joint distribution, including hyperparameters $\alpha$ can be written as
%
%\begin{align}
%p(\xdat, \zdat, \param) = p(\param | \beta)  \prod_d \prod_n p(\xd, \zd | \param)\end{align}
%
%Note that compared to the previous sections, $\param$ may be a set of random variables in this setting like $\zdat$ and not simply a parameter with no distribution.
%
%Inference can of course be carried out using EM or variational inference, however there are two issues that arise:
%
%\begin{enumerate}
%    \item On the first iteration, we take a full run through the dataset with what is essentially a nonsense value of $\param$ - convergence speed may be improved by updating it during the run.
%    \item The update for $\param$ requires a calculation involving the entire dataset, which may be prohibitive with large datasets.\
%\end{enumerate}
%
%These two problems are addressed by stochastic gradient descent. First however, we look at gradient descent.
%
%\subsubsection*{Gradient Descent}
%Gradient descent is used to find the root of a function $f(x; \param)$ using the update
%
%\begin{align}
%\param^{t+1} & \leftarrow \param^t - \rho_t \nabla_\param f(x; \param^t) &
%\text{where } \rho_t = (\tau_0 + t)^{-\kappa}
%\end{align}
%
%where $\kappa \in (0.5,1]$ controls the learning rate and $\tau_0 \geq 0$ slows down this rate in the early stages of the algorithm. Other kinds of step-size function $\rho_t$ can be used: so long as $\sum_t \rho_t = \infty$ and $\sum_t \rho^2_t < \infty$ the algorithm should converge to a local minimum\cite{RobbinsMonro1951}. 
%
%This can be used in machine learning where the objective function $f(x; \param)$ is an error-function, or in the case of probabilistic models, the negative log-likelihood.
%
%\subsubsection*{Stochastic Gradient Descent}
%In order to perform the update in gradient descent one needs to consider the gradient evaluated at the entire dataset, which is still costly. A better approach is to evaluate the gradient at a random function $b(\xdat; \param)$ such that $\ex{b(\xdat; \param)}{} = \sum_{x \in \xdat} f(x; \param)$. One can then perform a stochastic gradient descent of the form
%
%\begin{align}
%\param^{t+1} \leftarrow \param^t - \rho_t \nabla_\param b_t(\xdat; \param^t)
%\end{align}
%
%Note that in this formula above we sample a different instance of this function at each time step. The issue is how to choose $b(\xdat; \param)$.
%
%Assume that we are interested in $\ex{f(x)}{}$, the expectation of the given function (a loss function say) across all possible inputs according to the data-generating distribution. Since the data-generating distribution is unknown, we approximate it using the empirical distribution of the dataset
%
%\begin{align}
%p(x) = \frac{1}{D} \sum_d \delta(x, x_d)
%\end{align}
%where D is the size of our dataset, and additionally we define $D(x)$ to be the number of copies of input $x$ in the dataset. With this formulation we can write the following.
%
%\begin{align}
%\ex{\frac{D}{D(x)} f(x)}{}
%& = \sum_d \frac{D}{D(x_d)} f(x_d) p(x_d) \\
%& = \sum_d \frac{D}{D(x_d)} f(x_d) \left(\frac{1}{D} \sum_p \delta(x_d, x_p)\right) \\ 
%& = \sum_d \frac{D}{D(x_d)} f(x_d) \frac{D(x_d)}{D} = \sum_d f(x_d)
%\end{align}
%
%In the case where we assume all data-points are different, this simplifies to
%
%\begin{align}
%\ex{D f(x)}{} = \sum_d f(x_d)
%\end{align}
%
%Thus any approximation of the expectation (e.g. by sampling from the data-distribution) is an approximation of the sum, providing us with a definition for $b(\xdat; \param)$ of $\sum_{x \in \xdat} f(x;param)$
%
%\begin{align}
%b(x) = \frac{D}{S} \sum_s f(x_s) \approx \sum_d f(x_d)
%\end{align}
%
%where S is the number of samples. In theory one could evaluate the gradient a sample at a time, however in practice, to minimise the variance of the gradient estimates, the number of samples (often called the ``batch size") is usually set to a reasonably large number. The resulting stochastic EM algorithm is therefore
%
%\begin{enumerate}
%    \item Initialise latent variables $\zdat$ and parameters $\param$
%    \item Iterate until converged, time-step is $t$:
%    \begin{enumerate}
%        \item Sample a batch of $S$ observations $\{ x_s \}_{s=1}^S$ and infer the (factored\footnote{Note that this is not necessarily a variational mean-field factorization; for example in mixture-models using ordinary EM, the posterior naturally factors over the IID datapoints}) posterior:
%        \begin{align}
%            q(z_s) = \arg \max_{q_s} \fqt{q_s, \param}
%        \end{align}
%        \item Use the posteriors to perform a gradient update of the parameters
%        \begin{align}
%        \param \leftarrow \param + \rho_t \nabla_\param \left(\sum_s \ex{\ln p(x_s, z_s|\param)}{q(z_s)}\right)
%        \end{align}
%        \end{enumerate}
%\end{enumerate}
%
%While stochastic gradient descent has been motivated thus far as a means to handle datasets too large to fit into memory, and as a means update global parameters during  -- rather than after -- inference of ``local" variables, stochastic gradient descent is also useful in unbalanced classification problems.
%
%Consider a binary recommendation problem where the majority of products have never been consumed by customers, and so the number of negative datapoints $D^-$ vastly outnumbers the number of positive datapoints $D^+$. It is possible to develop a \emph{weighted} stochastic gradient algorithm such that relatively few negative datapoints are visited, without breaking the convergence guarantees. 
%
%As an example, consider a sampling distribution $q(x)$ such that with some very small probability $\epsilon$ we choose one of the $D^-$ negative datapoints, and with high probability $1 - \epsilon$ we choose one of the $D^+$ positive datapoints. We can re-write our expectation as
%
%\begin{align}
%\ex{D f(x)}{} \quad
%&= \quad \sum_d D f(x_d) \frac{p(x_d)}{q(x_d)} q(x_d) 
%& = \quad & \ex{D f(x) \frac{p(x)}{q(x)}}{q} \\
%& & \approx & \frac{D}{S} \sum_s f(x_s) \frac{p(x_s)}{q(x_s)} \\
%& & = & \frac{D}{S} \sum_s \frac{f(x_s)}{q(x_s)} \frac{1}{D} \\
%& & = & \frac{1}{S} \sum_s \frac{f(x_s)}{q(x_s)}
%\end{align}
%Where samples $x_s$ are drawn from $q$ and we've omitted the detail of $D(x)$ for brevity. If $x_s$ is sampled from the negative samples, its weight $\oneover{q(x_s)}$ will be the large number $\frac{D^-}{\epsilon}$, whereas if it's sampled from the positive group its weight will be the much smaller $\frac{D^+}{1 - \epsilon}$. This method was considerably extended in \cite{Gopalan2013b} (see supplemental material) for the problem of predicting citations within an academic corpus, where the majority of node-pairs had no links between them.
%
%\subsubsection*{The Natural Gradient}
%A final refinement of gradient descent, noted in \cite{Hoffman2012} is to observe that the gradient is the solution to
%
%\begin{align}
%\lim_{\epsilon \rightarrow 0} \arg \max_{d\param} f(\param + d\param) \qquad \text{where } ||d\param|| < \epsilon
%\end{align}
%And thus implicitly works on the idea of a Euclidean distance, which is inappropriate in the case of probability distribution parameters. If one were to use the symmetric KL divergence instead of the 2-norm:
%
%\begin{align}
%\symmkl{\param}{\param'}
% = \ex{\ln \frac{p(x; \param)}{p(x; \param'}}{\param}
% + \ex{\ln \frac{p(x; \param')}{p(x; \param}}{\param'}
%\end{align}
%
%one can obtain a ``natural gradient" instead. In practice one can convert a Euclidean gradient to a natural gradient by premultiplying the former by a Riemannian matrix, which for probability distributions is the Fisher information matrix. As noted in \cite{Bottou2004} gradient descent convergence is unaffected by premultiplying the gradient by a matrix, so long as that matrix is positive semi-definite.
%
%However as first observed in \cite{Hoffman2010}, and then fully explained in \cite{Hoffman2012}, for conditionally conjugate models composed of distributions in the exponential family, the formula for the the natural gradient is identical to the formula for the posterior, and so one need not determine the Fisher information. For example, in a variational mixture of multinomials model, where one infers a full posterior $q(\param_k)$ over the parameters instead of a MAP estimate, the batch update is:
%
%\begin{align}
%q(\param_k) = \dir{\vv{\beta} + \sum_d z_{dk} \vv{x}_{d}}
%\end{align}
%
%while the natural gradient update, where we parameterise the posterior $q(\param; \hat{\param})$ by $\hat{\param}$ is 
%
%\begin{align}
%\hat{\param} \leftarrow \hat{\param} + \rho_t \left(\vv{\beta} + \frac{D}{S} \sum_s z_{sk} \vv{x}_s\right)
%\end{align}


\input{../footer.tex}
