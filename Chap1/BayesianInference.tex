\input{../header.tex}

\newcommand \zd   { { \vv{z}_d } }
\newcommand \qfam { { \mathcal{Q} } }
\newcommand \xdat { { \mathcal{X} } }
\newcommand \zdat { { \mathcal{Z} } }
\newcommand \xnew { { \vv{x}_{\text{new}} } }
\newcommand \znew { { \vv{z}_{\text{new}} } }
\newcommand \param { { \vv{\phi} } }
\newcommand \ml[1] { { {#1}_{\text{ML}} } } 
\newcommand \map[1] { { {#1}_{\text{MAP}} } } 
\newcommand \quarter { { \oneover{4} } }
\newcommand \eighth { { \oneover{8} } }
\newcommand \fqt[1] { { \mathcal{F}\left( {#1} \right) } }
\newcommand \joint { { p(\xdat, \zdat | \param) } }
\newcommand \logjoint { { \ln \joint } }
\newcommand \exlogjoint[1] { { \ex{\logjoint}{{#1}} } }

\section{Inference in Probabilistic Models}
In probabilistic inference, we are concerned with two problems: inference is the problem of estimating parameter values $\param$ from a set of observations $\vv{x} \in \xdat$; prediction is the problem of predicting the distribution of a new datapoint from previous datapoints $p(\xnew | \xdat) = \int p(\xnew | \param) p(\param | \xdat) d\param$. Various methods have been proposed for this purpose

\subsubsection*{Maximum Likelihood}
The distribution $p(\xdat|\param)$ is known as the likelihood, and the maximum-likelihood approach to estimation seeks to find the form of $\param$ that maximises the log of that quantity

\begin{align*}
\ml{\param} = \arg \max_{\param} \ln p(\xdat | \param)
\end{align*}
As the log of a monotonically increasing function of its inputs, the $\param$ that maximises $\ln p(\xdat | \param)$ maximises $p(\xdat | \param)$, however by taking the log, the expression usually simplifies. In particular if we assume that the data are independently and identically distributed, conditional on the parameters, then we can factor the maximum likelihood update into a sum over every datapoint $\xd$

\begin{align*}
\ml{\param} = \arg \max_{\param} \sum_d \ln p(\xd | \param)
\end{align*}

For prediction we use the approximation

\begin{align*}
p(\xnew | \xdat) 
&= \int p(\xnew | \param) p(\param | \xdat) d\param \\
&\approx \int p(\xnew | \ml{\param}) p(\param | \xdat) d\param
=  p(\xnew | \ml{\param})
\end{align*}

As an example, consider a biased, four sided dice, for which we have seen 2, 4, 1 and 1 throws respectively for number 1 to 4. The likelihood distribution for this is the \emph{categorical} distribution, which is equivalent to a multinomial with a single draw (similar to the Bernoulli / Binomial relationship). Its parameter should have four parameters and like on the 3-simplex - i.e. its values should sum to one. 

The likelihood is therefore $p(\param | \xdat) = \prod_d \prod_i \phi_i^{x_{di}}$, which, by letting $n_i$ be the number of times the $k$-th face turned up, we can simplify to $p(\param | \xdat) = \prod_i \phi_i^{n_i}$. To find the maximum we add a Lagrange multiplier to the log-likelihood, to encode the simplex-constraint, take derivatives to find the saddle-point, and solve at zero. The solution has the pleasingly intuitive form $\param_i = \frac{n_i}{\sum_j n_j}$, which in this case is $\param \approx \{0.25, 0.50, 0.063, 0.063\}$. 

\subsubsection*{Maximum a Posteriori}
In MAP estimation we put a prior distribution $p(\param)$ on the parameter, and try to estimate the \emph{mode} of the posterior distribution, given by Bayes rule
\begin{align*}
p(\param | \xdat) = \frac{p(\xdat | \param)p(\param)}{p(\xdat)} = \frac{\text{likelihood} \times \text{prior}}{\text{evidence}}
\end{align*}

The MAP solution is:

\begin{align*}
\map{\param} = \arg \max_{\param} \frac{p(\xdat | \param)p(\param)}{p(\xdat)}
& = \arg \max_{\param} p(\xdat | \param)p(\param) \\
 & = \arg \max_\param \ln p(\xdat | \param) + \ln p(\param)
\end{align*}
which is found via differentiation as with maximum likelihood. Prediction also follows the maximum likelihood case example by substituting in the MAP point estimation. $p(\xnew | \xdat) \approx p(\xnew | \map{\param})$. Such an approximation is appropriate if we assume the posterior is tightly distributed around the mode and hence

The incorporation for the prior is motivated by two related reasons: firstly it allows a practitioner to supply some prior knowledge to the inference procedure (e.g. most dice are fair); secondly it prevents a model from overfitting in the presence of limited or noisy data, such as presuming a dice is unfairly biased towards four after only 8 throes.

We can illustrate this using our dice example from above. The distribution most frequently use for vectors on the simplex is the Dirichlet, which has the pdf:

\begin{align*}
p(\param) & = \oneover{B(\vv{\beta})} \prod_i \phi_i^{(\beta_i - 1)} &
B(\vv{\beta}) & = \frac{\prod_i \Gamma(\beta_i)}{\Gamma(\sum_i \beta_i)}
\end{align*}
where $B(\cdot)$ is the multivariate beta function, Gamma function $\Gamma(\beta)$ is a continuous generalisation of the factorial function from integers to the reals, such that for an integer $\beta$, $\Gamma(\beta + 1) = \beta!$. We encode a reasonably strong preference that the dice is fair by setting $\vv{\beta} = \{ 3, 3, 3, 3 \}$, known as a symmetric prior as $\beta_i = \beta_j$ for all $k, j$. Dropping the terms which don't rely on $\param$ and taking derivatives we arrive at the following
\begin{align}
\map{\param} & = \ln p(\xdat|\param) + \ln p(\param) \\
 &= \arg \max_\param \sum_i n_i \ln \phi_i + \sum_i (\beta-1) \ln \phi_i
\end{align}
Adding in the Lagrangian and taking derivatives we see the solution is $\phi_i = \frac{n_i + \beta_i - 1}{\sum_j n_j + \beta_j - 1}$. Thus in this, our estimation is $\param = \{0.3, 0.4, 0.1, 0.1\} $ which is a lot closer to the mean than our ML estimate.

\subsubsection*{Bayesian Inference}
In Bayeisan inference we solve Bayes rule directly, and so derive a \emph{distribution} over $\param$,  , instead of a single point-estimate. This involves evaluating the evidence $p(\xdat) = \int p(\xdat | \param) p(\param) d\param$. In many cases this is intractable, and so approximations need to be made, but this is not the case in our Dirichlet-Multinomial example, where we find

\begin{align*}
p(\xdat) & = \frac{1}{B(\beta)} \int \prod_i \phi_i^{n_i} \prod_i \phi_i^{(\beta_i - 1)} d\param  = \frac{1}{B(\beta)} \int \prod_i \phi_i^{n_i + \beta_i - 1} \\
& = \frac{B(\vv{n} + \vv{\beta})}{B(\vv{\beta})}.
\end{align*}

Which comes from observing that the integrand is just an unnormalised Dirichlet distribution parameterised by $\vv{n} + \vv{\beta}$ and so evaluates to $B(\vv{n} + \vv{\beta})$. This marginal distribution is sometimes known as the P\'{o}lya distribution. 

The product of the likelihood and evidence is $\frac{1}{B(\vv{\beta})} \prod_i \phi_i^{(n_i + \beta_i - 1)}$ and so the $B(\vv{\beta})$ terms cancel out, and leave us with just a Dirichlet distribution parameterised by $\vv{n} + \vv{\beta}$. In our dice example, we now evaluate the expected value $\ex{\param}{} = \frac{n_i + \beta_i}{\sum_j n_j + \beta_j}$ which, due to the lack of ``minus ones", brings the solution to our dice problem even closer to the prior than the MAP solution.

This neat result, where the prior and posterior have the same functional form, is known as conjugacy, and we say that the Dirichlet is the conjugate prior distribution of the multinomial likelihood. Conjugacy dramatically simplifies inference, and so is preferred. Additionally, conjugacy provides an intuitive understanding of the priors as being just pseudo data-points.

A final advantage of conjugate models is that is is possible to evaluate the predictive distribution $p(\xnew | \xdat)$ exactly as a function of the data and hyper-parameters only.

\begin{align*}
p(\xnew | \xdat) = \frac{B(\xnew + \vv{n} + \vv{\beta})}{B(\vv{\beta})}
\end{align*}
 
\subsection{Latent Variable Models}
Oftentimes it is appropriate to assume that there is some unobserved, ``latent" process which determines the distributions of the observed variables. Denoting the latent variables as $\vv{z}$ this gives rise to the following likelihood

\begin{align*}
p(\xdat|\param) = \int p(\xdat, \zdat|\param) d\zdat
\end{align*}

As an example consider now that we have a collection of dice supplied by three manufacturers. We suspect each manufacturers dices is biased in a different manner, but can't tell which dice is which. We therefore seek to identify which manufacturer fabricated each die $\zdat$, and what is the distribution over faces for that die $\param_i$ where i identifies the manufacturer. This is simply a mixture model, in this case over multinomials, with $i$ indicating the cluster where, for the $n$-th roll of the die $\vv{x}_d$, $\vv{z}_d$ indicates which of the three manufacturers fabricated that die. Like the distribution over the face $\vv{x}_d$, the latent manufacturer $\vv{z}_d$ has a categorical likelihood. This gives rise to the mixture of multinomials model\cite{Nigam2000}
\begin{align*}
\vv{z}_d & \sim \muln{\vv{\theta}}{1} & \vv{\theta} & \sim \dir{\vv{\alpha}} &
\vv{x}_d & \sim \muln{\param}{1} & \param & \sim \dir{\vv{\beta}}
\end{align*}

To derive a MAP estimate of $\param$, we need to approximate the log-likelihood using Jensen's inequality which, for any \emph{concave} function $f(x)$, states that $f\left(\ex{x}{}\right) \geq \ex{f(x)}{}$. Thus, if we choose, for now, an arbitrary distribution over the latent variables $q(\zdat)$ we can approximate the log-likelihood as

\begin{align*}
\ln p(\xdat|\param) & = \ln \int \joint p(\param) d\zdat 
 = \ln \int \frac{q(\zdat)}{q(\zdat)} \joint p(\param) d\zdat \\
& \geq \int  q(\zdat) \left(\logjoint + \ln p(\param)\right) d\zdat
  -     \int \ln q(\zdat) \ln q(\zdat) d\zdat \\
& = \exlogjoint{q} + \ln p(\param) + \ent{q} = \fqt{q, \param}
\end{align*}
where the term $\fqt{q, \param}$ is known as the ``free energy". The resulting maximisation algorithm requires iteratively alternating between

\begin{align*}
\text{E-Step:} & & q^t(\zdat) & = \arg \max_{q(\zdat)} \fqt{q(\zdat), \param^{t-1}} \\
\text{M-Step:} & & \param^t & = \arg \max_{\param} \fqt{q^t(\zdat), \param} \\
& & & = \arg \max_{\param} \exlogjoint{q} + \ln p(\param)
\end{align*}
where $t$ is the current iteration. To determine how to maximise $q(\zdat)$ we observe that, for a fixed $\param$, the difference between the free energy and the regularised log-likelihood is given by the Kullback Leibler divergence between $q(\zdat)$ and the posterior $p(\zdat|\zdat, \param)$.
\begin{align*}
\fqt{q, \param} 
& = \ln p(\param) + \int q(\zdat) \ln \frac{p(\xdat, \zdat|\param)}{q(\zdat)} d\zdat \\
& = \ln p(\param) + \int q(\zdat) \ln \frac{p(\zdat|\xdat, \param)p(\xdat | \param)}{q(\zdat)} d\zdat \\
& = \ln p(\param) + \int q(\zdat) \ln p(\xdat | \param) q\zdat + \int q(\zdat)\frac{p(\zdat|\xdat, \param)}{q(\zdat)} d\zdat \\
& = \ln p(\param) + \ln p(\xdat | \param) + \kl{q(\zdat)}{p(\zdat|\xdat, \param)}
\end{align*}
When $\kl{q(\zdat)}{p(\zdat|\xdat, \param)} = 0$, $\fqt{q, \param} = \ln(p(\xdat | \param)p(\param))$, from which we conclude the free-energy is maximised for a fixed $\param$ when $q(\zdat) = p(\zdat | \xdat, \param)$.

This is known as the MAP Expectation Maximization (EM) algorithm: if we drop the prior over parameters it is the standard EM algorithm, which was first codified in \cite{Dempster1977}. In the case of the mixture of multinomials model, the updates are

\begin{align*}
\text{E-Step:} & & q(\zdat) & = \prod_d \muln{\hat{\vv{\theta}}_d}{1}, \qquad
\hat{\vv{\theta}}_d \propto  \alpha_k + \sum_d x_{di} \ln \phi_{ki}  \\
\text{M-Step:} & & \phi_{ki} & = \frac{\sum_d \ex{z_{dk}}{q} x_{dk} + \beta_k - 1}{\sum_j \sum_d \ex{z_{dj}}{q} x_{dj} + \beta_j - 1}
\end{align*}
where $\ex{z_{nk}}{q} = \theta_{nk}$. The predictive distribution is

\begin{align*}
p(\xnew | \xdat)  = \sum_k p(\xnew, z_{\text{new}} = k | \param) 
\approx \sum_k p(\xnew | \param^{(k)}_{\text{MAP}})
\end{align*}

A final note on EM is that it is always guaranteed to find a possibly local maximum in the the bounded likelihood function. In the E-Step, for a fixed parameter value $\param$, we set the KL divergence between the $q(\zdat)$ and the true posterior to zero, increasing the bound such that its value exactly matches the likelihood. In the M-Step, we maximise the free-energy with respect to the parameter, such that it's greater than the previous value of the free-energy, which is less than the likelihood evaluated at that new parameter value, but which will be increased to that value once more in the E-Step.


\subsubsection{Variational EM}
It is not always feasible to evaluate the posterior distribution $p(\zdat|\xdat, \param)$ exactly. In such cases one needs to either approximate the update to the distribution (e.g. by parameterising it and taking a gradient step), or approximate the distribution itself.

We consider the latter approach, where one assumes the true distribution $q(\zdat)$ can be approximated by a family of distributions $\qfam$. A popular, though not the only choice, is the family of factorized distributions $q(\zdat) = \prod_m q(\zdat_m)$. Note that this factorization is not derived from the IID principle, as the in prior discussion on EM, rather it is a modelling assumption to be made by the practitioner. The question then is how to evaluate the variational E-Step

\begin{align*}
\text{Variational E-Step:} & & q(\zdat) = \arg \max_{q(\zdat_1) \ldots q(\zdat_M)} \fqt{\prod_m q(\zdat_m), \param}
\end{align*}

Given this formulation, we can solve independently for each block of latent variables $\zdat_m$ in turn, using the following decomposition.

\begin{align*}
\fqt{\prod_m q(\zdat_m), \param} 
 = \exlogjoint{\tinymath{\prod_m q(\zdat_m)}} + \ent{q} \qquad\qquad\qquad\\
\qquad = \int q(\zdat_n) \exlogjoint{\tinymath{\prod_{m\neq n} q(\zdat_m)}} d\zdat_n + \ent{q_n} + \sum_{m\neq n} \ent{q_m} \\
\qquad = -\kl{q(\zdat_n)}{\exp(\exlogjoint{\tinymath{\prod_{m\neq n} q(\zdat_m)}})} + \sum_{m\neq n} \ent{q_m}
\end{align*}

\fixme{This may be wrong}
As the KL divergence is always non-negative, the bound is maximised when it is set to zero, from which the following update is derived: 

\begin{align*}
q(\zdat) \propto \exp\left( \exlogjoint{\tinymath{\prod_{m\neq n} q(\zdat_m)}}  \right)
\end{align*}

Unlike the case of EM however, setting the KL divergence to zero does not cause the free-energy to become equal to the likelihood for a fixed $\param$. This is because the true posterior $p(\zdat | \xdat, \param)$ may not be in the family of distributions $\qfam$ which we have chosen. Consequently, the variational E-Step is not guaranteed to maximise the bound, and so the algorithm is not guaranteed to converge to a local maximum as with EM. However by optimising the bound will still improve the likelihood, and may therefore find useful parameter values. Fuller descriptions of variational learning are given in \cite{Jordan1999a}\cite{Bishop2006}\cite{Tzikas2008}. 

\subsection{Stochastic Variational Learning}
In many models, in addition to the IID assumption, we can further consider the set of random variables to be partitioned into two disjoint subsets: one consisting of the ``local" latent variables $\zd$  associated with each datapoint $\xd$, and then ``global" variables $\param$ which are associated with the model overall.

In more concrete terms, the joint distribution, including hyperparameters $\alpha$ can be written as

\begin{align*}
p(\xdat, \zdat, \param) = p(\param | \beta)  \prod_d \prod_n p(\xd, \zd | \param)\end{align*}

Note that compared to the previous sections, $\param$ may be a set of random variables in this setting like $\zdat$ and not simply a parameter with no distribution.

Inference can of course be carried out using EM or variational inference, however there are two issues that arise:

\begin{enumerate}
    \item On the first iteration, we take a full run through the dataset with what is essentially a nonsense value of $\param$ - convergence speed may be improved by updating it during the run.
    \item The update for $\param$ requires a calculation involving the entire dataset, which may be prohibitive with large datasets.\
\end{enumerate}

These two problems are addressed by stochastic gradient descent. First however, we look at gradient descent.

\subsubsection*{Gradient Descent}
Gradient descent is used to find the root of a function $f(x; \param)$ using the update

\begin{align*}
\param^{t+1} & \leftarrow \param^t - \rho_t \nabla_\param f(x; \param^t) &
\text{where } \rho_t = (\tau_0 + t)^{-\kappa}
\end{align*}

where $\kappa \in (0.5,1]$ controls the learning rate and $\tau_0 \geq 0$ slows down this rate in the early stages of the algorithm. Other kinds of step-size function $\rho_t$ can be used: so long as $\sum_t \rho_t = \infty$ and $\sum_t \rho^2_t < \infty$ the algorithm should converge to a local minimum\cite{RobbinsMonro1951}. 

This can be used in machine learning where the objective function $f(x; \param)$ is an error-function, or in the case of probabilistic models, the negative log-likelihood.

\subsubsection*{Stochastic Gradient Descent}
In order to perform the update in gradient descent one needs to consider the gradient evaluated at the entire dataset, which is still costly. A better approach is to evaluate the gradient at a random function $b(\xdat; \param)$ such that $\ex{b(\xdat; \param)}{} = \sum_{x \in \xdat} f(x; \param)$. One can then perform a stochastic gradient descent of the form

\begin{align*}
\param^{t+1} \leftarrow \param^t - \rho_t \nabla_\param b_t(\xdat; \param^t)
\end{align*}

Note that in this formula above we sample a different instance of this function at each time step. The issue is how to choose $b(\xdat; \param)$.

Assume that we are interested in $\ex{f(x)}{}$, the expectation of the given function (a loss function say) across all possible inputs according to the data-generating distribution. Since the data-generating distribution is unknown, we approximate it using the empirical distribution of the dataset

\begin{align*}
p(x) = \frac{1}{D} \sum_d \delta(x, x_d)
\end{align*}
where D is the size of our dataset, and additionally we define $D(x)$ to be the number of copies of input $x$ in the dataset. With this formulation we can write the following.

\begin{align*}
\ex{\frac{D}{D(x)} f(x)}{}
& = \sum_d \frac{D}{D(x_d)} f(x_d) p(x_d) \\
& = \sum_d \frac{D}{D(x_d)} f(x_d) \left(\frac{1}{D} \sum_p \delta(x_d, x_p)\right) \\ 
& = \sum_d \frac{D}{D(x_d)} f(x_d) \frac{D(x_d)}{D} = \sum_d f(x_d)
\end{align*}

In the case where we assume all data-points are different, this simplifies to

\begin{align*}
\ex{D f(x)}{} = \sum_d f(x_d)
\end{align*}

Thus any approximation of the expectation (e.g. by sampling from the data-distribution) is an approximation of the sum, providing us with a definition for $b(\xdat)$ in the case where $f(\xdat)$ is a sum, as is the case with log-likelihood over IID data:

\begin{align*}
b(x) = \frac{D}{S} \sum_s f(x_s) \approx \sum_d f(x_d) = f(\xdat)
\end{align*}
where S is the number of samples. In theory one could evaluate the gradient a sample at a time, however in practice, to minimise the variance of the gradient estimates, the number of samples (often called the ``batch size") is usually set to a reasonably large number. The resulting stochastic EM algorithm is therefore

\begin{enumerate}
    \item Initialise latent variables $\zdat$ and parameters $\param$
    \item Iterate until converged, time-step is $t$:
    \begin{enumerate}
        \item Sample a batch of $S$ observations $\{ x_s \}_{s=1}^S$ and infer the (factored\footnote{Note that this is not necessarily a variational mean-field factorization; for example in mixture-models using ordinary EM, the posterior naturally factors over the IID datapoints}) posterior:
        \begin{align*}
            q(z_s) = \arg \max_{q_s} \fqt{q_s, \param}
        \end{align*}
        \item Use the posteriors to perform a gradient update of the parameters
        \begin{align*}
        \param \leftarrow \param + \rho_t \nabla_\param \left(\sum_s \ex{\ln p(x_s, z_s|\param)}{q(z_s)}\right)
        \end{align*}
        \end{enumerate}
\end{enumerate}

While stochastic gradient descent has been motivated thus far as a means to handle datasets too large to fit into memory, and a means update global parameters during rather than after inference of ``local" variables, stochastic gradient descent is also useful in unbalanced classification problems.

Consider a binary recommendation problem where the majority of products have never been consumed by customers, and so the number of negative datapoints $D^-$ vastly outnumbers the number of positive datapoints $D^+$. It is possible to develop a \emph{weighted} stochastic gradient algorithm such that relatively few negative datapoints are visited, without breaking the convergence guarantees. 

Say we define a sampling distribution $q(x)$ such that with some very small probability $\epsilon$ we choose one of the $D^-$ negative datapoints, and with high probability $1 - \epsilon$ we choose one of the $D^+$ positive datapoints. We can re-write our expectation as

\begin{align*}
\ex{D f(x)}{} \quad
&= \quad \sum_d D f(x_d) \frac{p(x_d)}{q(x_d)} q(x_d) 
& = \quad & \ex{D f(x) \frac{p(x)}{q(x)}}{q} \\
& & \approx & \frac{D}{S} \sum_s f(x_s) \frac{p(x_s)}{q(x_s)} \\
& & = & \frac{D}{S} \sum_s f(x_s) \frac{1}{D}q(x_s) \\
& & = & \frac{1}{S} \sum_s f(x_s) q(x_s)
\end{align*}
Where samples $x_s$ are drawn from $q$ and we've omitted the detail of $D(x)$ for brevity. If $x_s$ is sampled from the negative samples, its weight will be the large number $\frac{D^-}{\epsilon}$, whereas if it's sampled from the positive group its weight will be the much smaller $\frac{D^+}{1 - \epsilon}$. This method was considerably extended in \cite{Gopalan2013b} (see supplemental material) for the problem of predicting citations within an academic corpus, where the majority of node-pairs had no links between them.

\subsubsection*{The Natural Gradient}
A final refinement of gradient descent, noted in \cite{Hoffman2012} is to observe that the gradient is the solution to

\begin{align*}
\lim_{\epsilon \rightarrow 0} \arg \max_{d\param} f(\param + d\param) \qquad \text{where } ||d\param|| < \epsilon
\end{align*}
And thus implicitly works on the idea of a Euclidean distance, which is inappropriate in the case of probability distribution parameters. If one were to use the symmetric KL divergence instead of the 2-norm:

\begin{align*}
\symmkl{\param}{\param'}
 = \ex{\ln \frac{p(x; \param)}{p(x; \param'}}{\param}
 + \ex{\ln \frac{p(x; \param')}{p(x; \param}}{\param'}
\end{align*}

one can obtain a ``natural gradient" instead. In practice one can convert a Euclidean gradient to a natural gradient by premultiplying the former by a Riemannian matrix, which for probability distributions is the Fisher information matrix. As noted in \cite{Bottou2004} so long as the matrix in question is postive semi-definite, this will not affect the convergence guarantees.

However as first observed in \cite{Hoffman2010}, and then fully explained in \cite{Hoffman2012}, for conditionally conjugate models composed of distributions in the exponential family, the formula for the the natural gradient is identical to the formula for the posterior, and so one need not determine the Fisher information. For example, in a variational mixture of multinomials model, where one infers a full posterior $q(\param_k)$ over the parameters instead of a MAP estimate, the batch update is simply

\begin{align*}
q(\param_k) = \dir{\vv{\beta} + \sum_d z_{dk} \vv{x}_{d}}
\end{align*}

while the natural gradient update, where we parameterise the posterior $q(\param; \hat{\param})$ by $\hat{\param}$ is 

\begin{align}
\hat{\param} \leftarrow \hat{\param} + \rho_t \left(\vv{\beta} + \frac{D}{S} \sum_s z_{sk} \vv{x}_s\right)
\end{align}


\input{../footer.tex}
